{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "09d2eb5fbe70bea445a5bdf28ac0697b",
     "grade": false,
     "grade_id": "cell-e785bf541c3c6566",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Homework: PCA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "50ce8f8109224a024605fa44f6a5958b",
     "grade": false,
     "grade_id": "cell-b5f4e240125e48e2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Principal Component Analysis\n",
    "---\n",
    "\n",
    "In this problem you'll be implementing Dimensionality reduction using Principal Component Analysis technique. \n",
    "\n",
    "The gist of PCA Algorithm to compute principal components is follows:\n",
    "- Calculate the covariance matrix X of data points.\n",
    "- Calculate eigenvectors and corresponding eigenvalues.\n",
    "- Sort the eigenvectors according to their eigenvalues in decreasing order.\n",
    "- Choose first k eigenvectors which satisfies target explained variance.\n",
    "- Transform the original data of shape m observations times n features into m observations times k selected features.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f4571ea879ea92d09a2831fcdac74887",
     "grade": false,
     "grade_id": "cell-eea5ece608ed4ac5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The skeleton for the *PCA* class is below. Scroll down to find more information about your tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "43c4c380877a398b95cc910c3e3f0244",
     "grade": false,
     "grade_id": "cell-0e7782486f935045",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "import pandas\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0876c53bb156647022a7851eb5a2003d",
     "grade": false,
     "grade_id": "cell-5874a755dcce55e2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "class PCA:\n",
    "    def __init__(self, target_explained_variance=None):\n",
    "        self.target_explained_variance = target_explained_variance\n",
    "        self.feature_size = -1\n",
    "\n",
    "    ''' standardize - Standardize the data\n",
    "            Input:\n",
    "                X - Data\n",
    "            Algorithm:\n",
    "                * Use StandardScaler to standardize the data\n",
    "                * Fit the transform on the data\n",
    "            Output:\n",
    "                Returns the standardized data\n",
    "    '''\n",
    "    def standardize(self, X):\n",
    "        scaler = StandardScaler()\n",
    "        X_std = scaler.fit_transform(X)\n",
    "        return X_std\n",
    "\n",
    "    ''' compute_mean_vector - Compute the mean vector\n",
    "            Input:\n",
    "                X_std - Standardized data\n",
    "            Algorithm:\n",
    "                * Compute the mean of the data\n",
    "            Output:\n",
    "                Returns the mean vector\n",
    "    '''\n",
    "    def compute_mean_vector(self, X_std):\n",
    "        mean_vec = np.mean(X_std, axis=0)\n",
    "        return mean_vec\n",
    "\n",
    "    ''' compute_cov - Compute the covariance matrix\n",
    "            Input:\n",
    "                X_std - Standardized data\n",
    "                mean_vec - Mean vector\n",
    "            Algorithm:\n",
    "                * Get the shape of the data\n",
    "                * Center the data\n",
    "                * Compute the covariance matrix\n",
    "            Output:\n",
    "                Returns the covariance matrix\n",
    "    '''\n",
    "    def compute_cov(self, X_std, mean_vec):\n",
    "        m = X_std.shape[0]\n",
    "        X_centered = X_std - mean_vec\n",
    "        cov_mat = np.dot(X_centered.T, X_centered) / (m)\n",
    "        return cov_mat\n",
    "\n",
    "    ''' compute_eigen_vector - Compute the eigen vector\n",
    "            Input:\n",
    "                cov_mat - Covariance matrix\n",
    "            Algorithm:\n",
    "                * Use numpy's linalg.eig to compute the eigen vector\n",
    "            Output:\n",
    "                Returns the eigen values and eigen vectors\n",
    "    '''\n",
    "    def compute_eigen_vector(self, cov_mat):\n",
    "        eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n",
    "        return eig_vals, eig_vecs\n",
    "\n",
    "    ''' compute_explained_variance - Compute the explained variance\n",
    "            Input:\n",
    "                eigen_vals - Eigen values\n",
    "            Algorithm:\n",
    "                * Sum up the eigen values\n",
    "                * Sort the eigen values\n",
    "                * Compute the explained variance\n",
    "            Output:\n",
    "                Returns the explained variance\n",
    "    '''\n",
    "    def compute_explained_variance(self, eigen_vals):\n",
    "        total = np.sum(eigen_vals)\n",
    "        sorted = np.sort(eigen_vals)[::-1]\n",
    "        var_exp = sorted / total\n",
    "        return var_exp\n",
    "\n",
    "    ''' cumulative_sum - Compute the cumulative sum\n",
    "            Input:\n",
    "                var_exp - Explained variance\n",
    "            Algorithm:\n",
    "                * Use numpy's cumsum to compute the cumulative sum\n",
    "            Output:\n",
    "                Returns the cumulative sum\n",
    "    '''\n",
    "    def cumulative_sum(self, var_exp):\n",
    "        return np.cumsum(var_exp)\n",
    "\n",
    "    ''' compute_weight_matrix - Compute the weight matrix\n",
    "            Input:\n",
    "                eig_pairs - Eigen pairs\n",
    "            Algorithm:\n",
    "                * Iterate over the eigen pairs\n",
    "                * If the cumulative variance is less than the target explained variance\n",
    "                    * Append the eigen vector to the matrix\n",
    "                * Else break\n",
    "                * Stack the matrix\n",
    "            Output:\n",
    "                Returns the weight matrix\n",
    "    '''\n",
    "    def compute_weight_matrix(self, eig_pairs, cum_var_exp):\n",
    "        matrix_w = []\n",
    "        for i, (eig_val, eig_vec) in enumerate(eig_pairs):\n",
    "            if cum_var_exp[i] <= self.target_explained_variance:\n",
    "                matrix_w.append(eig_vec.reshape(-1, 1))\n",
    "            else:\n",
    "                break\n",
    "        matrix_w = np.hstack(matrix_w)\n",
    "        return matrix_w\n",
    "\n",
    "    ''' transform_data - Transform the data\n",
    "            Input:\n",
    "                X_std - Standardized data\n",
    "                matrix_w - Weight matrix\n",
    "            Algorithm:\n",
    "                * Dot product of the standardized data and the weight matrix\n",
    "            Output:\n",
    "                Returns the transformed data\n",
    "    '''\n",
    "    def transform_data(self, X_std, matrix_w):\n",
    "        return X_std.dot(matrix_w)\n",
    "\n",
    "    ''' fit - Fit the data\n",
    "            Input:\n",
    "                X - Data\n",
    "            Algorithm:\n",
    "                * Standardize the data\n",
    "                * Compute the mean vector\n",
    "                * Compute the covariance matrix\n",
    "                * Compute the eigen vector\n",
    "                * Compute the explained variance\n",
    "                * Compute the cumulative sum\n",
    "                * Compute the weight matrix\n",
    "                * Transform the data\n",
    "            Output:\n",
    "                Returns the transformed\n",
    "    '''\n",
    "    def fit(self, X):\n",
    "        self.feature_size = X.shape[1]\n",
    "        X_std = self.standardize(X)\n",
    "        mean_vec = self.compute_mean_vector(X_std)\n",
    "        cov_mat = self.compute_cov(X_std, mean_vec)\n",
    "        eig_vals, eig_vecs = self.compute_eigen_vector(cov_mat)\n",
    "        eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "        var_exp = self.compute_explained_variance(eig_vals)\n",
    "        cum_var_exp = self.cumulative_sum(var_exp)\n",
    "        # This step calculates the matrix_w\n",
    "        matrix_w = self.compute_weight_matrix(eig_pairs=eig_pairs,cum_var_exp=cum_var_exp) \n",
    "        print(len(matrix_w),len(matrix_w[0]))\n",
    "        return self.transform_data(X_std=X_std, matrix_w=matrix_w)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a69a61d06f52609eca00d653f99e94b9",
     "grade": false,
     "grade_id": "cell-72c2a70fcc1b5457",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "To Do: Complete helper functions above.\n",
    "\n",
    "complete `fit()` to using all helper functions to find reduced dimension data.\n",
    "\n",
    "Run PCA on *fashion mnist dataset* to reduce the dimension of the data.\n",
    "\n",
    "fashion mnist data consists of samples with *784 dimensions*.\n",
    "\n",
    "Report the reduced dimension $k$ for target explained variance of **0.99**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6b81e50a1e169900d57f6bde5059b554",
     "grade": false,
     "grade_id": "cell-1ca6638507fdcbd6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X_train = pickle.load(open('./data/fashionmnist/train_images.pkl','rb'))\n",
    "y_train = pickle.load(open('./data/fashionmnist/train_image_labels.pkl','rb'))\n",
    "\n",
    "X_train = X_train[:1500]\n",
    "y_train = y_train[:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fa28b9a88044825ca1e35e05796e6a26",
     "grade": false,
     "grade_id": "cell-7ccdd941a2845fa3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784 409\n"
     ]
    }
   ],
   "source": [
    "pca_handler = PCA(target_explained_variance=0.99)\n",
    "X_train_updated = pca_handler.fit(X_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
