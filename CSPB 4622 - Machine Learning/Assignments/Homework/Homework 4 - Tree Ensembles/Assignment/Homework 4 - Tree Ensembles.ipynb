{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "48cc68d5e6ef4d62dd331e0a56d2fe65",
     "grade": false,
     "grade_id": "cell-453482616cdb8d13",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Grading\n",
    "The final score that you will receive for your programming assignment is generated in relation to the total points set in your programming assignment itemâ€”not the total point value in the nbgrader notebook.<br>\n",
    "When calculating the final score shown to learners, the programming assignment takes the percentage of earned points vs. the total points provided by nbgrader and returns a score matching the equivalent percentage of the point value for the programming assignment.<br>\n",
    "**DO NOT CHANGE VARIABLE OR METHOD SIGNATURES** The autograder will not work properly if your change the variable or method signatures. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "858db041ca18418851312ee5acb5ea89",
     "grade": false,
     "grade_id": "cell-3c0f1d3b9e739cf4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Ensemble methods (adaBoost, random forests) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5b0d6749181b5e355f06c07b09992bbe",
     "grade": false,
     "grade_id": "cell-57691d3f0f3d712f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.base import clone \n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pylab as plt \n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "305ea6860ac6210e8de96e45a2d08ee2",
     "grade": false,
     "grade_id": "cell-2cb4f68eef534010",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Run the helper code below to create a training and validation set of threes and eights from the MNIST dataset. There is also a helper function to display digits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c8280522ca34851557005286ac66e105",
     "grade": false,
     "grade_id": "cell-42b1785ca793847d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class ThreesandEights:\n",
    "    \"\"\"\n",
    "    Class to store MNIST 3s and 8s data\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, location):\n",
    "\n",
    "        import pickle, gzip\n",
    "\n",
    "        # Load the dataset\n",
    "        f = gzip.open(location, 'rb')\n",
    "\n",
    "        # Split the data set \n",
    "        x_train, y_train, x_test, y_test = pickle.load(f)\n",
    "                \n",
    "        # Extract only 3's and 8's for training set \n",
    "        self.x_train = x_train[np.logical_or(y_train== 3, y_train == 8), :]\n",
    "        self.y_train = y_train[np.logical_or(y_train== 3, y_train == 8)]\n",
    "        self.y_train = np.array([1 if y == 8 else -1 for y in self.y_train])\n",
    "        \n",
    "        # Shuffle the training data \n",
    "        shuff = np.arange(self.x_train.shape[0])\n",
    "        np.random.shuffle(shuff)\n",
    "        self.x_train = self.x_train[shuff,:]\n",
    "        self.y_train = self.y_train[shuff]\n",
    "\n",
    "        # Extract only 3's and 8's for validation set \n",
    "        self.x_test = x_test[np.logical_or(y_test== 3, y_test == 8), :]\n",
    "        self.y_test = y_test[np.logical_or(y_test== 3, y_test == 8)]\n",
    "        self.y_test = np.array([1 if y == 8 else -1 for y in self.y_test])\n",
    "        \n",
    "        f.close()\n",
    "\n",
    "def view_digit(ex, label=None, feature=None):\n",
    "    \"\"\"\n",
    "    function to plot digit examples \n",
    "    \"\"\"\n",
    "    if label: print(\"true label: {:d}\".format(label))\n",
    "    img = ex.reshape(21,21)\n",
    "    col = np.dstack((img, img, img))\n",
    "    if feature is not None: col[feature[0]//21, feature[0]%21, :] = [1, 0, 0]\n",
    "    plt.imshow(col)\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    \n",
    "data = ThreesandEights(\"data/mnist21x21_3789.pklz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5f204445656ac10cff884e71e35c9c46",
     "grade": false,
     "grade_id": "cell-64e3366069583de8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Problem 1: Building an Adaboost Classifier to classify MNIST digits 3 and 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5f38766fb07c738cccba991bb22a02b3",
     "grade": false,
     "grade_id": "cell-f492e6362be1f32a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Recall that the model we attempt to learn in AdaBoost is given by \n",
    "\n",
    "$$\n",
    "H({\\bf x}) = \\textrm{sign}\\left[\\displaystyle\\sum_{k=1}^K\\alpha_k h_k({\\bf x}) \\right]\n",
    "$$\n",
    "\n",
    "where $h_k({\\bf x})$ is the $k^\\textrm{th}$ weak learner and $\\alpha_k$ is it's associated ensemble coefficient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "927bd4f00ea70315d7e2b9f947ab82b3",
     "grade": false,
     "grade_id": "cell-d8f15295dfa1942a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class AdaBoost:\n",
    "    def __init__(self, n_learners=20, base=DecisionTreeClassifier(max_depth=3), random_state=1234):\n",
    "        \"\"\"\n",
    "        Create a new adaboost classifier.\n",
    "        \n",
    "        Args:\n",
    "            N (int, optional): Number of weak learners in classifier.\n",
    "            base (BaseEstimator, optional): Your general weak learner \n",
    "            random_state (int, optional): set random generator.  needed for unit testing. \n",
    "\n",
    "        Attributes:\n",
    "            base (estimator): Your general weak learner \n",
    "            n_learners (int): Number of weak learners in classifier.\n",
    "            alpha (ndarray): Coefficients on weak learners. \n",
    "            learners (list): List of weak learner instances. \n",
    "        \"\"\"\n",
    "        \n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "        self.n_learners = n_learners \n",
    "        self.base = base\n",
    "        self.alpha = np.zeros(self.n_learners)\n",
    "        self.learners = []\n",
    "        \n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Train AdaBoost classifier on data. Sets alphas and learners. \n",
    "        \n",
    "        Args:\n",
    "            X_train (ndarray): [n_samples x n_features] ndarray of training data   \n",
    "            y_train (ndarray): [n_samples] ndarray of data \n",
    "        \"\"\"\n",
    "\n",
    "        # =================================================================\n",
    "        # TODO \n",
    "\n",
    "        # Note: You can create and train a new instantiation \n",
    "        # of your sklearn decision tree as follows \n",
    "        # you don't have to use sklearn's fit function, \n",
    "        # but it is probably the easiest way \n",
    "\n",
    "        # w = np.ones(len(y_train))\n",
    "        # h = clone(self.base)\n",
    "        # h.fit(X_train, y_train, sample_weight=w)\n",
    "        \n",
    "        # =================================================================\n",
    "        \n",
    "        # your code here\n",
    "        \n",
    "        return self  \n",
    "            \n",
    "    def error_rate(self, y_true, y_pred, weights):\n",
    "        # =================================================================\n",
    "        # TODO \n",
    "\n",
    "        # Implement the weighted error rate\n",
    "        # =================================================================\n",
    "        # your code here\n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Adaboost prediction for new data X.\n",
    "        \n",
    "        Args:\n",
    "            X (ndarray): [n_samples x n_features] ndarray of data \n",
    "            \n",
    "        Returns: \n",
    "            yhat (ndarray): [n_samples] ndarray of predicted labels {-1,1}\n",
    "        \"\"\"\n",
    "\n",
    "        # =================================================================\n",
    "        # TODO\n",
    "        # =================================================================\n",
    "        yhat = np.zeros(X.shape[0])\n",
    "        \n",
    "        # your code here\n",
    "        \n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Computes prediction accuracy of classifier.  \n",
    "        \n",
    "        Args:\n",
    "            X (ndarray): [n_samples x n_features] ndarray of data \n",
    "            y (ndarray): [n_samples] ndarray of true labels  \n",
    "            \n",
    "        Returns: \n",
    "            Prediction accuracy (between 0.0 and 1.0).\n",
    "        \"\"\"\n",
    "        \n",
    "        # your code here\n",
    "        \n",
    "    \n",
    "    def staged_score(self, X, y):\n",
    "        \"\"\"\n",
    "        Computes the ensemble score after each iteration of boosting \n",
    "        for monitoring purposes, such as to determine the score on a \n",
    "        test set after each boost.\n",
    "        \n",
    "        Args:\n",
    "            X (ndarray): [n_samples x n_features] ndarray of data \n",
    "            y (ndarray): [n_samples] ndarray of true labels  \n",
    "            \n",
    "        Returns: \n",
    "            scores (ndarary): [n_learners] ndarray of scores \n",
    "        \"\"\"\n",
    "\n",
    "        scores = []\n",
    "        \n",
    "        \n",
    "        # your code here\n",
    "        \n",
    "        \n",
    "        return np.array(scores)        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "f6f0dfe27f6997c4b82310f1e2bc3d80",
     "grade": false,
     "grade_id": "cell-f4fce861f3b2900d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**1.a: [25 pts]** In the `AdaBoost` class above, implement the `fit` method to learn the sequence of weak learners $\\left\\{h_k({\\bf x})\\right\\}_{k=1}^K$ and corresponding coefficients $\\left\\{ \\alpha_k\\right\\}_{k=1}^K$. Note that you may use sklearn's implementation of DecisionTreeClassifier as your weak learner which allows you to pass as an optional parameter the weights associated with each training example.  An example of instantiating and training a single learner is given in the comments of the `fit` method.  \n",
    "\n",
    "Recall that the AdaBoost algorithm is as follows: \n",
    "\n",
    "`for k=1 to K:`\n",
    "\n",
    "$~~~~~~~$ `    a) Fit kth weak learner to training data with weights w`\n",
    "\n",
    "$~~~~~~~$ `    b) Computed weighted error errk for the kth weak learner` \n",
    "\n",
    "$~~~~~~~$ `    c) compute vote weight alpha[k] = 0.5 ln ((1-errk)/errk))`\n",
    "\n",
    "$~~~~~~~$ `    d) update training example weights w[i] *= exp[-alpha[k] y[i] h[k](x[i])]`\n",
    "\n",
    "$~~~~~~~$ `    e) normalize training weights so they sum to 1`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "fbce3cfc36a48b0c0b6de5563e33c340",
     "grade": false,
     "grade_id": "cell-4f4acabb25dd62f7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**1.b [5 pts]** Use the fit function to fit the Adaboost classifier with 150 base decision tree stumps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e79fd796b5a7611e0454384623d22c3d",
     "grade": false,
     "grade_id": "cell-7ab85ec815746434",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# use fit function to fit Adaboost classifier called clf with 150 base decision stumps\n",
    "# your code here\n",
    "\n",
    "# clf = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "751ec316c546d48d585fcc6d3bb46649",
     "grade": true,
     "grade_id": "cell-b1710d6b7de2374c",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# tests using the fit function to fit AdaBoost classifier with 150 base decision stumps  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "93afebff53c09067a029b980cb1b8aa2",
     "grade": false,
     "grade_id": "cell-da0bb0f15f3bce3c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**1.c [10 pts]:** After your `fit` method is working properly, implement the `predict` method to make predictions for unseen examples. You can test out the predictions in the cell below.   **Note**: Remember that AdaBoost assumes that your predictions are of the form $y \\in \\{-1, 1\\}$. \n",
    "\n",
    "Just print out your predictions on the training set in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "79a6b4f01b4fb64ee65b93fd94d86701",
     "grade": false,
     "grade_id": "cell-4fdff85a8c9e6389",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# print out predictions on the training set \n",
    "# your code here\n",
    "\n",
    "# train_predict = \n",
    "print(train_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(train_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "79d643e34a282ebb4f79f5c77caca5d4",
     "grade": true,
     "grade_id": "cell-56582ae44c7e8a04",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# tests train_predict which uses the predict method  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "cf83b9274482dde17c38aa4d65abff24",
     "grade": false,
     "grade_id": "cell-925fb725ff149f43",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**1.d [15 pts]:** Once your `predict` function is written up, you need to test the scores on the function. To do this compute the scores on the prediction in the `score` function. Use the `score` function to then complete `staged_score` to collect the scores for every boosting iterations. Plot the misclassification error for train and test sets (misclassification error = 1- score). <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "cfc23be1a573f90b7f049ffe870af2af",
     "grade": false,
     "grade_id": "cell-a3f37d45515375fe",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# plot misclassification error for train and test sets \n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "f0c4eb1ddbe65707e8652c1e37a2512b",
     "grade": false,
     "grade_id": "cell-c878ca4627ff8331",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Problem 2 [25 pts] : Building an Random Forest Classifier to classify MNIST digits 3 and 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c0cbf8227db738b9b33d5829c5b0537f",
     "grade": false,
     "grade_id": "cell-13dff04654655660",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Remember that training the random forest algorithms involves the following steps: \n",
    "\n",
    "`for k=1 to K:`\n",
    "\n",
    "$~~~~~~~$ `    a) build kth tree of depth d `\n",
    "\n",
    "$~~~~~~~$ `    b) Return the kth tree trained on the subset of dataset with the random feature splits`\n",
    "\n",
    "Predicting the classification result on new data involves returning the majority vote by all the trees in the random forest.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a1d0b838c63b5361acd732b495da08c1",
     "grade": false,
     "grade_id": "cell-fbc92bf11a3d06c1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**2.a [10 points]:** Complete the `create_tree` function to build a new tree trained on a subset of data. Within this function a decision tree classifier is built and trained on the subset of data with the subset of features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "4bb469d648e093d61609115416748177",
     "grade": false,
     "grade_id": "cell-2a9fac3a8f7c3fcf",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class RandomForest():\n",
    "    \n",
    "    def __init__(self, x, y, sample_sz, n_trees=200, n_features='sqrt', max_depth=10, min_samples_leaf=5):\n",
    "        \"\"\"\n",
    "        Create a new random forest classifier.\n",
    "        \n",
    "        Args:\n",
    "            x : Input Feature vector\n",
    "            y : Corresponding Labels\n",
    "            sample_sz : Sample size\n",
    "            n_trees : Number of trees to ensemble\n",
    "            n_features : Method to select subset of features \n",
    "            max_depth : Maximum depth of the trees in the ensemble\n",
    "            min_sample_leaf : Minimum number of samples per leaf \n",
    "        \"\"\"\n",
    "        np.random.seed(12)\n",
    "        if n_features == 'sqrt':\n",
    "            self.n_features = int(np.sqrt(x.shape[1]))\n",
    "        elif n_features == 'log2':\n",
    "            self.n_features = int(np.log2(x.shape[1]))\n",
    "        else:\n",
    "            self.n_features = n_features\n",
    "        print(self.n_features, \"sha: \",x.shape[1])  \n",
    "        self.features_set = []\n",
    "        self.x, self.y, self.sample_sz, self.max_depth, self.min_samples_leaf  = x, y, sample_sz, max_depth, min_samples_leaf\n",
    "        self.trees = [self.create_tree(i) for i in range(n_trees)]\n",
    "\n",
    "    def create_tree(self,i):\n",
    "        \"\"\"\n",
    "        create a single decision tree classifier\n",
    "        \"\"\"\n",
    "        \n",
    "        idxs = np.random.permutation(len(self.y))[:self.sample_sz]\n",
    "        idxs = np.asarray(idxs)\n",
    "\n",
    "        f_idxs = np.random.permutation(self.x.shape[1])[:self.n_features]\n",
    "        f_idxs = np.asarray(f_idxs)\n",
    "        \n",
    "        \n",
    "        if i==0:\n",
    "            self.features_set = np.array(f_idxs, ndmin=2)\n",
    "        else:\n",
    "            self.features_set = np.append(self.features_set, np.array(f_idxs,ndmin=2),axis=0)\n",
    "        \n",
    "        # TODO: build a decision tree classifier and train it with x and y that is a subset of data (use idxs and f_idxs)\n",
    "        \n",
    "        # your code here\n",
    "        \n",
    "        # clf = \n",
    "        # x, y = \n",
    "       \n",
    "    def predict(self, x):\n",
    "        \n",
    "        # TODO: create a vector of predictions  and return\n",
    "        # You will have to return the predictions of the final ensembles based on the individual trees' predicitons\n",
    "        \n",
    "        \n",
    "        # your code here\n",
    "        \n",
    "        return 0.0\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \n",
    "        # TODO: Compute the score using the predict function and true labels y\n",
    "        \n",
    "        # your code here\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "121dc7f807d5eac84d75c95d6fbc196c",
     "grade": true,
     "grade_id": "cell-9ee26343a1c4f51c",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# tests create_tree function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a2c5b415a2041f9aebcce6bc70b5921a",
     "grade": false,
     "grade_id": "cell-bd35978978ba4fc7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**2.b [15 pts]:** In this part you will have to complete three steps: \n",
    "\n",
    "1. Complete the `predict` function in RandomForest class so as to make predictions using just the features. \n",
    "2. Finally complete the RandomForest class by completing the `score`function to compute the random forest model's accuracy on any dataset. \n",
    "3. Build a random forest classifier and train it on the MNIST data to classify 3s and 8s in the cell below. Then see how the classifier performs on the test data by computing the misclassification error. (Remember: error = 1-score)\n",
    "<br>\n",
    "Answer the Peer Review questions about this section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "d385a6251746488f0497bb2634a1da8d",
     "grade": false,
     "grade_id": "cell-2d182b373095f48e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: build a random forest classifier and make predictions\n",
    "\n",
    "# your code here\n",
    "\n",
    "\n",
    "print('Misclassification error on test data : %0.3f'%pred_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
