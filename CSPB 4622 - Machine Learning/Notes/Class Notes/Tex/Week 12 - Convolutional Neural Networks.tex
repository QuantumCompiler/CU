\clearpage

\renewcommand{\ChapTitle}{Convolutional Neural Networks}
\renewcommand{\SectionTitle}{Convolutional Neural Networks}

\chapter{\ChapTitle}
\section{\SectionTitle}
\horizontalline{0}{0}

\subsection{Assigned Reading}

The reading for this week comes from \ISLRPython, \ISLRR, and \ESLII \hspace*{1pt} and is:

\begin{itemize}
    \item \textbf{ISLR Chapter 10.3: Convolutional Neural Networks}
    \item \textbf{Deep Learning Chapter 9.1: The Convolutional Operation}
    \item \textbf{Deep Learning Chapter 9.2: Motivation}
    \item \textbf{Deep Learning Chapter 9.3: Pooling}
    \item \textbf{Deep Learning Chapter 9.10: The Neuroscientific Basis for Convolutional Networks}
    \item \textbf{Deep Learning Chapter 9.11: Convolutional Networks And The History Of Deep Learning}
\end{itemize}

\subsection{Piazza}

Must post \textbf{one} dataset that aligns with weekly material.

\subsection{Lectures}

The lectures for this week are:

\begin{itemize}
    \item \lecture{https://www.youtube.com/watch?v=KreWCcaVDXI}{MLP Review}{15}
    \item \lecture{https://www.youtube.com/watch?v=X9UtC9lMUwg}{Convolution}{17}
    \item \lecture{https://www.youtube.com/watch?v=NE5GNcSfnxI}{Convolutional Layer - Hyperparameters}{6}
    \item \lecture{https://www.youtube.com/watch?v=eHcJWLXWpvs}{Advantage Of CNN}{11}
    \item \lecture{https://www.youtube.com/watch?v=kk77aFOq7sk}{Pooling Layers}{10}
    \item \lecture{https://www.youtube.com/watch?v=B3QJBtVUu4Y}{What Do Multiple Convolutional Layers Do}{6}
    \item \lecture{https://www.youtube.com/watch?v=d9JEjgTgGEQ}{Review}{10}
    \item \lecture{https://www.youtube.com/watch?v=LhGQvWmyIwk}{What Made The Success Of Deep Learning Possible}{15}
    \item \lecture{https://www.youtube.com/watch?v=UC1dTMlLYG8}{Three Basic CNN Architectures}{14}
    \item \lecture{https://www.youtube.com/watch?v=NXWqpKgXfXM}{Training Tips}{17}
    \item \lecture{https://www.youtube.com/watch?v=-4q8cZo2Mf4}{Transfer Learning}{14}
\end{itemize}

\noindent The lecture notes for this week are:

\begin{itemize}
    \item \pdflink{\LecNoteDir Convolutional Neural Networks I Lecture Notes.pdf}{Convolutional Neural Networks I Lecture Notes}
    \item \pdflink{\LecNoteDir Convolutional Neural Networks II Lecture Notes.pdf}{Convolutional Neural Networks II Lecture Notes}
\end{itemize}

\subsection{Assignments}

The assignment(s) for the week is:

\begin{itemize}
    \item \textbf{Assignment 9 - CNN}
\end{itemize}

\subsection{Quiz}

The quiz for this week is:

\begin{itemize}
    \item \pdflink{\QuizDir Quiz 12 - CNN.pdf}{Quiz 12 - CNN}
\end{itemize}

\newpage

\subsection{Chapter Summary}

The first chapter that is being covered this week is \textbf{Chapter 10: Deep Learning}. The section that is being covered from this chapter this week is \textbf{Section 10.3: Convolutional Neural Networks}.

\begin{notes}{Section 10.3: Convolutional Neural Networks}
    \subsection*{Overview}

    Convolutional Neural Networks (CNNs) are specialized neural networks designed for tasks such as image classification. They have achieved remarkable success in applications involving visual data by 
    leveraging hierarchical feature extraction mechanisms inspired by human vision. CNNs process images through layers of convolutions and pooling to detect patterns ranging from simple edges to complex 
    shapes. This section explores the architecture, convolution and pooling layers, data augmentation, and the use of pretrained CNNs.
    
    \subsubsection*{Convolution Layers}
    
    Convolution layers form the core of a CNN, identifying local patterns in images through convolution filters. Each filter scans the image to detect specific features such as edges, stripes, or corners. The 
    result is a feature map that highlights regions resembling the filter. Multiple filters are applied to capture a variety of patterns, and the weights of these filters are learned during training.
    
    \[
    \text{Convolved Image} =
    \begin{bmatrix}
        a\alpha + b\beta + d\gamma + e\delta & b\alpha + c\beta + e\gamma + f\delta \\
        d\alpha + e\beta + g\gamma + h\delta & e\alpha + f\beta + h\gamma + i\delta
    \end{bmatrix}
    \]
    
    \begin{highlight}[Convolution Layer Details]
        \begin{itemize}
            \item \textbf{Convolution Filters}: Small \(k \times k\) templates that detect specific patterns in images.
            \item \textbf{Feature Maps}: Highlight regions of the image resembling the filters.
            \item \textbf{ReLU Activation}: Often applied to the convolved image to introduce nonlinearity and sparsity.
        \end{itemize}
    \end{highlight}
    
    \subsubsection*{Pooling Layers}
    
    Pooling layers reduce the spatial dimensions of feature maps, condensing information while providing invariance to small shifts in the input. Max pooling, the most common type, selects the maximum value 
    from each non-overlapping block of pixels.
    
    \[
    \text{Max Pool Example:}
    \quad
    \begin{bmatrix}
        1 & 2 & 5 & 3 \\
        3 & 0 & 1 & 2 \\
        2 & 1 & 3 & 4 \\
        1 & 1 & 2 & 0
    \end{bmatrix}
    \to
    \begin{bmatrix}
        3 & 5 \\
        2 & 4
    \end{bmatrix}
    \]
    
    \begin{highlight}[Pooling Layer Features]
        \begin{itemize}
            \item \textbf{Dimensionality Reduction}: Decreases the size of feature maps by summarizing local information.
            \item \textbf{Shift Invariance}: Ensures robustness to small positional changes in the input.
        \end{itemize}
    \end{highlight}
    
    \subsubsection*{CNN Architecture}
    
    CNNs typically alternate between convolution and pooling layers, with multiple convolution filters capturing increasingly abstract patterns. Pooling layers reduce feature map dimensions after every few 
    convolution layers. Once the feature maps are sufficiently condensed, they are flattened and passed through fully connected layers to make the final prediction.
    
    \begin{highlight}[CNN Architectural Features]
        \begin{itemize}
            \item \textbf{Feature Hierarchy}: Lower layers detect simple patterns (e.g., edges), while deeper layers capture complex features (e.g., shapes).
            \item \textbf{Fully Connected Layers}: Combine the outputs of convolutional layers to make class predictions.
            \item \textbf{Softmax Output}: Produces class probabilities for classification tasks.
        \end{itemize}
    \end{highlight}
    
    \subsubsection*{Data Augmentation}
    
    Data augmentation artificially expands the training dataset by applying transformations such as rotations, flips, and shifts to the original images. This method acts as a form of regularization, reducing 
    overfitting and improving generalization by exposing the model to a wider variety of data.
    
    \begin{highlight}[Benefits of Data Augmentation]
        \begin{itemize}
            \item \textbf{Increased Training Data}: Generates diverse versions of the original dataset.
            \item \textbf{Regularization Effect}: Helps prevent overfitting by adding variability to the training set.
            \item \textbf{Efficiency}: Augmentations are applied on-the-fly during training, minimizing storage requirements.
        \end{itemize}
    \end{highlight}
    
    \subsubsection*{Pretrained CNNs and Transfer Learning}
    
    Pretrained CNNs like ResNet50, trained on large datasets such as ImageNet, can be adapted to new tasks with smaller datasets through transfer learning. This approach involves freezing the learned 
    convolution filters and training only the final fully connected layers for the specific task.
    
    \begin{highlight}[Pretrained CNNs and Transfer Learning]
        \begin{itemize}
            \item \textbf{Feature Reuse}: Use convolution filters learned on large datasets for new tasks.
            \item \textbf{Weight Freezing}: Retain pretrained weights for early layers while fine-tuning later layers.
            \item \textbf{Reduced Data Requirements}: Requires fewer labeled examples for training.
        \end{itemize}
    \end{highlight}
\end{notes}

The next chapter that is being covered is \textbf{Chapter 9: Convolutional Networks}.  The first section that is being covered from this chapter this week is \textbf{Section 9.1: The Convolutional Operation}.

\begin{notes}{Section 9.1: The Convolutional Operation}
    \subsection*{Overview}

    The convolution operation is a fundamental building block of convolutional neural networks (CNNs). It allows efficient feature extraction from data with grid-like structures, such as time-series and 
    images. This section introduces the concept of convolution, its mathematical formulation, and its application in CNNs to derive feature maps from inputs.
    
    \subsubsection*{Mathematical Definition of Convolution}
    
    Convolution is a mathematical operation that combines two functions, typically denoted as $x$ (the input) and $w$ (the kernel). The result is a third function $s$, representing the output or 
    feature map. For real-valued functions, convolution is defined as:
    
    \[
    s(t) = (x \ast w)(t) = \int x(a) w(t - a) \, da.
    \]
    
    In machine learning, where discrete inputs and kernels are common, the discrete convolution is expressed as:
    
    \[
    s(t) = (x \ast w)(t) = \sum_{a=-\infty}^{\infty} x(a) w(t - a).
    \]
    
    \begin{highlight}[Key Characteristics of Convolution]
        \begin{itemize}
            \item \textbf{Input and Kernel}: The input $x$ represents data (e.g., an image), while $w$ is a filter that scans the input for patterns.
            \item \textbf{Feature Map}: The output $s$ encodes the presence of patterns detected by the kernel at different locations.
            \item \textbf{Weight Sharing}: The same kernel is applied across all positions in the input, significantly reducing the number of parameters.
        \end{itemize}
    \end{highlight}
    
    \subsubsection*{Discrete Convolution in CNNs}
    
    In CNNs, convolution is performed over multidimensional inputs such as images. For a two-dimensional input $I$ and a two-dimensional kernel $K$, the convolution is:
    
    \[
    S(i, j) = (I \ast K)(i, j) = \sum_{m} \sum_{n} I(m, n) K(i - m, j - n).
    \]
    
    This operation results in a two-dimensional feature map $S$, where each element represents the activation of a local feature detected by $K$ at position $(i, j)$.
    
    \begin{highlight}[Properties of Convolution in CNNs]
        \begin{itemize}
            \item \textbf{Sparse Connectivity}: Each output is connected only to a small region (receptive field) of the input, reducing computational cost.
            \item \textbf{Parameter Sharing}: A single kernel is used across all spatial locations, improving efficiency and generalization.
            \item \textbf{Translation Equivariance}: Features detected by the kernel remain consistent across spatial translations of the input.
        \end{itemize}
    \end{highlight}
    
    \subsubsection*{Implementation and Efficiency}
    
    In practice, the convolution operation is implemented efficiently by exploiting its sparse and structured nature. The kernel is typically much smaller than the input, enabling significant reductions in 
    computational complexity and memory usage compared to fully connected operations. Moreover, modern implementations leverage matrix representations such as Toeplitz matrices for efficient computation.
    
    \begin{highlight}[Efficiency of Convolution]
        \begin{itemize}
            \item \textbf{Sparse Matrix Representation}: Convolution is equivalent to multiplication by a sparse matrix, reducing storage and computation costs.
            \item \textbf{Multidimensional Extensions}: Convolutions are generalized to higher dimensions for data such as color images and videos.
            \item \textbf{Scalable Operations}: Advanced implementations support convolution over large-scale inputs and kernels.
        \end{itemize}
    \end{highlight}
    
    \subsubsection*{Applications and Intuition}
    
    Convolution is widely used in CNNs for tasks like image recognition, where it detects low-level patterns (e.g., edges) in early layers and combines them into higher-level features (e.g., shapes) in deeper 
    layers. Its inherent translation equivariance and parameter sharing make it particularly suited for grid-structured data.
    
    \begin{highlight}[Applications in Machine Learning]
        \begin{itemize}
            \item \textbf{Feature Extraction}: Convolution identifies spatial patterns and hierarchical features in images.
            \item \textbf{Efficiency}: Reduces the number of parameters compared to fully connected networks.
            \item \textbf{Broad Use Cases}: Employed in diverse domains, including image classification, object detection, and time-series analysis.
        \end{itemize}
    \end{highlight}
\end{notes}

The next section that is being covered from this chapter this week is \textbf{Section 9.2: Motivation}.

\begin{notes}{Section 9.2: Motivation}
    \subsection*{Overview}

    The convolutional operation leverages three critical principles—sparse interactions, parameter sharing, and equivariant representations—that make convolutional neural networks (CNNs) highly efficient 
    and effective for tasks involving structured data like images and time-series. This section explores the motivation behind using these principles and highlights how convolution reduces computational 
    and memory requirements while enhancing generalization.
    
    \subsubsection*{Sparse Interactions}
    
    Sparse interactions, also referred to as sparse connectivity, mean that each output unit in a CNN interacts with only a small, localized subset of the input units. This is achieved by using small kernels 
    compared to the input size. For example, detecting features in an image requires kernels spanning only tens or hundreds of pixels, even though the input image may contain millions of pixels.
    
    \begin{highlight}[Benefits of Sparse Interactions]
        \begin{itemize}
            \item \textbf{Efficiency}: Sparse connectivity reduces the number of parameters, improving statistical and computational efficiency.
            \item \textbf{Fewer Operations}: Significantly decreases the number of calculations needed to generate outputs.
            \item \textbf{Local Feature Detection}: Kernels focus on localized patterns (e.g., edges) that can later combine into more complex structures.
        \end{itemize}
    \end{highlight}
    
    \subsubsection*{Parameter Sharing}
    
    Parameter sharing in CNNs refers to reusing the same parameters (kernel weights) across multiple input regions. Unlike traditional neural networks, where each weight represents a unique interaction, CNNs 
    use shared parameters to scan the entire input for similar patterns. For example, a single kernel can detect edges at any location in an image, allowing the model to generalize efficiently.
    
    \begin{highlight}[Advantages of Parameter Sharing]
        \begin{itemize}
            \item \textbf{Reduced Memory Requirements}: Fewer unique parameters lead to lower storage costs.
            \item \textbf{Generalization}: Enables the model to learn patterns that apply across the input, independent of position.
            \item \textbf{Efficiency}: Simplifies training by reducing the number of parameters to optimize.
        \end{itemize}
    \end{highlight}
    
    \subsubsection*{Equivariant Representations}
    
    An operation is equivariant if its output changes predictably when the input is transformed. For instance, if the input shifts spatially, the output of a convolution also shifts by the same amount. 
    Convolutional layers naturally exhibit equivariance to translation, making them well-suited for tasks like image recognition, where spatial relationships are crucial.
    
    \begin{highlight}[Properties of Equivariant Representations]
        \begin{itemize}
            \item \textbf{Translation Equivariance}: Ensures consistent detection of features regardless of their position in the input.
            \item \textbf{Robustness}: Helps maintain meaningful feature representations under transformations.
            \item \textbf{Application in Hierarchies}: Supports hierarchical feature extraction in deeper CNN layers.
        \end{itemize}
    \end{highlight}
    
    \subsubsection*{Variable Input Size Handling}
    
    Another key advantage of convolution is its ability to handle inputs of varying sizes. Unlike fully connected layers, which depend on fixed input dimensions, convolutional operations adapt to inputs 
    of different sizes, making them versatile for real-world data applications.
    
    \begin{highlight}[Handling Variable Input Sizes]
        \begin{itemize}
            \item \textbf{Adaptability}: Supports flexible input dimensions without reconfiguring the network.
            \item \textbf{Efficiency}: Simplifies preprocessing steps, allowing for seamless handling of diverse datasets.
        \end{itemize}
    \end{highlight}
\end{notes}

The next section that is being covered from this chapter this week is \textbf{Section 9.3: Pooling}.

\begin{notes}{Section 9.3: Pooling}
    \subsection*{Overview}

    Pooling is a critical component in convolutional neural networks (CNNs) that simplifies feature maps by summarizing local regions into representative values. This process reduces the spatial dimensions 
    of data, improves computational efficiency, and introduces invariance to small translations in the input. Pooling is commonly used after convolution and nonlinearity stages to refine the feature map 
    representation.
    
    \subsubsection*{Pooling Functions}
    
    Pooling functions replace the output at a specific location with a summary statistic computed from a local neighborhood. The most common pooling functions include:
    
    \begin{itemize}
        \item **Max Pooling**: Selects the maximum value in the local region, emphasizing the most salient features.
        \item **Average Pooling**: Computes the average value within the region, producing a smoother representation.
        \item **L2 Norm Pooling**: Calculates the square root of the sum of squares of values in the local region.
    \end{itemize}
    
    \begin{highlight}[Key Pooling Functions]
        \begin{itemize}
            \item \textbf{Max Pooling}: Captures the strongest feature activation in a region.
            \item \textbf{Average Pooling}: Provides a balanced summary of local activations.
            \item \textbf{L2 Norm Pooling}: Retains information about the magnitude of features.
        \end{itemize}
    \end{highlight}
    
    \subsubsection*{Benefits of Pooling}
    
    Pooling introduces several advantages that enhance the performance and scalability of CNNs:
    
    \begin{highlight}[Advantages of Pooling]
        \begin{itemize}
            \item \textbf{Dimensionality Reduction}: Reduces the size of feature maps, decreasing the computational and memory load.
            \item \textbf{Translation Invariance}: Ensures that small translations in the input do not significantly affect the output, improving robustness.
            \item \textbf{Regularization Effect}: Combats overfitting by simplifying feature representations.
        \end{itemize}
    \end{highlight}
    
    \subsubsection*{Pooling Implementation and Invariance}
    
    The pooling process involves sliding a window across the feature map and applying the pooling function to each local region. This approach introduces approximate invariance to small translations in the input, 
    which is useful in applications where precise feature location is less important than its presence.
    
    \begin{highlight}[Pooling and Invariance]
        \begin{itemize}
            \item \textbf{Local Translation Invariance}: Maintains consistency of output when features shift slightly within the input.
            \item \textbf{Application Example}: Detects facial features without requiring pixel-perfect accuracy for their positions.
        \end{itemize}
    \end{highlight}
    
    \subsubsection*{Dynamic and Learned Pooling Strategies}
    
    While fixed pooling strategies like max or average pooling are common, dynamic approaches offer additional flexibility:
    
    \begin{itemize}
        \item **Dynamic Pooling**: Uses clustering algorithms to define pooling regions dynamically for each input.
        \item **Learned Pooling**: Optimizes pooling regions based on training data, applying a uniform structure across all inputs.
    \end{itemize}
    
    \begin{highlight}[Dynamic and Learned Pooling]
        \begin{itemize}
            \item \textbf{Dynamic Pooling}: Adapts pooling regions to specific input patterns, enhancing representation flexibility.
            \item \textbf{Learned Pooling}: Optimizes pooling strategies during training for consistent outputs across inputs.
        \end{itemize}
    \end{highlight}
    
    \subsubsection*{Trade-Offs in Pooling Design}
    
    Pooling introduces trade-offs, particularly when spatial precision is critical. Overuse of pooling may result in underfitting if the network overly simplifies feature representations, leading to information 
    loss about the precise location of features.
    
    \begin{highlight}[Trade-Offs in Pooling]
        \begin{itemize}
            \item \textbf{Underfitting Risk}: Excessive pooling can degrade the model’s ability to learn precise spatial relationships.
            \item \textbf{Task-Dependent Design}: Pooling should be applied judiciously depending on the task requirements, balancing invariance and spatial precision.
        \end{itemize}
    \end{highlight}
\end{notes}

The next section that is being covered from this chapter this week is \textbf{Section 9.10: The Neuroscientific Basis for Convolutional Networks}.

\begin{notes}{Section 9.10: The Neuroscientific Basis for Convolutional Networks}
    \subsection*{Overview}

    Convolutional neural networks (CNNs) are among the most successful biologically inspired architectures in artificial intelligence. Their design draws heavily on principles of the mammalian visual system, 
    particularly insights from the primary visual cortex (V1). This section explores the neuroscientific findings that influenced CNNs, focusing on the hierarchical structure of feature detection, simple 
    and complex cell mechanisms, and the concept of "grandmother cells."
    
    \subsubsection*{Neuroscientific Foundations}
    
    The origins of convolutional networks are rooted in the work of neurophysiologists David Hubel and Torsten Wiesel, who studied the mammalian visual system. They discovered that neurons in the early visual 
    cortex responded selectively to specific patterns, such as oriented edges, within their receptive fields. These findings provided a blueprint for the hierarchical feature extraction mechanisms in CNNs.
    
    \begin{highlight}[Key Neuroscientific Insights]
        \begin{itemize}
            \item \textbf{Receptive Fields}: Neurons in V1 respond to localized regions of the input, a principle mirrored by convolutional filters.
            \item \textbf{Selective Activation}: Neurons are activated by specific patterns, such as oriented bars, which inspired the design of feature detectors in CNNs.
            \item \textbf{Hierarchical Processing}: Early layers in V1 detect simple patterns, while deeper layers combine them into complex features, similar to CNN architectures.
        \end{itemize}
    \end{highlight}
    
    \subsubsection*{Structure of the Primary Visual Cortex (V1)}
    
    The V1 region of the brain performs advanced visual processing by organizing neurons into spatial maps. These maps reflect the structure of the retina, maintaining a two-dimensional representation of the 
    visual field. Three key properties of V1 inspired the design of CNNs:
    
    \begin{highlight}[Features of V1 Relevant to CNNs]
        \begin{itemize}
            \item \textbf{Spatial Mapping}: V1 preserves the spatial structure of the input, akin to the grid-based organization of convolutional filters.
            \item \textbf{Simple Cells}: Respond linearly to patterns in localized receptive fields, similar to CNN convolutional units.
            \item \textbf{Complex Cells}: Exhibit invariance to small translations and other transformations, inspiring pooling mechanisms in CNNs.
        \end{itemize}
    \end{highlight}
    
    \subsubsection*{The Grandmother Cell Hypothesis}
    
    Deeper layers in the visual cortex aggregate features from earlier layers to form complex, invariant representations. This culminates in "grandmother cells," which respond to specific high-level concepts 
    (e.g., the face of a known individual) regardless of transformations like lighting or orientation. While CNNs emulate this hierarchical abstraction, they lack the semantic richness of biological systems.
    
    \begin{highlight}[The Grandmother Cell Analogy]
        \begin{itemize}
            \item \textbf{Hierarchical Abstraction}: Layers progressively combine features, from edges to complex objects.
            \item \textbf{Invariant Representation}: Similar to complex cells, deeper units in CNNs remain invariant to transformations of the input.
            \item \textbf{Biological Difference}: Unlike CNNs, the brain integrates multimodal inputs and contextual information.
        \end{itemize}
    \end{highlight}
    
    \subsubsection*{Applications of Neuroscience in CNN Design}
    
    Although neuroscientific insights influenced the foundational structure of CNNs, modern training methods (e.g., backpropagation) and architectural advancements are largely computational innovations. 
    Future research aims to incorporate biologically inspired mechanisms like feedback loops and foveation to enhance CNN performance.
    
    \begin{highlight}[Influence and Future Directions]
        \begin{itemize}
            \item \textbf{Parameter Sharing}: Inspired by the shared response properties of neurons across the visual field.
            \item \textbf{Feedback Mechanisms}: An active area of research to emulate the top-down connections in biological vision systems.
            \item \textbf{Multisensory Integration}: Exploring how to combine modalities (e.g., vision and sound) as the brain does.
        \end{itemize}
    \end{highlight}
\end{notes}

The last section that is being covered from this chapter this week is \textbf{Section 9.11: Convolutional Networks And The History Of Deep Learning}.

\begin{notes}{Section 9.11: Convolutional Networks And The History Of Deep Learning}
    \subsection*{Overview}

    Convolutional neural networks (CNNs) have played a pivotal role in the evolution of deep learning, serving as one of the earliest success stories of biologically inspired artificial intelligence. They have 
    been instrumental in solving key commercial problems and pioneering advancements in deep learning applications. This section outlines the historical development of CNNs, their commercial adoption, and 
    their impact on the broader field of machine learning.
    
    \subsubsection*{Historical Development of CNNs}
    
    CNNs were among the first neural networks to demonstrate the viability of deep models. Inspired by neuroscience, they incorporated hierarchical feature extraction mechanisms and were trained successfully 
    using backpropagation. Early breakthroughs include their application to Optical Character Recognition (OCR) and handwriting recognition.
    
    \begin{highlight}[Milestones in CNN Development]
        \begin{itemize}
            \item \textbf{Neuroscientific Inspiration}: CNNs were inspired by the mammalian visual cortex, particularly the hierarchical organization of simple and complex cells.
            \item \textbf{Early Applications}: In the 1990s, CNNs were used to read over 10% of U.S. checks, demonstrating commercial viability.
            \item \textbf{Backpropagation Success}: CNNs were some of the first networks successfully trained using backpropagation, paving the way for future deep learning models.
        \end{itemize}
    \end{highlight}
    
    \subsubsection*{Impact on Competitions and Commercial Use}
    
    CNNs gained widespread attention through their dominance in machine learning competitions. A major turning point was the 2012 ImageNet challenge, where a deep CNN achieved unprecedented accuracy. This 
    success spurred significant interest and investment in deep learning technologies.
    
    \begin{highlight}[Key Achievements in Competitions]
        \begin{itemize}
            \item \textbf{ImageNet 2012}: CNNs revolutionized computer vision by significantly outperforming traditional methods.
            \item \textbf{Pre-ImageNet Wins}: CNNs had earlier successes in smaller contests, though their impact was less pronounced at the time.
            \item \textbf{Commercial Relevance}: CNNs were quickly adopted for tasks such as image classification, object detection, and OCR.
        \end{itemize}
    \end{highlight}
    
    \subsubsection*{Barriers to Early Adoption}
    
    The initial reluctance to embrace neural networks, including CNNs, stemmed from limited computational resources and skepticism among researchers. However, CNNs stood out due to their efficiency and 
    performance on early applications, helping to sustain interest in neural networks during periods of broader disillusionment.
    
    \begin{highlight}[Challenges and Perseverance]
        \begin{itemize}
            \item \textbf{Computational Constraints}: Early hardware limitations made training large networks infeasible.
            \item \textbf{Psychological Barriers}: Widespread skepticism about neural networks hindered their adoption.
            \item \textbf{Sustained Relevance}: CNNs' success in real-world applications ensured continued exploration and development.
        \end{itemize}
    \end{highlight}
    
    \subsubsection*{Broader Significance in Deep Learning}
    
    CNNs exemplify how specialization can enhance neural network performance. By leveraging grid-like input data structures, CNNs introduced scalable architectures capable of solving high-dimensional 
    problems. Their success laid the groundwork for modern deep learning, influencing the design of other specialized networks like recurrent neural networks (RNNs).
    
    \begin{highlight}[Legacy of CNNs in Deep Learning]
        \begin{itemize}
            \item \textbf{Specialization for Grid Data}: Demonstrated the value of tailoring architectures to data structures.
            \item \textbf{Scalable Architectures}: Enabled large-scale models, essential for modern applications.
            \item \textbf{Influence on Other Models}: Inspired subsequent advances, such as RNNs for sequential data.
        \end{itemize}
    \end{highlight}
\end{notes}