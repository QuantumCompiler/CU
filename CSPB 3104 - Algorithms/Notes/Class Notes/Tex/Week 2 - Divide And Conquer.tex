\clearpage

\renewcommand{\ChapTitle}{Divide And Conquer}
\renewcommand{\SectionTitle}{Divide And Conquer}

\chapter{\ChapTitle}
\section{\SectionTitle}
\horizontalline{0}{0}

\subsection{Assigned Reading}

The reading assignment for this week is from, \Textbook:

\begin{itemize}
    \item \textbf{Chapter 4.1 - The Maximum-Subarray Problem}
    \item \textbf{Chapter 4.2 - Strassen's Algorithm For Matrix Multiplication}
    \item \textbf{Chapter 4.3 - The Substitution Method For Solving Recurrences}
    \item \textbf{Chapter 4.4 - The Recursion-Tree Method For Solving Recurrences}
\end{itemize}

\subsection{Lectures}

The lecture videos for this week are:

\begin{itemize}
    \item \lecture{https://www.youtube.com/watch?v=Gb8xmmH7o1c}{Divide And Conquer: Mergesort}{38}
    \item \lecture{https://www.youtube.com/watch?v=TVIVFWWieO0}{Analysis Of Mergesort}{24}
    \item \lecture{https://www.youtube.com/watch?v=UlA39DpObJ4}{Max Subarray Problem}{24}
    \item \lecture{https://www.youtube.com/watch?v=Scvp0jIuWUY}{Karatsuba Multiplication Algorithm}{40}
    \item \lecture{https://www.youtube.com/watch?v=R0mdPgU3S7E}{Solving Recurrences: Master Method}{28}
\end{itemize}

\subsection{Assignments}

The assignment for this week is:

\begin{itemize}
    \item \href{https://github.com/QuantumCompiler/CU/tree/main/CSPB%203104%20-%20Algorithms/Assignments/Problem%20Sets/Problem%20Set%202%20-%20Divide%20And%20Conquer}{Problem Set 2 - Divide And Conquer}
\end{itemize}

\subsection{Quiz}

The quizzes for this week are:

\begin{itemize}
    \item \pdflink{\QuizDir Quiz 2 - Divide And Conquer.pdf}{Quiz 2 - Divide And Conquer}
\end{itemize}

\newpage

\subsection{Chapter Summary}

The chapter for this week is \textbf{Chapter 4: Divide And Conquer}. The first section from this chapter is \textbf{Section 4.1: The Maximum Sub-Array Problem}.

\begin{notes}{Section 4.1: The Maximum Sub-Array Problem}
    \subsection*{Overview}

    The Maximum Sub-Array Problem involves finding the contiguous sub-array within a one-dimensional array of numbers which has the largest sum. It's a classic example of a divide-and-conquer approach 
    in algorithm design. \vspace*{1em}

    \subsection*{Problem Statement}

    Given an array of integers (both positive and negative), the goal is to find a contiguous sub-array that maximizes the sum of its elements. \vspace*{1em}

    \subsection*{Divide and Conquer Strategy}

    \begin{itemize}
        \item \textbf{Divide}: The array is divided into two halves.
        \item \textbf{Conquer}: Recursively find the maximum sub-array in each half.
        \item \textbf{Combine}: Determine the maximum sub-array that crosses the midpoint and combine the results of the three cases (left, right, and crossing the midpoint) to find the overall maximum.
    \end{itemize}

    \subsection*{Key Components}

    \begin{itemize}
        \item \textbf{Crossing Sub-Array}: A crucial step is finding the maximum sub-array that crosses the midpoint. This is done by looking for the maximum sub-array ending at the midpoint from the 
        left side and starting from the midpoint on the right side, then combining these two.
        \item \textbf{Recursive Nature}: The problem is recursively solved for the left and right halves, and the solutions are merged.
    \end{itemize}

    \subsection*{Algorithmic Steps}

    \begin{enumerate}
        \item \textbf{Base Case}: If the array has only one element, return this element as the maximum sum.
        \item \textbf{Divide the Array}: Find the midpoint of the array.
        \item \textbf{Recursive Calls}:
        \begin{itemize}
            \item Find the maximum sub-array in the left half.
            \item Find the maximum sub-array in the right half.
        \end{itemize}
        \item \textbf{Find the Crossing Maximum Sub-Array}: Calculate the maximum sub-array that crosses the midpoint.
        \item \textbf{Combine Results}: The maximum of these three values (left, right, crossing) is the solution.
    \end{enumerate}

    \subsection*{Complexity Analysis}

    The time complexity of this algorithm is $\mathcal{O}(n\log{(n)})$, where $n$ is the number of elements in the array. This is more efficient than the $\mathcal{O}(n^{2})$ complexity of the 
    brute-force approach.
\end{notes}

The next section from this chapter is \textbf{Section 4.2: Strassen's Algorithm For Matrix Multiplication}.

\begin{notes}{Section 4.2: Strassen's Algorithm For Matrix Multiplication}
    \subsection*{Overview}

    Strassen's Algorithm is an innovative approach to matrix multiplication. It's known for reducing the computational complexity compared to the standard matrix multiplication technique. \vspace*{1em}

    \subsection*{Problem Statement}

    \begin{itemize}
        \item Standard matrix multiplication of two $n \times n$ matrices has a time complexity of $\mathcal{O}(n^{3})$.
        \item Strassen's Algorithm improves this, especially for large matrices.
    \end{itemize}

    \subsection*{Divide And Conquer Approach}

    \begin{itemize}
        \item \textbf{Divide}: The algorithm divides each matrix into four $\frac{n}{2} \times \frac{n}{2}$ sub-matrices.
        \item \textbf{Conquer}: It recursively multiplies these smaller matrices.
        \item \textbf{Combine}: Utilizes fewer multiplications (7 instead of 8 in standard approach) to combine these sub-matrices into the final product.
    \end{itemize}

    \subsection*{Strassen's Formulae}

    \begin{itemize}
        \item The algorithm uses 7 multiplication operations (M1 to M7) on combined elements of the sub-matrices, reducing the number of multiplications needed.
        \item These 7 products are then used to calculate the entries of the final $n \times n$ matrices.
    \end{itemize}

    \subsection*{Key Concepts}

    \begin{itemize}
        \item \textbf{Sub-Matrix Multiplication}: Instead of multiplying matrices directly, the algorithm operates on smaller matrices.
        \item \textbf{Addition and Subtraction of Matrices}: Employed extensively to prepare the matrices for the 7 multiplication operations.
    \end{itemize}

    \subsection*{Complexity Analysis}

    \begin{itemize}
        \item Time Complexity: Approximately $\mathcal{O}(n^{2.81})$, which is lower than the $\mathcal{O}(n^{3})$ of the standard method.
        \item This is due to the master theorem in the analysis of divide-and-conquer algorithms.
    \end{itemize}

    \subsection*{Limitations And Extensions}

    \begin{itemize}
        \item Overhead for smaller matrices can outweigh its theoretical efficiency.
        \item The algorithm has paved the way for further research into even more efficient matrix multiplication algorithms.
    \end{itemize}
\end{notes}

The next section from this chapter is \textbf{Section 4.3: The Substitution Method For Solving Recurrences}.

\begin{notes}{Section 4.3: The Substitution Method For Solving Recurrences}
    \subsection*{Overview}

    The Substitution Method, also known as the 'Guess-and-Check' method, is used to solve recurrence relations, which often arise in the analysis of recursive algorithms. It involves making a guess 
    about the solution's form and then proving it by induction. \vspace*{1em}

    \subsection*{Problem Statement}

    \begin{itemize}
        \item Recurrence relations define a sequence of values using recursion.
        \item The method helps in determining the time complexity of recursive algorithms by solving these relations.
    \end{itemize}

    \subsection*{Basic Steps}

    \begin{itemize}
        \item \textbf{Guess the Form}: Start with a hypothesis about the form of the solution (usually based on experience or pattern observation).
        \item \textbf{Verify by Induction}: Use mathematical induction to prove that the guess is correct.
        \item \textbf{Refine if Necessary}: If the initial guess doesn't hold, refine it and attempt the proof again.
    \end{itemize}

    \subsection*{Process Of Induction}

    \begin{itemize}
        \item \textbf{Base Case}: Verify that the solution holds for the initial term(s) of the sequence.
        \item \textbf{Inductive Step}: Assume the solution holds for a general term (say, the $n$-th term) and then prove it for the next term $(n + 1)$.
    \end{itemize}

    \subsection*{Common Recurrence Types}

    \begin{itemize}
        \item Linear Recurrences (e.g. $T(n) = T(n - 1) + n$).
        \item Divide-and-Conquer Recurrences (e.g. $T(n) = 2T(n / 2) + n$).
    \end{itemize}

    \subsection*{Advantages And Challenges}

    \begin{itemize}
        \item \textbf{Advantage}: Allows for a tailored approach for different recurrences.
        \item \textbf{Challenge}: Finding the correct guess can be non-trivial and requires insight.
    \end{itemize}

    \subsection*{Example}

    For a recurrence like $T(n) = 2T(\lfloor n / 2 \rfloor) + n$, one might guess that $T(n) = \mathcal{O}(n \log{(n)})$ and then use induction to prove it.
\end{notes}

The next section from this chapter is \textbf{Section 4.4: The Recursion-Tree Method For Solving Recurrences}.

\begin{notes}{Section 4.4: The Recursion-Tree Method For Solving Recurrences}
    \subsection*{Overview}

    The Recursion-Tree Method is a visual and intuitive approach to solving recurrence relations. It involves drawing a tree to represent the recursive calls and their costs, helping to visualize the 
    structure of the recursion. \vspace*{1em}

    \subsection*{Purpose And Application}

    \begin{itemize}
        \item Used to determine the time complexity of recursive algorithms.
        \item Particularly useful for recurrences arising from divide-and-conquer algorithms.
    \end{itemize}

    \subsection*{Steps In The Method}

    \begin{itemize}
        \item \textbf{Draw the Recursion Tree}: Each node represents a recursive call, and the tree shows how the problem is divided and conquered.
        \item \textbf{Calculate Cost at Each Level}: Assign costs to each level of the tree based on the recurrence relation.
        \item \textbf{Summarize the Total Cost}: Add up the costs across all levels of the tree to find the total cost of the algorithm.
    \end{itemize}

    \subsection*{Tree Structure}

    \begin{itemize}
        \item The root represents the initial problem.
        \item Child nodes represent sub-problems created by recursive calls.
        \item The depth of the tree correlates with the number of recursive calls.
    \end{itemize}

    \subsection*{Analyzing The Tree}

    \begin{itemize}
        \item Determine the number of levels in the tree.
        \item Calculate the total number of nodes or the total cost at each level.
        \item Sum these costs to estimate the overall complexity of the recurrence.
    \end{itemize}

    \subsection*{Example Of Analysis}

    \begin{itemize}
        \item For a recurrence like $T(n) = 2T(n / 2) + n$, the tree will have 2 branches at each level, and the cost at each level will be proportional to $n$.
        \item The depth of the tree is typically $\log{(n)}$, leading to a total cost of $\mathcal{O}(n \log{(n)})$.
    \end{itemize}

    \subsection*{Advantages And Limitations}

    \begin{itemize}
        \item \textbf{Advantages}: Intuitive and visual; helpful in understanding the pattern of recursive calls.
        \item \textbf{Limitations}: Can be complex for some recurrences; less precise than mathematical methods.
    \end{itemize}
\end{notes}

The last section from this chapter is \textbf{Section 4.5: The Master Method For Solving Recurrences}.

\begin{notes}{Section 4.5: The Master Method For Solving Recurrences}
    \subsection*{Overview}

    The Master Method provides a cookbook-style solution for solving recurrences, particularly those arising from divide-and-conquer algorithms. It's a formulaic approach that, when applicable, quickly 
    gives the asymptotic behavior of the recurrence. \vspace*{1em}

    \subsection*{Applicability}

    Best suited for recurrences of the form $T(n) = aT(n / b) + f(n)$, where:

    \begin{itemize}
        \item $a$ is the number of sub-problems in the recursion.
        \item $n / b$ is the size of each sub-problem.
        \item $f(n)$ is the cost of the work done outside the recursive calls.
    \end{itemize}

    \subsection*{The Three Cases Of The Master Method}

    \begin{itemize}
        \item \textbf{Case 1}: If $f(n) = \mathcal{O}(n^{c})$ where $c < \log_{b}{(a)}$, then $T(n) = \Theta(n^{\log_{b}{(a)}})$.
        \item \textbf{Case 2}: If $f(n) = \Theta(n^{c})$ where $c = \log_{b}{(a)}$, then $T(n) = \Theta(n^{c}\log{(n)})$.
        \item \textbf{Case 3}: If $f(n) = \Omega(n^{c})$ where $c > \log_{b}{(a)}$ and if $af(n / b) \leq kf(n)$ for some constant $k < 1$ and sufficiently large $n$, then $T(n) = \Theta(f(n))$.
    \end{itemize}

    \subsection*{How to Use the Master Method}

    \begin{itemize}
        \item Identify $a$, $b$, and $f(n)$ in the given recurrence.
        \item Determine which of the three cases applies.
        \item Apply the corresponding formula to find the solution.
    \end{itemize}

    \subsection*{Examples}

    \begin{itemize}
        \item \textbf{Merge Sort}: For $T(n) = 2T(n / 2) + n$, it falls under Case 2 (since $f(n) = n$ and $c = \log_{2}{(2)} = 1$), leading to $T(n) = \Theta(n \log{(n)})$.
        \item \textbf{Binary Search}: For $T(n) = T(n / 2) + 1$, it falls under Case 2, giving $T(n) = \Theta(\log{(n)})$.
    \end{itemize}

    \subsection*{Advantages And Limitations}

    \begin{itemize}
        \item \textbf{Advantages}: Simplifies the process of solving recurrences; provides quick and direct answers.
        \item \textbf{Limitations}: Not applicable to all types of recurrences; specific conditions must be met.
    \end{itemize}
\end{notes}