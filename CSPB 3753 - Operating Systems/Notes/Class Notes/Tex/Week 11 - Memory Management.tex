\clearpage

\renewcommand{\ChapTitle}{Memory Management}
\renewcommand{\SectionTitle}{Memory Management}

\chapter{\ChapTitle}
\section{\SectionTitle}
\horizontalline{0}{0}

\subsection{Assigned Reading}

The reading for this week comes from the \href{https://learn.zybooks.com/zybook/COLORADOCSPB3753KnoxFall2024}{Zybooks} for the week is:

\begin{itemize}
    \item \textbf{Chapter 9: Main Memory}
\end{itemize}

\subsection{Lectures}

The lecture videos for the week are:

\begin{itemize}
    \item \lecture{https://www.youtube.com/watch?v=bW_GxorbAGk}{Memory Management}{28}
    \item \lecture{https://www.youtube.com/watch?v=sw3-9m3bhEM}{Address Binding}{14}
    \item \lecture{https://www.youtube.com/watch?v=PbABBwyskPM}{Swapping, Fragmentation And Segmentation}{26}
    \item \lecture{https://www.youtube.com/watch?v=yKG6_7ANMqY}{Paging}{29}
\end{itemize}

\noindent The lecture notes for the week are:

\begin{itemize}
    \item \pdflink{\LecNoteDir Address Binding Lecture Notes.pdf}{Address Binding Lecture Notes}
    \item \pdflink{\LecNoteDir Beladys Anomoly Lecture Notes.pdf}{Beladys Anomoly Lecture Notess}
    \item \pdflink{\LecNoteDir Frame Allocation Lecture Notes.pdf}{Frame Allocation Lecture Notes}
    \item \pdflink{\LecNoteDir Memory Management Lecture Notes.pdf}{Memory Management Lecture Notes}
    \item \pdflink{\LecNoteDir Memory Mapped Files Lecture Notes.pdf}{Memory Mapped Files Lecture Notes}
    \item \pdflink{\LecNoteDir Page Replacement Policies Lecture Notes.pdf}{Page Replacement Policies Lecture Notes}
    \item \pdflink{\LecNoteDir Paging Lecture Notes.pdf}{Paging Lecture Notes}
    \item \pdflink{\LecNoteDir Swapping Fragmentation Segmentation Lecture Notes.pdf}{Swapping Fragmentation Segmentation Lecture Notes}
    \item \pdflink{\LecNoteDir Thrashing Lecture Notes.pdf}{Thrashing Lecture Notes}
    \item \pdflink{\LecNoteDir Virtual Memory Lecture Notes.pdf}{Virtual Memory Lecture Notes}
    \item \pdflink{\LecNoteDir Unit 4 Exam Review Lecture Notes.pdf}{Unit 4 Exam Review Lecture Notes}
    \item \pdflink{\LecNoteDir Unit 4 Terms Lecture Notes.pdf}{Unit 4 Terms Lecture Notes}
\end{itemize}

\subsection{Quiz}

The quiz for the week is:

\begin{itemize}
    \item \pdflink{\QuizDir Quiz 11 - Memory Management.pdf}{Quiz 11 - Memory Management}
\end{itemize}

\newpage

\subsection{Chapter Summary}

The chapter that is being covered this week is \textbf{Chapter 9: Main Memory}. The first section that is covered from this chapter this week is \textbf{Section 9.1: Background}.

\begin{notes}{Section 9.1: Background}
    \subsection*{Overview}

    This section establishes the foundation for memory management in modern computer systems. It discusses how memory serves as the primary storage for program execution, emphasizing its critical 
    role in enabling CPU operations. The content covers the various aspects of memory management, including hardware basics, address translation, and the distinction between logical and physical 
    addresses. Dynamic linking and shared libraries are also introduced, highlighting their role in efficient memory utilization.
    
    \subsubsection*{Memory and CPU Interaction}
    
    Memory is central to the operation of a computer system, consisting of a large array of bytes, each uniquely addressed. The CPU interacts with memory through a sequence of fetch and store 
    operations, driven by program instructions.
    
    \begin{highlight}[Memory and CPU Interaction]
    
        \begin{itemize}
            \item \textbf{Instruction Fetch Cycle}: The CPU fetches instructions based on the program counter, decodes them, and executes operations involving memory access.
            \item \textbf{Memory Viewpoint}: Memory sees only a series of addresses, independent of their generation (e.g., indexing or literal addressing).
            \item \textbf{Logical Addressing}: Focus is placed on the sequence of memory addresses generated by a program, abstracting away the address generation mechanism.
        \end{itemize}
    
    \end{highlight}
    
    \subsubsection*{Memory Access and Protection}
    
    The CPU accesses memory directly via main memory and registers. Registers offer fast access within one clock cycle, while main memory access involves multiple clock cycles, potentially stalling the 
    processor. To alleviate this, caches provide faster intermediate storage.
    
    \begin{highlight}[Memory Access and Protection]
    
        \begin{itemize}
            \item \textbf{Registers vs. Main Memory}: Registers are directly accessible, but main memory access may require stalling the CPU.
            \item \textbf{Cache Systems}: Fast memory caches reduce access time, improving performance.
            \item \textbf{Hardware Protection}: Hardware ensures processes cannot interfere with each other's memory spaces, using mechanisms like base and limit registers.
        \end{itemize}
    
    \end{highlight}
    
    \subsubsection*{Address Binding}
    
    Address binding ties program references to actual memory locations, which can occur at compile time, load time, or execution time. Execution-time binding is common in modern systems, requiring 
    hardware like the memory management unit (MMU) for address translation.
    
    \begin{highlight}[Address Binding]
    
        \begin{itemize}
            \item \textbf{Compile-Time Binding}: Absolute addresses are fixed during compilation.
            \item \textbf{Load-Time Binding}: Relocatable code is adjusted during program loading.
            \item \textbf{Execution-Time Binding}: Logical addresses are mapped to physical addresses dynamically, often using a relocation register.
        \end{itemize}
    
    \end{highlight}
    
    \subsubsection*{Logical vs. Physical Address Space}
    
    Logical addresses, generated by the CPU, differ from physical addresses in execution-time binding. Logical addresses form the program’s address space, while physical addresses correspond to actual 
    memory locations.
    
    \begin{highlight}[Logical vs. Physical Address Space]
    
        \begin{itemize}
            \item \textbf{Logical Address}: CPU-generated address during program execution.
            \item \textbf{Physical Address}: Actual location in main memory.
            \item \textbf{Memory Mapping}: The MMU maps logical addresses to physical ones, enabling dynamic relocation and address separation.
        \end{itemize}
    
    \end{highlight}
    
    \subsubsection*{Dynamic Linking and Shared Libraries}
    
    Dynamic linking allows programs to utilize shared libraries at runtime, reducing memory usage and enabling updates without recompilation. Shared libraries use memory pages to optimize sharing 
    across processes.
    
    \begin{highlight}[Dynamic Linking and Shared Libraries]
    
        \begin{itemize}
            \item \textbf{Dynamic Loading}: Code routines are loaded into memory only when invoked.
            \item \textbf{Shared Libraries}: Multiple processes can use the same library, with proper memory management to ensure consistency.
            \item \textbf{Version Control}: Libraries can be updated with compatibility checks, ensuring smooth program execution.
        \end{itemize}
    
    \end{highlight}
    
    \begin{highlight}[Summary of Key Concepts]
    
        \begin{itemize}
            \item \textbf{Memory Hierarchy}: Registers, caches, and main memory manage data access efficiently.
            \item \textbf{Address Binding}: Processes require dynamic relocation of memory references.
            \item \textbf{Logical Addressing}: Logical addresses differ from physical ones to allow flexible memory allocation.
            \item \textbf{Dynamic Libraries}: Efficient memory usage through runtime linking and shared libraries.
        \end{itemize}
    
    Memory management ensures optimal use of CPU and memory resources, balancing performance and protection requirements.
        
    \end{highlight}
\end{notes}

The next section being covered this week is \textbf{Section 9.2: Contiguous Memory Allocation}.

\begin{notes}{Section 9.2: Contiguous Memory Allocation}
    \begin{notes}{Contiguous Memory Allocation}
        \subsection*{Overview}
        
        This section explains contiguous memory allocation as a method of memory management where each process occupies a single continuous block of memory. It is one of the earliest and simplest 
        methods for managing memory, dividing it into two partitions: one for the operating system and the other for user processes. The section also addresses issues of memory protection and 
        allocation strategies, including the problems of fragmentation and possible solutions.
    
        \subsubsection*{Memory Protection}
        
        Contiguous memory allocation relies on hardware mechanisms like relocation and limit registers to ensure memory protection. These registers define boundaries for each process, preventing 
        processes from accessing memory outside their allocated region. This system safeguards the operating system and user data from interference by other processes.
        
        \begin{highlight}[Memory Protection]
            \begin{itemize}
                \item \textbf{Relocation Register}: Specifies the base physical address for a process.
                \item \textbf{Limit Register}: Defines the range of accessible logical addresses for the process.
                \item \textbf{Context Switch}: Updates these registers during a process switch to maintain protection.
            \end{itemize}
        \end{highlight}
        
        \subsubsection*{Memory Allocation Strategies}
        
        The operating system must allocate memory efficiently to processes, using strategies such as first-fit, best-fit, and worst-fit. Each strategy has trade-offs regarding speed, fragmentation, 
        and memory utilization.
        
        \begin{highlight}[Memory Allocation Strategies]
            \begin{itemize}
                \item \textbf{First-Fit}: Allocates the first suitable block of memory encountered. Faster but may lead to fragmentation.
                \item \textbf{Best-Fit}: Allocates the smallest available block that fits the process, reducing wasted space but requiring more searches.
                \item \textbf{Worst-Fit}: Allocates the largest available block, leaving larger fragments for subsequent processes.
            \end{itemize}
        \end{highlight}
        
        \subsubsection*{Fragmentation Issues}
        
        Contiguous allocation is prone to both external and internal fragmentation. External fragmentation occurs when free memory is available but scattered in small, unusable blocks. Internal 
        fragmentation arises when allocated memory exceeds process requirements, leaving unused space within allocated blocks.
        
        \begin{highlight}[Fragmentation Issues]
            \begin{itemize}
                \item \textbf{External Fragmentation}: Unused memory is scattered, causing inefficient utilization.
                \item \textbf{Internal Fragmentation}: Allocated memory blocks contain unused but inaccessible space.
                \item \textbf{50-Percent Rule}: Approximately 50% of memory may be lost to fragmentation.
            \end{itemize}
        \end{highlight}
        
        \subsubsection*{Compaction}
        
        To combat external fragmentation, compaction rearranges memory contents to consolidate free spaces into a single block. This process is computationally expensive and feasible only if dynamic 
        relocation is supported.
        
        \begin{highlight}[Compaction]
            \begin{itemize}
                \item \textbf{Purpose}: Combines scattered free memory into a contiguous block.
                \item \textbf{Feasibility}: Requires dynamic relocation during execution.
                \item \textbf{Trade-Offs}: Reduces fragmentation but incurs performance overhead.
            \end{itemize}
        \end{highlight}
        
        \begin{highlight}[Summary of Key Concepts]
            \begin{itemize}
                \item \textbf{Contiguous Allocation}: Simplistic but efficient for small-scale memory management.
                \item \textbf{Protection Mechanisms}: Relocation and limit registers safeguard process memory.
                \item \textbf{Fragmentation}: A significant drawback addressed partially by compaction.
                \item \textbf{Allocation Strategies}: Each method balances speed and memory efficiency differently.
            \end{itemize}
            Contiguous memory allocation provides foundational concepts for understanding modern memory management techniques and their limitations.
        \end{highlight}
    \end{notes}
\end{notes}

The next section being covered this week is \textbf{Section 9.3: Paging}.

\begin{notes}{Section 9.3: Paging}
    \subsection*{Overview}

    This section introduces paging as a memory management scheme that eliminates the need for contiguous allocation of physical memory, significantly reducing fragmentation issues. Paging divides both 
    logical and physical memory into fixed-sized blocks called pages and frames, respectively. A process’s pages are mapped to available frames, enabling non-contiguous allocation and improving memory 
    utilization.
    
    \subsubsection*{Structure of Paging}
    
    Paging divides memory into fixed-sized units:
    \begin{itemize}
        \item \textbf{Pages}: Fixed-sized blocks of logical memory.
        \item \textbf{Frames}: Fixed-sized blocks of physical memory, equal in size to pages.
        \item \textbf{Page Table}: Maps logical pages to physical frames, allowing non-contiguous memory allocation.
        \item \textbf{No Fragmentation}: Paging avoids external fragmentation by utilizing available frames efficiently.
    \end{itemize}
    
    \subsubsection*{Translation from Logical to Physical Address}
    
    When a program accesses a logical address, it is split into two parts:
    \begin{itemize}
        \item \textbf{Page Number}: Index into the page table to find the frame number.
        \item \textbf{Page Offset}: Displacement within the frame where the data resides.
        \item \textbf{MMU Role}: Performs the address translation dynamically during execution.
    \end{itemize}
    
    \subsubsection*{Advantages of Paging}
    
    Paging offers several benefits over contiguous memory allocation, including:
    \begin{itemize}
        \item \textbf{Efficient Memory Utilization}: Allows non-contiguous allocation, reducing fragmentation.
        \item \textbf{Process Isolation}: Ensures one process cannot access another process’s memory.
        \item \textbf{Flexibility}: Easily accommodates processes that grow or shrink in size.
    \end{itemize}
    
    \subsubsection*{Challenges of Paging}
    
    Despite its advantages, paging introduces overhead in maintaining and accessing the page table. The time taken to access a page table and then retrieve the data from memory can slow down performance. 
    To mitigate this, a hardware cache known as the Translation Lookaside Buffer (TLB) stores recent page table entries for faster access.
    \begin{itemize}
        \item \textbf{Page Table Overhead}: Requires additional memory to store page tables for all processes.
        \item \textbf{Access Time}: Two memory accesses (page table and data retrieval) may cause delays.
        \item \textbf{TLB Usage}: Hardware caching of page table entries reduces address translation time.
    \end{itemize}
    
    \begin{highlight}[Summary of Key Concepts]
    
        \begin{itemize}
            \item \textbf{Paging Structure}: Divides memory into pages and frames, enabling non-contiguous allocation.
            \item \textbf{Address Translation}: Uses the page table and MMU to map logical addresses to physical addresses dynamically.
            \item \textbf{Benefits of Paging}: Reduces fragmentation, enhances memory isolation, and supports flexible process resizing.
            \item \textbf{Challenges}: Requires careful management of page tables and reliance on TLB for performance.
        \end{itemize}
    
    Paging is a foundational memory management technique, balancing efficiency and flexibility while addressing fragmentation and memory isolation challenges.
    
    \end{highlight}
\end{notes}

The next section being covered this week is \textbf{Section 9.4: Structure Of The Page Table}.

\begin{notes}{Section 9.4: Structure Of The Page Table}
    \subsection*{Overview}

    This section explores various techniques for structuring page tables to efficiently manage large logical address spaces. Page tables are a critical component of memory management, translating 
    logical addresses to physical addresses. However, their potentially large size poses challenges for memory allocation and access speed. This section discusses hierarchical paging, hashed page 
    tables, and inverted page tables as solutions.

    \subsubsection*{Hierarchical Paging}

    Hierarchical paging divides the page table into multiple levels, enabling efficient management of large address spaces. For example, in a two-level paging scheme, the logical address is divided 
    into indices for an outer page table and an inner page table, along with an offset.

    \begin{highlight}[Hierarchical Paging]

        \begin{itemize}
            \item \textbf{Two-Level Paging}: Logical addresses are split into multiple parts, directing address translation through hierarchical page tables.
            \item \textbf{Advantages}: Reduces contiguous memory requirements for the page table.
            \item \textbf{Limitations}: Increased number of memory accesses for multi-level translations.
        \end{itemize}

    \end{highlight}

    \subsubsection*{Hashed Page Tables}

    Hashed page tables address the needs of address spaces larger than 32 bits. They use a hash function to map virtual page numbers to table entries containing physical frame numbers and linked 
    lists for collision resolution.

    \begin{highlight}[Hashed Page Tables]

        \begin{itemize}
            \item \textbf{Structure}: Entries consist of a virtual page number, corresponding frame number, and a pointer to the next entry in case of collisions.
            \item \textbf{Benefits}: Efficient handling of sparse address spaces.
            \item \textbf{Clustered Page Tables}: An optimized variant where each entry maps multiple pages, reducing overhead for sparse spaces.
        \end{itemize}

    \end{highlight}

    \subsubsection*{Inverted Page Tables}

    Inverted page tables condense the page table by maintaining one entry per physical memory frame, mapping it to a virtual address. This approach significantly reduces memory usage but can increase 
    the time required for address translation.

    \begin{highlight}[Inverted Page Tables]

        \begin{itemize}
            \item \textbf{Structure}: Stores mappings between physical frames and virtual pages for all processes.
            \item \textbf{Efficiency}: Reduces memory consumption by eliminating per-process page tables.
            \item \textbf{Challenges}: Address translation may require exhaustive searches, often mitigated with hash tables.
        \end{itemize}

    \end{highlight}

    \begin{highlight}[Summary of Key Concepts]

        \begin{itemize}
            \item \textbf{Hierarchical Paging}: Splits page tables into levels, balancing memory requirements and access speed.
            \item \textbf{Hashed Page Tables}: Utilize hash functions to efficiently map large or sparse address spaces.
            \item \textbf{Inverted Page Tables}: Reduce memory usage by representing all processes in a single system-wide table.
        \end{itemize}

    These techniques illustrate the trade-offs in page table design, balancing memory usage and translation efficiency to optimize system performance.

    \end{highlight}
\end{notes}

The next section being covered this week is \textbf{Section 9.5: Swapping}.

\begin{notes}{Section 9.5: Swapping}
    \subsection*{Overview}

    This section discusses swapping as a memory management technique where processes or portions of processes can be moved between main memory and a secondary storage area (backing store). Swapping 
    allows the system to exceed its physical memory limits, thus enabling a higher degree of multiprogramming. The text delves into traditional swapping, swapping with paging, and the unique 
    considerations of swapping on mobile systems.
    
    \subsubsection*{Standard Swapping}
    
    Standard swapping involves moving entire processes between main memory and a backing store. This method, while allowing more processes to coexist, incurs significant overhead due to the time 
    required to swap processes in and out.
    
    \begin{highlight}[Standard Swapping]
        \begin{itemize}
            \item \textbf{Back Store Requirements}: A fast secondary storage device large enough to hold the memory images of swapped processes.
            \item \textbf{Metadata Maintenance}: The operating system must retain metadata for swapped processes to restore them correctly.
            \item \textbf{Process Selection}: Idle or minimally active processes are prime candidates for swapping.
            \item \textbf{Impact on Multithreaded Processes}: Per-thread data structures must also be swapped for multithreaded processes.
        \end{itemize}
    \end{highlight}
    
    \subsubsection*{Swapping with Paging}
    
    Modern systems such as Linux and Windows implement a refined version of swapping, focusing on paging rather than moving entire processes. Individual pages of a process are swapped to and from the 
    backing store, offering a more efficient approach.
    
    \begin{highlight}[Swapping with Paging]
        \begin{itemize}
            \item \textbf{Page-Level Operations}: A page-out operation moves a specific page from memory to the backing store; a page-in operation does the reverse.
            \item \textbf{Efficiency}: Reduces the overhead compared to swapping entire processes.
            \item \textbf{Integration with Virtual Memory}: Works synergistically with virtual memory systems for improved performance.
        \end{itemize}
    \end{highlight}
    
    \subsubsection*{Swapping on Mobile Systems}
    
    Mobile systems typically avoid swapping due to constraints like limited storage and the reduced reliability of flash memory. Instead, mobile operating systems adopt alternative strategies.
    
    \begin{highlight}[Swapping on Mobile Systems]
        \begin{itemize}
            \item \textbf{Memory Management}: Systems like iOS and Android ask applications to voluntarily relinquish memory when free memory is low.
            \item \textbf{Application Termination}: If insufficient memory is freed, inactive applications are terminated, often with their states saved for rapid recovery.
            \item \textbf{Flash Memory Concerns}: Flash memory has limited write cycles and low throughput compared to traditional disks, discouraging swapping.
        \end{itemize}
    \end{highlight}
    
    \subsubsection*{Performance Considerations}
    
    Swapping can indicate an overloaded system with more active processes than available physical memory. While paging improves efficiency over traditional swapping, excessive swapping (thrashing) 
    still degrades system performance.
    
    \begin{highlight}[Performance Considerations]
        \begin{itemize}
            \item \textbf{Process Termination}: When physical memory is insufficient, terminating processes may be necessary to maintain system stability.
            \item \textbf{Hardware Upgrades}: Adding physical memory is a common solution to alleviate swapping-induced performance bottlenecks.
        \end{itemize}
    \end{highlight}
    
    \begin{highlight}[Summary of Key Concepts]
        \begin{itemize}
            \item \textbf{Standard Swapping}: Moves entire processes to and from the backing store but incurs high overhead.
            \item \textbf{Swapping with Paging}: Efficiently handles individual memory pages, integrating well with virtual memory.
            \item \textbf{Mobile System Strategies}: Employ alternatives like memory relinquishment and application termination instead of traditional swapping.
            \item \textbf{System Balance}: Excessive swapping highlights resource constraints, necessitating process management or hardware upgrades.
        \end{itemize}
        Swapping remains a critical memory management strategy, evolving to address the limitations of traditional techniques and adapt to the constraints of modern systems.
    \end{highlight}
\end{notes}

The next section being covered this week is \textbf{Section 9.6: Example: Intel 32 And 64 Bit Architectures}.

\begin{notes}{Section 9.6: Example: Intel 32 And 64 Bit Architectures}
    \subsection*{Overview}

    This section explores the memory management mechanisms of Intel's 32-bit and 64-bit architectures, focusing on the structure and operation of their paging systems. Both architectures use hierarchical 
    paging with multiple levels of page tables, enabling efficient address translation for large memory spaces. The 64-bit architecture expands memory addressing capabilities while maintaining backward 
    compatibility with the 32-bit architecture.
    
    \subsubsection*{Intel 32-Bit Architecture}
    
    The Intel 32-bit architecture employs a two-level paging scheme consisting of a page directory and page tables. Logical addresses are divided into three components: the directory index, table index, 
    and offset. This hierarchical structure reduces the memory overhead for managing page tables.
    
    \begin{highlight}[Intel 32-Bit Architecture]
    
        \begin{itemize}
            \item \textbf{Page Directory}: Contains pointers to page tables.
            \item \textbf{Page Tables}: Contain pointers to physical memory frames.
            \item \textbf{Address Translation}: Divides logical addresses into directory index, table index, and offset.
        \end{itemize}
    
    \end{highlight}
    
    \subsubsection*{Intel 64-Bit Architecture}
    
    The Intel 64-bit architecture extends the address space to support significantly larger memory. It uses a four-level paging hierarchy: PML4 (Page Map Level 4), PDPT (Page Directory Pointer Table), 
    Page Directory, and Page Table. Logical addresses are split into five components to navigate this hierarchy. The architecture also supports page sizes of 4 KB, 2 MB, and 1 GB.
    
    \begin{highlight}[Intel 64-Bit Architecture]
    
        \begin{itemize}
            \item \textbf{PML4}: The top-level page map, indexing pointers to PDPTs.
            \item \textbf{PDPT}: Points to page directories, part of the hierarchical translation.
            \item \textbf{Large Page Sizes}: Includes support for 2 MB and 1 GB pages to reduce translation overhead.
            \item \textbf{Backward Compatibility}: Retains support for 32-bit paging schemes.
        \end{itemize}
    
    \end{highlight}
    
    \subsubsection*{Address Translation and TLB}
    
    Both architectures rely on the Translation Lookaside Buffer (TLB) to cache recent page translations, minimizing the performance cost of hierarchical paging. Efficient use of the TLB is critical for 
    maintaining high memory access speeds in systems with large address spaces.
    
    \begin{highlight}[Address Translation and TLB]
    
        \begin{itemize}
            \item \textbf{Translation Lookaside Buffer (TLB)}: Caches page table entries to speed up address translation.
            \item \textbf{Hierarchical Overhead}: Mitigated by the TLB to ensure efficient access.
        \end{itemize}
    
    \end{highlight}
    
    \begin{highlight}[Summary of Key Concepts]
    
        \begin{itemize}
            \item \textbf{Intel 32-Bit Architecture}: Uses a two-level paging scheme with a page directory and page tables.
            \item \textbf{Intel 64-Bit Architecture}: Expands to a four-level paging hierarchy, supporting larger memory spaces and page sizes.
            \item \textbf{TLB Efficiency}: Essential for mitigating the performance cost of multi-level paging.
        \end{itemize}
    
    Intel's memory management designs illustrate the evolution of paging systems to accommodate growing memory demands while maintaining backward compatibility and performance.
    
    \end{highlight}
\end{notes}

The last section being covered this week is \textbf{Section 9.7: Example: ARMv8 Architecture}.

\begin{notes}{Section 9.7: Example: ARMv8 Architecture}
    \subsection*{Overview}

    This section examines the ARMv8 architecture, focusing on its memory management system and support for virtual memory through a hierarchical paging scheme. ARMv8 is a 64-bit architecture widely used 
    in mobile, embedded, and server environments, offering flexibility and scalability. Its paging mechanism supports multiple page sizes and enables efficient address translation for large address spaces.
    
    \subsubsection*{ARMv8 Paging Structure}
    
    The ARMv8 architecture employs a four-level hierarchical paging scheme, similar to the Intel 64-bit architecture. Logical addresses are divided into several components, each corresponding to a level 
    in the hierarchy. The levels include the translation table base register (TTBR), which points to the top-level table, and subsequent levels that refine the translation to the final physical address.
    
    \begin{highlight}[ARMv8 Paging Structure]
    
        \begin{itemize}
            \item \textbf{Translation Table Base Register (TTBR)}: Points to the top-level table for address translation.
            \item \textbf{Four-Level Paging}: Includes levels for refining logical address mapping to physical frames.
            \item \textbf{Page Sizes}: Supports 4 KB, 16 KB, and 64 KB pages for flexible memory management.
        \end{itemize}
    
    \end{highlight}
    
    \subsubsection*{Memory Addressing in ARMv8}
    
    ARMv8 supports 48-bit virtual addresses, with extensions for up to 52-bit addresses in future implementations. Logical addresses are split into indices for the hierarchical paging levels and an offset 
    for the specific memory location within the page. This structure allows efficient management of large address spaces.
    
    \begin{highlight}[Memory Addressing in ARMv8]
    
        \begin{itemize}
            \item \textbf{48-Bit Virtual Addressing}: Allows addressing of a vast memory space.
            \item \textbf{Hierarchical Translation}: Divides addresses into indices and offsets for multi-level translation.
            \item \textbf{Future Expansion}: Supports extensions for 52-bit virtual addresses.
        \end{itemize}
    
    \end{highlight}
    
    \subsubsection*{Translation Lookaside Buffer (TLB)}
    
    As with other architectures, ARMv8 employs a TLB to cache recent translations, improving memory access performance. The TLB supports multiple levels of caching to handle large translation tables efficiently.
    
    \begin{highlight}[Translation Lookaside Buffer (TLB)]
    
        \begin{itemize}
            \item \textbf{Caching Efficiency}: Reduces the performance cost of hierarchical paging.
            \item \textbf{Multi-Level TLB}: Supports handling of extensive address spaces with minimal latency.
        \end{itemize}
    
    \end{highlight}
    
    \begin{highlight}[Summary of Key Concepts]
    
        \begin{itemize}
            \item \textbf{ARMv8 Paging}: Employs a four-level hierarchical structure for virtual memory management.
            \item \textbf{Flexible Page Sizes}: Supports 4 KB, 16 KB, and 64 KB pages to optimize memory usage.
            \item \textbf{Address Translation}: Uses a TLB to accelerate mapping of logical to physical addresses.
            \item \textbf{Scalability}: Designed for large address spaces, with future extensions supporting 52-bit addressing.
        \end{itemize}
    
    The ARMv8 architecture exemplifies a scalable and efficient memory management design, catering to diverse applications from embedded systems to high-performance computing.
    
    \end{highlight}
\end{notes}