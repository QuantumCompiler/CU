\clearpage

\renewcommand{\ChapTitle}{Virtual Memory}
\renewcommand{\SectionTitle}{Virtual Memory}

\chapter{\ChapTitle}
\section{\SectionTitle}
\horizontalline{0}{0}

\subsection{Assigned Reading}

The reading for this week comes from the \href{https://learn.zybooks.com/zybook/COLORADOCSPB3753KnoxFall2024}{Zybooks} for the week is:

\begin{itemize}
    \item \textbf{Chapter 10: Virtual Memory}
\end{itemize}

\subsection{Lectures}

The lecture videos for the week are:

\begin{itemize}
    \item \lecture{https://www.youtube.com/watch?v=duOSqko0sQk}{Virtual Memory}{19}
    \item \lecture{https://www.youtube.com/watch?v=vcYXnLbcWbo}{Page Replacement Algorithms}{36}
    \item \lecture{https://www.youtube.com/watch?v=LvX96WV-M9U}{Belady's Anomaly}{6}
    \item \lecture{https://www.youtube.com/watch?v=cb46wC-QEXg}{Frame Allocation}{14}
    \item \lecture{https://www.youtube.com/watch?v=Z9zPLRC4egs}{Thrashing}{13}
    \item \lecture{https://www.youtube.com/watch?v=L1zhW6wxQyw}{Memory Mapped Files}{15}
\end{itemize}

\subsection{Assignments}

The assignment(s) for the week is:

\begin{itemize}
    \item \href{https://github.com/cu-cspb-3753-fall-2024/lab-12-QuantumCompiler}{Lab 12 - LRU}
    \item \href{https://github.com/cu-cspb-3753-fall-2024/pa-paging-QuantumCompiler}{Programming Assignment 4 - Paging}
\end{itemize}

\subsection{Quiz}

The quiz for the week is:

\begin{itemize}
    \item \pdflink{\QuizDir Quiz 12 - Virtual Memory.pdf}{Quiz 12 - Virtual Memory}
\end{itemize}

\newpage

\subsection{Chapter Summary}

The chapter that is being covered this week is \textbf{Chapter 10: Virtual Memory}. The first section that is being covered from this chapter this week is \textbf{Section 10.1: Background}.

\begin{notes}{Section 10.1: Background}
    \subsection*{Overview}

    This section introduces the concept of virtual memory, a fundamental component of modern operating systems that allows a program to execute even if it is not fully loaded into physical memory. 
    Virtual memory enables efficient utilization of memory resources, supports multitasking, and provides isolation between processes. It achieves these goals by separating logical memory addresses 
    used by a program from the physical addresses in main memory.
    
    \subsubsection*{Purpose of Virtual Memory}
    
    Virtual memory provides several key advantages:
    \begin{itemize}
        \item \textbf{Efficient Memory Utilization}: Allows programs to use more memory than physically available by paging or swapping data between main memory and disk storage.
        \item \textbf{Process Isolation}: Ensures that processes operate in separate memory spaces, preventing unintended interference.
        \item \textbf{Simplified Programming}: Offers programmers an abstraction of memory that appears contiguous, regardless of physical memory fragmentation.
    \end{itemize}
    
    \subsubsection*{Logical vs. Physical Address Space}
    
    Virtual memory relies on the distinction between logical and physical address spaces:
    \begin{itemize}
        \item \textbf{Logical Address Space}: The set of addresses generated by a program, also known as virtual addresses.
        \item \textbf{Physical Address Space}: The actual locations in main memory where data is stored.
        \item \textbf{Address Translation}: A memory management unit (MMU) translates logical addresses to physical addresses dynamically, enabling efficient access and protection.
    \end{itemize}
    
    \subsubsection*{Demand Paging}
    
    A cornerstone of virtual memory is demand paging, where pages are loaded into memory only when they are needed:
    \begin{itemize}
        \item \textbf{Lazy Loading}: Reduces memory usage by only loading data as required.
        \item \textbf{Page Faults}: Occur when a program references a page not currently in memory, triggering the operating system to load the page from secondary storage.
        \item \textbf{Performance Trade-offs}: Minimizes memory usage but incurs a performance penalty due to disk I/O during page faults.
    \end{itemize}
    
    \begin{highlight}[Summary of Key Concepts]
    
        \begin{itemize}
            \item \textbf{Virtual Memory}: Separates logical and physical address spaces to enhance memory utilization and process isolation.
            \item \textbf{Logical Addressing}: Abstracts memory management, simplifying programming and multitasking.
            \item \textbf{Demand Paging}: Loads memory pages on-demand, balancing memory efficiency with potential performance costs.
        \end{itemize}
    
    Virtual memory is a cornerstone of modern operating systems, enabling efficient memory management, multitasking, and process isolation.
    
    \end{highlight}
\end{notes}

The next section that is being covered from this chapter this week is \textbf{Section 10.2: Demand Paging}.

\begin{notes}{Section 10.2: Demand Paging}
    \subsection*{Overview}

    This section introduces demand paging, a mechanism that loads memory pages only when they are accessed, optimizing memory utilization. Unlike preloading all pages into memory at process startup, 
    demand paging ensures that only the necessary pages are brought into physical memory, reducing memory consumption. This approach underpins virtual memory systems and leverages paging hardware for 
    efficient operation.
    
    \subsubsection*{Basic Concepts}
    
    Demand paging relies on loading pages from secondary storage into main memory only when required during execution. Pages not accessed remain in secondary storage, and the system uses a valid-invalid 
    bit to differentiate between pages in memory and those not currently loaded. This mechanism supports dynamic and efficient memory management.
    
    \begin{highlight}[Basic Concepts]
    \begin{itemize}
        \item \textbf{Valid-Invalid Bit}: Indicates whether a page is in memory (valid) or in secondary storage (invalid).
        \item \textbf{Page Fault}: Occurs when an invalid page is accessed, triggering a trap to load the page into memory.
        \item \textbf{Hardware Requirements}: Requires page tables to track valid-invalid bits and secondary memory (e.g., swap space) for non-resident pages.
    \end{itemize}
    \end{highlight}
    
    \subsubsection*{Handling Page Faults}
    
    When a page fault occurs, the system performs several steps to load the required page. The process includes determining the legality of the reference, locating the page in secondary storage, reading 
    it into memory, and updating the page table. Execution then resumes as if the page had always been in memory.
    
    \begin{highlight}[Handling Page Faults]
    \begin{itemize}
        \item \textbf{Trap to Kernel}: The operating system intercepts the fault and verifies the legality of the page reference.
        \item \textbf{Loading Pages}: Finds a free frame, reads the page from secondary storage, and updates the page table.
        \item \textbf{Instruction Restart}: Resumes execution of the interrupted instruction.
    \end{itemize}
    \end{highlight}
    
    \subsubsection*{Free-Frame Management}
    
    Free frames in memory are tracked using a free-frame list. When frames are needed for demand paging, the system allocates zero-filled frames from this list, ensuring no residual data is exposed to processes.
    
    \begin{highlight}[Free-Frame Management]
    \begin{itemize}
        \item \textbf{Free-Frame List}: Tracks available memory frames for allocation.
        \item \textbf{Zero-Fill on Demand}: Ensures frames are cleared before reuse for security and consistency.
    \end{itemize}
    \end{highlight}
    
    \subsubsection*{Performance Considerations}
    
    Effective access time in a demand-paged system depends on the frequency of page faults. Reducing the page fault rate is critical to maintaining performance, as page faults introduce significant delays 
    due to secondary storage access.
    
    \begin{highlight}[Performance Considerations]
    \begin{itemize}
        \item \textbf{Effective Access Time}: Combines memory access time and page fault service time.
        \item \textbf{Minimizing Page Faults}: Achieved through optimized algorithms and locality of reference.
    \end{itemize}
    \end{highlight}
    
    \subsubsection*{Program Structure and Locality}
    
    Demand paging benefits from the locality of reference, where programs access memory in predictable patterns. Optimizing program structure, such as iterating over arrays in row-major order, can 
    significantly reduce page faults.
    
    \begin{highlight}[Program Structure and Locality]
    \begin{itemize}
        \item \textbf{Locality of Reference}: Ensures efficient memory usage by accessing nearby memory addresses.
        \item \textbf{Optimized Loop Structures}: Reduces page faults by aligning access patterns with memory organization.
    \end{itemize}
    \end{highlight}
    
    \begin{highlight}[Summary of Key Concepts]
    \begin{itemize}
        \item \textbf{Demand Paging}: Efficiently loads pages into memory only when accessed.
        \item \textbf{Page Fault Handling}: Includes validation, loading, and resuming processes.
        \item \textbf{Free-Frame Management}: Ensures clean and secure memory allocation.
        \item \textbf{Performance Optimization}: Focuses on reducing page fault rates and leveraging memory locality.
    \end{itemize}
    Demand paging enhances memory efficiency and enables the execution of large programs on systems with limited physical memory.
    \end{highlight}
\end{notes}

The next section that is being covered from this chapter this week is \textbf{Section 10.3: Copy-on-Write}.

\begin{notes}{Section 10.3: Copy-on-Write}
    \subsection*{Overview}

    This section explains the concept and functionality of copy-on-write (COW) as a memory management optimization. It highlights how COW facilitates efficient process creation and minimizes 
    unnecessary duplication of memory pages. This technique is integral to systems using the `fork()` system call and similar methods.
    
    \subsubsection*{Process Creation and Page Sharing}
    
    Traditionally, when a child process is created using `fork()`, it duplicates the parent's entire address space. This duplication often results in inefficiencies, especially when the child 
    immediately executes a new program with `exec()`. Copy-on-write addresses this by initially sharing pages between the parent and child, deferring duplication until a modification occurs.
    
    \begin{highlight}[Process Creation and Page Sharing]
    
    \begin{itemize}
        \item \textbf{Shared Pages}: Parent and child processes share pages marked as copy-on-write.
        \item \textbf{Deferred Copying}: Only modified pages are duplicated, conserving memory.
        \item \textbf{Efficiency}: Reduces overhead associated with duplicating memory during process creation.
    \end{itemize}
    
    \end{highlight}
    
    \subsubsection*{Page Modification with COW}
    
    If either the parent or the child modifies a shared page, the operating system copies the page to a new memory location. The process then modifies the copied page, ensuring that shared pages remain intact.
    
    \begin{highlight}[Page Modification with COW]
    
    \begin{itemize}
        \item \textbf{Triggered Duplication}: A shared page is duplicated upon a write attempt.
        \item \textbf{Memory Isolation}: Modifications affect only the process making the change.
        \item \textbf{Optimized Resource Use}: Unmodified pages remain shared.
    \end{itemize}
    
    \end{highlight}
    
    \subsubsection*{Applications in Modern Systems}
    
    Copy-on-write is widely used in operating systems such as Windows, Linux, and macOS. Its benefits extend to scenarios beyond `fork()`-based process creation, such as optimizing shared libraries and 
    virtual memory systems.
    
    \begin{highlight}[Applications in Modern Systems]
    
    \begin{itemize}
        \item \textbf{Shared Libraries}: Allows multiple processes to use a single instance of a library without redundancy.
        \item \textbf{Memory Efficiency}: Reduces the need for memory allocation and duplication.
        \item \textbf{System Compatibility}: Supported by UNIX variations like Linux and macOS.
    \end{itemize}
    
    \end{highlight}
    
    \subsubsection*{Alternative Method: \texttt{vfork()}}
    
    Certain UNIX systems offer `vfork()`, which suspends the parent process while the child uses its address space directly. Unlike COW, `vfork()` avoids duplication entirely but carries risks if the 
    child modifies the parent's memory.
    
    \begin{highlight}[Alternative Method: \texttt{vfork()}]
    
    \begin{itemize}
        \item \textbf{Direct Address Space Use}: The child operates directly within the parent's memory.
        \item \textbf{Efficiency Trade-Offs}: No page duplication occurs, but care is needed to avoid unintended modifications.
        \item \textbf{Use Cases}: Optimal when the child executes `exec()` immediately.
    \end{itemize}
    
    \end{highlight}
    
    \begin{highlight}[Summary of Key Concepts]
    
    \begin{itemize}
        \item \textbf{Copy-on-Write}: Enables efficient memory sharing and delayed duplication.
        \item \textbf{Memory Optimization}: Conserves resources by sharing unmodified pages.
        \item \textbf{vfork()}: Provides an alternative process creation mechanism with distinct trade-offs.
    \end{itemize}
    
    Copy-on-write is a powerful technique that optimizes memory usage and process creation in modern operating systems while ensuring performance and isolation.
    \end{highlight}
\end{notes}

The next section that is being covered from this chapter this week is \textbf{Section 10.4: Page Replacement}.

\begin{notes}{Section 10.4: Page Replacement}
    \subsection*{Overview}

    This section explains the process and significance of page replacement within a virtual memory system. Page replacement enables systems to manage limited physical memory while providing the illusion 
    of a large virtual memory. It ensures efficient use of memory and minimizes performance degradation caused by page faults.
    
    \subsubsection*{Purpose of Page Replacement}
    
    Page replacement is fundamental to demand paging. It allows logical memory to be much larger than physical memory by swapping pages between main memory and secondary storage. This section emphasizes 
    that logical and physical addresses can differ, and demand paging enables processes to run even when all their pages are not in memory.
    
    \begin{highlight}[Purpose of Page Replacement]
        \begin{itemize}
            \item \textbf{Separation of Memory Types}: Ensures logical and physical memory are decoupled, allowing flexible memory usage.
            \item \textbf{Dynamic Allocation}: Only actively used pages are kept in memory, optimizing resource utilization.
            \item \textbf{Reduction of Physical Constraints}: Supports processes larger than physical memory by swapping pages as needed.
        \end{itemize}
    \end{highlight}
    
    \subsubsection*{Challenges in Page Replacement}
    
    Implementing page replacement requires solving two critical problems: frame allocation and selecting a victim page for replacement. These decisions must be optimized due to the high cost of secondary 
    storage I/O operations. The section introduces the concept of reference strings to evaluate algorithms.
    
    \begin{highlight}[Challenges in Page Replacement]
        \begin{itemize}
            \item \textbf{Frame Allocation}: Deciding how many frames to allocate to each process.
            \item \textbf{Victim Selection}: Identifying which pages to replace when memory is full.
            \item \textbf{Performance Metrics}: Algorithms are evaluated based on page fault rates and system throughput.
        \end{itemize}
    \end{highlight}
    
    \subsubsection*{Algorithms for Page Replacement}
    
    Various algorithms have been developed to minimize page faults. These include:
    \begin{itemize}
        \item \textbf{Optimal Algorithm (OPT)}: Achieves the lowest possible page-fault rate by replacing the page that will not be used for the longest period. However, it requires future knowledge, 
        making it impractical for real-time use.
        \item \textbf{First-In-First-Out (FIFO)}: Replaces the oldest page, which is simple but prone to inefficiencies like Belady's anomaly.
        \item \textbf{Least Recently Used (LRU)}: Replaces the page least recently accessed, approximating OPT but requiring hardware or software support for tracking access times.
        \item \textbf{Enhanced Algorithms}: Approaches such as the clock algorithm improve efficiency by leveraging access bits.
    \end{itemize}
    
    \begin{highlight}[Algorithms for Page Replacement]
        \begin{itemize}
            \item \textbf{Optimal Algorithm}: Theoretical ideal, used for comparison.
            \item \textbf{FIFO Algorithm}: Simple but may suffer from anomalies.
            \item \textbf{LRU Algorithm}: Practical approximation of optimal behavior.
            \item \textbf{Enhanced Techniques}: Improvements like second-chance algorithms reduce overhead.
        \end{itemize}
    \end{highlight}
    
    \subsubsection*{Practical Considerations}
    
    The design of page replacement systems must balance complexity with performance. Memory references and the associated access patterns significantly influence the effectiveness of replacement algorithms. 
    Advanced systems may incorporate hybrid strategies to adapt to workload variations.
    
    \begin{highlight}[Practical Considerations]
        \begin{itemize}
            \item \textbf{Reference Patterns}: Analyze workload for optimal algorithm selection.
            \item \textbf{Hybrid Strategies}: Combine multiple techniques for improved adaptability.
            \item \textbf{System Performance}: Minimize I/O costs and improve throughput.
        \end{itemize}
    \end{highlight}
    
    \begin{highlight}[Summary of Key Concepts]
        \begin{itemize}
            \item \textbf{Page Replacement Significance}: Manages limited physical memory while offering virtual memory benefits.
            \item \textbf{Algorithm Diversity}: Tailored solutions to meet system-specific needs.
            \item \textbf{Evaluation Metrics}: Page fault rates guide algorithm optimization.
            \item \textbf{Advanced Techniques}: Hybrid and adaptive methods enhance system efficiency.
        \end{itemize}
    \end{highlight}
\end{notes}

The next section that is being covered from this chapter this week is \textbf{Section 10.5: Allocation of Frames}.

\begin{notes}{Section 10.5: Allocation of Frames}
    \subsection*{Overview}

    This section explores the strategies for frame allocation in a virtual memory system. Frame allocation determines how available memory is distributed among processes and impacts performance through 
    fault rates and execution efficiency. The discussion covers equal and proportional allocation, minimum frame requirements, and global versus local allocation strategies.
    
    \subsubsection*{Equal and Proportional Allocation}
    
    Two basic allocation strategies are equal allocation and proportional allocation. Equal allocation divides frames equally among processes, while proportional allocation distributes frames based on 
    the relative sizes or priorities of processes.
    
    \begin{highlight}[Equal and Proportional Allocation]
    
    \begin{itemize}
        \item \textbf{Equal Allocation}: All processes receive an equal number of frames. Example: In a system with 93 frames and 5 processes, each process gets 18 frames, with 3 frames reserved for 
        the free-frame pool.
        \item \textbf{Proportional Allocation}: Frames are distributed based on process size. Example: For processes of size 10 and 127 pages with 62 frames available, allocation is approximately 4 
        and 57 frames, respectively.
        \item \textbf{Priority Adjustment}: Proportional allocation can also factor in process priorities to allocate more frames to higher-priority tasks.
    \end{itemize}
    
    \end{highlight}
    
    \subsubsection*{Minimum Number of Frames}
    
    A minimum number of frames per process is required to prevent excessive page faults and maintain performance. The minimum is determined by the architecture and the complexity of instructions, such 
    as indirect addressing.
    
    \begin{highlight}[Minimum Number of Frames]
    
    \begin{itemize}
        \item \textbf{Instruction Requirements}: Some architectures require multiple frames per instruction for indirect addressing or operand references.
        \item \textbf{Performance Impact}: Fewer frames lead to higher fault rates, which slow execution due to frequent restarts of interrupted instructions.
        \item \textbf{Example}: A process with two-level indirect addressing may require at least three frames to execute a single instruction properly.
    \end{itemize}
    
    \end{highlight}
    
    \subsubsection*{Global vs. Local Allocation}
    
    Frame replacement strategies can be classified as global or local. Global allocation allows processes to take frames from a shared pool, while local allocation restricts processes to their allocated frames.
    
    \begin{highlight}[Global vs. Local Allocation]
    
    \begin{itemize}
        \item \textbf{Global Replacement}: Frames are shared among all processes. High-priority processes can preempt frames from lower-priority ones, improving system throughput but potentially causing process starvation.
        \item \textbf{Local Replacement}: Each process uses only its allocated frames. This strategy provides stability but may limit flexibility, leading to inefficiencies.
        \item \textbf{Performance Trade-Offs}: Global replacement generally achieves higher throughput, while local replacement offers predictable performance per process.
    \end{itemize}
    
    \end{highlight}
    
    \begin{highlight}[Summary of Key Concepts]
    
    \begin{itemize}
        \item \textbf{Allocation Strategies}: Equal and proportional allocation methods balance simplicity and efficiency.
        \item \textbf{Frame Requirements}: A minimum number of frames ensures instruction execution without excessive faults.
        \item \textbf{Replacement Strategies}: Global replacement favors throughput, whereas local replacement ensures fairness and isolation.
    \end{itemize}
    
    Effective frame allocation is crucial for optimizing virtual memory performance, balancing resource distribution, and minimizing fault rates.
    
    \end{highlight}
\end{notes}

The next section that is being covered from this chapter this week is \textbf{Section 10.6: Thrashing}.

\begin{notes}{Section 10.6: Thrashing}
    \subsection*{Overview}

    This section explores the concept of thrashing, a condition where excessive paging activity prevents processes from making meaningful progress. Thrashing occurs when processes lack sufficient frames 
    to maintain their working sets, leading to frequent page faults, degraded CPU utilization, and overall system inefficiency. Strategies for detecting and mitigating thrashing are discussed, including 
    the working-set model and page-fault frequency techniques.
    
    \subsubsection*{Causes of Thrashing}
    
    Thrashing arises when a system's degree of multiprogramming exceeds its memory capacity. Processes compete for limited frames, replacing pages that are immediately needed, causing high page-fault 
    rates and significant performance degradation.
    
    \begin{highlight}[Causes of Thrashing]
        \begin{itemize}
            \item \textbf{High Paging Activity}: Processes repeatedly page-fault due to insufficient allocated frames.
            \item \textbf{Global Replacement Algorithms}: These algorithms allow processes to steal frames from each other, exacerbating paging conflicts.
            \item \textbf{Increasing Multiprogramming}: Adding more processes leads to resource contention and higher thrashing likelihood.
        \end{itemize}
    \end{highlight}
    
    \subsubsection*{Detecting Thrashing}
    
    The operating system detects thrashing by monitoring CPU utilization and page-fault rates. Thrashing is indicated by high page-fault rates coupled with low CPU utilization.
    
    \begin{highlight}[Detecting Thrashing]
        \begin{itemize}
            \item \textbf{Page-Fault Rate Monitoring}: High rates signal excessive paging.
            \item \textbf{CPU Utilization Trends}: Declining utilization suggests thrashing despite increased multiprogramming.
        \end{itemize}
    \end{highlight}
    
    \subsubsection*{Mitigating Thrashing}
    
    To mitigate thrashing, systems use techniques like local replacement algorithms, the working-set model, and page-fault frequency control.
    
    \begin{highlight}[Mitigating Thrashing]
        \begin{itemize}
            \item \textbf{Local Replacement Algorithms}: Restrict frame replacement to within a process's allocated frames.
            \item \textbf{Working-Set Model}: Allocate frames based on the actively used pages within a process's working set.
            \item \textbf{Page-Fault Frequency}: Adjust frame allocation dynamically to maintain page-fault rates within acceptable bounds.
        \end{itemize}
    \end{highlight}
    
    \subsubsection*{Working-Set Model}
    
    The working-set model assumes processes exhibit locality of reference, where a set of pages are actively used during specific execution phases. This model ensures processes have enough frames to cover 
    their working sets, reducing page faults.
    
    \begin{highlight}[Working-Set Model]
        \begin{itemize}
            \item \textbf{Locality of Reference}: Processes operate within specific localities, defined by actively accessed pages.
            \item \textbf{Dynamic Adjustment}: Frames are allocated based on the process's current working set.
            \item \textbf{Process Suspension}: If total working-set demands exceed available frames, processes are swapped out.
        \end{itemize}
    \end{highlight}
    
    \subsubsection*{Page-Fault Frequency Control}
    
    The page-fault frequency (PFF) method directly adjusts frame allocation based on observed fault rates. Frames are added or removed to keep fault rates within predefined limits.
    
    \begin{highlight}[Page-Fault Frequency Control]
        \begin{itemize}
            \item \textbf{Upper and Lower Bounds}: Define acceptable page-fault rate thresholds.
            \item \textbf{Dynamic Allocation}: Add frames when fault rates exceed the upper bound; reclaim frames when rates are too low.
        \end{itemize}
    \end{highlight}
    
    \begin{highlight}[Summary of Key Concepts]
        \begin{itemize}
            \item \textbf{Thrashing Causes}: Insufficient frames and global replacement lead to high paging activity.
            \item \textbf{Detection}: Monitored through page-fault rates and CPU utilization trends.
            \item \textbf{Mitigation}: Strategies include local replacement, working-set models, and PFF control.
            \item \textbf{System Balance}: Adequate memory provisioning is essential to prevent thrashing and maintain performance.
        \end{itemize}
        Addressing thrashing requires balancing memory allocation and process demands to optimize system throughput.
    \end{highlight}
\end{notes}

The next section that is being covered from this chapter this week is \textbf{Section 10.7: Memory-Mapped Files}.

\begin{notes}{Section 10.7: Memory-Mapped Files}
    \subsection*{Overview}

    This section discusses memory-mapped files, a feature of virtual memory systems that allows file contents to be mapped directly into the logical address space of a process. This mechanism provides 
    an efficient way for processes to access files, treating file contents as if they were part of the process's memory. Memory-mapped files enable faster file I/O operations and facilitate interprocess 
    communication by allowing shared memory access.
    
    \subsubsection*{Memory Mapping Process}
    
    To map a file into memory, the operating system uses the following process:
    \begin{itemize}
        \item \textbf{Mapping Files}: A file is associated with a region of the process's virtual address space.
        \item \textbf{On-Demand Loading}: File contents are loaded into memory as pages, on-demand, using the demand paging mechanism.
        \item \textbf{Synchronization}: Updates to the memory region are reflected in the underlying file and vice versa, either immediately or when explicitly flushed.
    \end{itemize}
    
    \subsubsection*{Benefits of Memory-Mapped Files}
    
    Memory-mapped files provide several advantages:
    \begin{itemize}
        \item \textbf{Efficient I/O}: Reduces the need for explicit read and write system calls, as file contents are accessed like memory.
        \item \textbf{Shared Memory}: Multiple processes can map the same file into their address space, enabling efficient interprocess communication.
        \item \textbf{Simplified Programming}: Treats files as contiguous memory regions, simplifying access patterns and code structure.
    \end{itemize}
    
    \subsubsection*{Implementation Considerations}
    
    Memory-mapped files rely on the paging mechanism of virtual memory, which introduces the following considerations:
    \begin{itemize}
        \item \textbf{Page Faults}: Accessing a file region not currently in memory triggers a page fault, prompting the operating system to load the necessary data.
        \item \textbf{File Size}: Files larger than available physical memory can still be mapped, as only required pages are loaded.
        \item \textbf{Access Synchronization}: Care must be taken when multiple processes modify the mapped file to avoid inconsistencies.
    \end{itemize}
    
    \begin{highlight}[Summary of Key Concepts]
    
        \begin{itemize}
            \item \textbf{Memory Mapping}: Maps file contents directly into a process's virtual address space for efficient access.
            \item \textbf{On-Demand Loading}: Uses demand paging to load file contents into memory as needed.
            \item \textbf{Shared Memory}: Allows multiple processes to access and modify the same file efficiently.
            \item \textbf{Implementation Considerations}: Handles page faults, large files, and synchronization to ensure performance and consistency.
        \end{itemize}
    
    Memory-mapped files integrate file I/O with virtual memory, enabling efficient access and sharing of file data across processes.
    
    \end{highlight}
\end{notes}

The next section that is being covered from this chapter this week is \textbf{Section 10.8: Allocating Kernel Memory}.

\begin{notes}{Section 10.8: Allocating Kernel Memory}
    \subsection*{Overview}

    This section explores kernel memory allocation, differentiating it from user-mode memory management. Kernel memory is allocated for data structures essential to operating system functionality and 
    hardware interaction. Unlike user-mode memory, kernel memory often requires physically contiguous allocation to meet hardware constraints and minimize fragmentation.
    
    \subsubsection*{Kernel Memory Requirements}
    
    Kernel memory allocation addresses specific challenges not encountered in user-mode memory management. Key distinctions include:
    
    \begin{highlight}[Kernel Memory Requirements]
    
    \begin{itemize}
        \item \textbf{Fragmentation Minimization}: Kernel memory requests vary in size, requiring efficient allocation to prevent waste.
        \item \textbf{Contiguity for Hardware Access}: Certain hardware devices bypass the virtual memory interface, necessitating physically contiguous memory.
    \end{itemize}
    
    \end{highlight}
    
    \subsubsection*{Buddy System}
    
    The buddy system allocates memory using a power-of-2 scheme, dividing memory into "buddies" of increasingly smaller sizes to fulfill requests. This hierarchical method facilitates rapid coalescing 
    of adjacent free blocks but can suffer from internal fragmentation.
    
    \begin{highlight}[Buddy System]
    
    \begin{itemize}
        \item \textbf{Allocation Strategy}: Requests are rounded to the nearest power of 2, with memory segments split recursively as needed.
        \item \textbf{Fragmentation}: Allocations often exceed the requested size, potentially wasting space.
        \item \textbf{Coalescing}: Adjacent free buddies can merge to form larger segments, reclaiming contiguous memory.
    \end{itemize}
    
    \end{highlight}
    
    \subsubsection*{Slab Allocation}
    
    Slab allocation offers a more efficient alternative by dividing memory into slabs associated with specific data structures. Each slab contains a predefined number of objects, reducing fragmentation 
    and enabling rapid allocation and deallocation.
    
    \begin{highlight}[Slab Allocation]
    
    \begin{itemize}
        \item \textbf{Cache Structure}: A cache is created for each data structure, consisting of one or more slabs.
        \item \textbf{Object Management}: Objects are preallocated and marked as free or used, optimizing allocation times.
        \item \textbf{Advantages}: 
            \begin{itemize}
                \item \textbf{Minimal Fragmentation}: Exact allocation for objects eliminates wasted memory.
                \item \textbf{Quick Allocation}: Preallocated objects enable rapid reuse, ideal for frequently used structures.
            \end{itemize}
    \end{itemize}
    
    \end{highlight}
    
    \subsubsection*{Kernel Memory Allocators in Practice}
    
    Modern operating systems employ variations of these methods to balance efficiency and hardware compatibility:
    
    \begin{highlight}[Kernel Memory Allocators in Practice]
    
    \begin{itemize}
        \item \textbf{Linux Kernel}: Initially used the buddy system but transitioned to the slab allocator (SLAB), followed by enhancements like SLUB and SLOB for specific use cases.
        \item \textbf{Windows Kernel}: Utilizes hybrid approaches for efficient kernel memory management.
    \end{itemize}
    
    \end{highlight}
    
    \begin{highlight}[Summary of Key Concepts]
    
    \begin{itemize}
        \item \textbf{Kernel Memory Needs}: Address fragmentation and ensure contiguity for hardware.
        \item \textbf{Buddy System}: Simple but prone to internal fragmentation.
        \item \textbf{Slab Allocation}: Efficient allocation for kernel data structures with minimal waste.
        \item \textbf{Practical Implementations}: Evolving techniques in Linux and Windows optimize kernel memory usage.
    \end{itemize}
    
    Kernel memory management ensures the efficient and effective operation of operating system components and hardware interactions.
    \end{highlight}
\end{notes}

The next section that is being covered from this chapter this week is \textbf{Section 10.9: Other Considerations}.

\begin{notes}{Section 10.9: Other Considerations}
    \subsection*{Overview}

    This section examines additional considerations in virtual memory management, addressing factors that influence system performance and behavior. Topics include the handling of program execution 
    stacks, support for non-uniform memory access (NUMA), and the role of virtual memory in system design. These considerations ensure the memory management subsystem is robust and capable of supporting 
    diverse workloads efficiently.
    
    \subsubsection*{Program Execution Stacks}
    
    Every process in a system maintains a stack for managing function calls, local variables, and control flow. Virtual memory provides dynamic growth of stacks, ensuring efficient utilization of memory:
    \begin{itemize}
        \item \textbf{Dynamic Allocation}: Stacks are allowed to grow dynamically within limits set by the operating system.
        \item \textbf{Guard Pages}: Special non-accessible pages are placed at the end of the stack to detect and prevent overflow.
        \item \textbf{Efficiency}: Virtual memory allows processes to use only as much physical memory as needed for the stack at any given time.
    \end{itemize}
    
    \subsubsection*{Non-Uniform Memory Access (NUMA)}
    
    In modern multicore systems, NUMA architectures influence memory access patterns:
    \begin{itemize}
        \item \textbf{NUMA Nodes}: Each CPU or group of CPUs has local memory, which provides faster access than remote memory.
        \item \textbf{Memory Allocation Policies}: Operating systems attempt to allocate memory close to the CPU accessing it to minimize latency.
        \item \textbf{Performance Considerations}: NUMA-aware allocation ensures optimal performance in systems with distributed memory.
    \end{itemize}
    
    \subsubsection*{Role of Virtual Memory in System Design}
    
    Virtual memory serves as a foundation for other system features:
    \begin{itemize}
        \item \textbf{Checkpointing and Snapshots}: Virtual memory enables efficient capture of system state for fault tolerance and recovery.
        \item \textbf{Security}: Memory isolation and access control mechanisms safeguard processes and data.
        \item \textbf{Scalability}: Supports large and complex workloads by decoupling logical and physical memory.
    \end{itemize}
    
    \begin{highlight}[Summary of Key Concepts]
    
        \begin{itemize}
            \item \textbf{Program Stacks}: Virtual memory dynamically allocates stack space and uses guard pages to prevent overflow.
            \item \textbf{NUMA Awareness}: Optimizes memory allocation in systems with non-uniform memory access architectures.
            \item \textbf{Virtual Memory Integration}: Provides a foundation for advanced system features like snapshots, security, and scalability.
        \end{itemize}
    
    These considerations highlight the versatility and critical role of virtual memory in modern system design, addressing both performance and functionality needs.
    
    \end{highlight}
\end{notes}

The last section that is being covered from this chapter this week is \textbf{Section 10.10: Operating System Examples}.

\begin{notes}{Section 10.10: Operating System Examples}
    \subsection*{Overview}

    This section explores how virtual memory is implemented across three operating systems: Linux, Windows, and Solaris. Each system employs unique strategies to manage memory efficiently, leveraging 
    concepts like demand paging, page replacement, and working-set management. These implementations illustrate the flexibility of virtual memory techniques in diverse environments.
    
    \subsubsection*{Linux Virtual Memory Management}
    
    Linux uses demand paging to allocate pages from a list of free frames. Its global page-replacement policy approximates the LRU (Least Recently Used) algorithm through active and inactive page lists.
    
    \begin{highlight}[Linux Virtual Memory Management]
    
    \begin{itemize}
        \item \textbf{Page Lists:}
            \begin{itemize}
                \item \textbf{Active List:} Contains pages in active use.
                \item \textbf{Inactive List:} Holds less recently used pages, eligible for reclamation.
            \end{itemize}
        \item \textbf{Access Tracking:} Pages are tracked via an accessed bit. When pages are referenced, they move to the active list.
        \item \textbf{Reclamation:} The page-out daemon \texttt{kswapd} monitors free memory and reclaims pages when necessary.
    \end{itemize}
    
    \end{highlight}
    
    \subsubsection*{Windows Virtual Memory Management}
    
    Windows implements demand paging with clustering, using working-set management to balance memory allocation dynamically.
    
    \begin{highlight}[Windows Virtual Memory Management]
    
    \begin{itemize}
        \item \textbf{Clustering:} Handles page faults by bringing in neighboring pages, leveraging memory locality.
        \item \textbf{Working Set:}
            \begin{itemize}
                \item \textbf{Minimum and Maximum Limits:} Processes have defined memory allocation boundaries.
                \item \textbf{Dynamic Adjustment:} Memory allocation can expand or contract based on system demand.
        \end{itemize}
        \item \textbf{Architecture Support:} Supports extensive virtual address spaces (e.g., 128 TB on 64-bit systems).
    \end{itemize}
    
    \end{highlight}
    
    \subsubsection*{Solaris Virtual Memory Management}
    
    Solaris employs a similar demand paging approach, integrating robust page replacement strategies and kernel memory management.
    
    \begin{highlight}[Solaris Virtual Memory Management]
    
    \begin{itemize}
        \item \textbf{Paging:} Implements advanced replacement policies, optimizing performance.
        \item \textbf{Kernel Memory:} Leverages unique strategies like slab allocation for kernel memory.
        \item \textbf{Scalability:} Designed for large-scale systems, Solaris accommodates diverse workload demands.
    \end{itemize}
    
    \end{highlight}
    
    \begin{highlight}[Summary of Key Concepts]
    
    \begin{itemize}
        \item \textbf{Demand Paging:} A common thread across all systems, ensuring memory is allocated as needed.
        \item \textbf{Page Replacement:} Strategies like LRU approximation and clustering reduce paging overhead.
        \item \textbf{Working Sets:} Enable dynamic memory allocation to optimize performance.
        \item \textbf{Adaptability:} Each system tailors virtual memory strategies to its architecture and use cases.
    \end{itemize}
    
    Virtual memory implementations highlight the interplay between hardware capabilities and software design, delivering efficiency and scalability in modern operating systems.
    
    \end{highlight}
\end{notes}