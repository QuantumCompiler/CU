\clearpage
\chapter{Study Guide 10}

\section{Matrix Multiplication}
\horizontalline{0}{0}

\begin{center}
    \large{\textbf{Study Guide Instructions}}
\end{center}

\horizontalline{-1}{0}

\begin{itemize}
    \item Submit your work in Gradescope as a PDF - you will identify where your “questions are.”
    \item Identify the question number as you submit.  Since we grade "blind" if the questions are NOT identified, the work WILL NOT BE GRADED and a 0 will be recorded. Always leave enough time to 
    identify the questions when submitting.
    \item One section per page (if a page or less) - We prefer to grade the main solution in a single page, extra work can be included on the following page.
    \item Long instructions may be removed to fit on a single page.
    \item \textbf{Do not start a new question in the middle of a page.}
    \item Solutions to book questions are provided for reference.
    \item You may NOT submit given solutions - this includes minor modifications - as your own.
    \item Solutions that do not show individual engagement with the solutions will be marked as no credit and can be considered a violation of honor code.
    \item If you use the given solutions you must reference or explain how you used them, in particular...
\end{itemize}

\horizontalline{-1}{0}

\begin{center}
    \large{\textbf{Method Selection}}
\end{center}

\horizontalline{-1}{0}

\textbf{For full credit,  EACH book exercise in the Study Guides must use one or more of the following methods and FOR EACH QUESTION.  Identify the number the method by number to ensure full credit.}

\begin{itemize}
    \item \textbf{Method 1} - Provide original examples which demonstrate the ideas of the exercise in addition to your solution.
    \item \textbf{Method 2} - Include and discuss the specific topics needed from the chapter and how they relate to the question.
    \item \textbf{Method 3} - Include original Python code, of reasonable length (as screenshot or text)  to show how the topic or concept was explored.
    \item \textbf{Method 4} - Expand the given solution in a significant way, with additional steps and comments. All steps are justified. This is a good method for a proof for which you are only given a basic outline.
    \item \textbf{Method 5} - Attempt the exercise without looking at the solution and then the solution is used to check work. Words are used to describe the results.
    \item \textbf{Method 6} - Provide an analysis of the strategies used to understand the exercise, describing in detail what was challenging, who helped you or what resources were used. The process of understanding is
    described.
\end{itemize}

% Problem 1
\begin{problem}{Problem 1}
    \begin{statement}{Problem Statement}
        Consider pages 179 - 181.  Demonstrate with simple matrix examples

        \begin{itemize}
            \item The 4 properties of matrix multiplication.
            \item The inner product and matrix-vector products.
            \item A block matrix multiplication.
            \item A column interpretation of matrix-matrix product.
            \item Multiple sets of linear equations (skip row interpretation if you wish).
            \item Inner product representation.
            \item Gram Matrix.
            \item Outer Product representation.
        \end{itemize}

        You can use the same simple matrices where possible - however, don't use the identity matrix. You may wish to do more examples to illustrate the additional types of products given. You may use 
        multiple pages.
    \end{statement}

    \begin{Highlight}[Solution]
        For this problem, the two matrices that we are going to use for demonstration is

        \setcounter{equation}{0}
        \begin{equation}
            A = 
            \begin{bmatrix}
                1 & 2 \\
                3 & 4 \\
            \end{bmatrix}
            \hspace*{10pt} , \hspace*{10pt}
            B = 
            \begin{bmatrix}
                5 & 6 \\
                7 & 8 \\
            \end{bmatrix}
            \hspace*{10pt} , \hspace*{10pt}
            C = 
            \begin{bmatrix}
                9 & 10 \\
                11 & 12 \\
            \end{bmatrix}
            \hspace*{10pt} , \hspace*{10pt}
            x = 
            \begin{bmatrix}
                1 \\
                2 \\
            \end{bmatrix}
            \hspace*{10pt} , \hspace*{10pt}
            y = 
            \begin{bmatrix}
                3 \\
                4 \\
            \end{bmatrix}
            \hspace*{10pt} , \hspace*{10pt}
            \gamma = 2.
        \end{equation}
        \noindent \textbf{Four Properties Of Matrix Multiplication} \vspace*{1em}

        We begin with the \textbf{associativity} property. Namely

        \begin{equation}
            (AB)C = A(BC).
        \end{equation}

        We first begin with $(AB)C$
        \begin{align*}
            (AB)C & = 
            \Bigg(
                \begin{bmatrix}
                    1 & 2 \\
                    3 & 4 \\
                \end{bmatrix}
                \begin{bmatrix}
                    5 & 6 \\
                    7 & 8 \\
                \end{bmatrix}
            \Bigg)
            \begin{bmatrix}
                9 & 10 \\
                11 & 12 \\
            \end{bmatrix}
            = 
            \Bigg( 
                \begin{bmatrix}
                    1(5) + 2(7) & 1(6) + 2(8) \\
                    3(5) + 4(7) & 3(6) + 4(8) \\ 
                \end{bmatrix}
            \Bigg)
            \begin{bmatrix}
                9 & 10 \\
                11 & 12 \\
            \end{bmatrix} \\
            & = 
            \begin{bmatrix}
                19 & 22 \\
                43 & 50 \\
            \end{bmatrix}
            \begin{bmatrix}
                9 & 10 \\
                11 & 12 \\
            \end{bmatrix}
            = 
            \begin{bmatrix}
                19(9) + 22(11) & 19(10) + 22(12) \\
                43(9) + 50(11) & 43(10) + 50(12) \\
            \end{bmatrix}
            =
            \begin{bmatrix}
                413 & 454 \\
                937 & 1030 \\
            \end{bmatrix}.
        \end{align*}
        Next we show that the last line of $(AB)C$ will be equal to $A(BC)$
        \begin{align*}
            A(BC) & = 
            \begin{bmatrix}
                1 & 2 \\
                3 & 4 \\
            \end{bmatrix}
            \Bigg( 
                \begin{bmatrix}
                    5 & 6 \\
                    7 & 8 \\
                \end{bmatrix}
                \begin{bmatrix}
                    9 & 10 \\
                    11 & 12 \\
                \end{bmatrix}
            \Bigg) 
            = 
            \begin{bmatrix}
                1 & 2 \\
                3 & 4 \\
            \end{bmatrix}
            \Bigg(
                \begin{bmatrix}
                    7(9) + 6(11) & 5(10) + 6(12) \\
                    7(9) + 8(11) & 7(10) + 8(12) \\
                \end{bmatrix}
            \Bigg) \\
            & = 
            \begin{bmatrix}
                1 & 2 \\
                3 & 4 \\
            \end{bmatrix}
            \begin{bmatrix}
                111 & 122 \\
                151 & 166 \\
            \end{bmatrix}
            = 
            \begin{bmatrix}
                1(111) + 2(151) & 1(122) + 2(166) \\
                3(111) + 4(151) & 3(122) + 4(166) \\
            \end{bmatrix}
            = 
            \begin{bmatrix}
                413 & 454 \\
                937 & 1030 \\
            \end{bmatrix}.
        \end{align*}
        From the above results we have demonstrated the property found in (2), namely the \textbf{associativity} property.

        We next will demonstrate \textbf{associativity with scalar multiplication}, namely

        \begin{equation}
            \gamma (AB) = (\gamma A)B.
        \end{equation}
        Using the values for matrices and scalar values in (1) for $\gamma (AB)$ we have
        \begin{align*}
            \gamma (AB) & = (2)
            \Bigg(
                \begin{bmatrix}
                    1 & 2 \\
                    3 & 4 \\
                \end{bmatrix}
                \begin{bmatrix}
                    5 & 6 \\
                    7 & 8 \\
                \end{bmatrix}
            \Bigg)
            = 2 
            \begin{bmatrix}
                19 & 22 \\
                43 & 50 \\
            \end{bmatrix}
            = 
            \begin{bmatrix}
                38 & 44 \\
                86 & 100 \\
            \end{bmatrix}.
        \end{align*}
        We now move on to calculating $(\gamma A)B$, for this we have
        \begin{align*}
            (\gamma A)B & = 
            \Bigg(
                (2)
                \begin{bmatrix}
                    1 & 2 \\
                    3 & 4 \\
                \end{bmatrix}
            \Bigg)
            \begin{bmatrix}
                5 & 6 \\
                7 & 8 \\
            \end{bmatrix}
            = 
            \begin{bmatrix}
                2 & 4 \\
                6 & 8 \\
            \end{bmatrix}
            \begin{bmatrix}
                5 & 6 \\
                7 & 8 \\
            \end{bmatrix}
            = 
            \begin{bmatrix}
                2(5) + 4(7) & 2(6) + 4(8) \\
                6(5) + 8(7) & 6(6) + 8(8) \\
            \end{bmatrix}
            = 
            \begin{bmatrix}
                28 & 44 \\
                86 & 100 \\
            \end{bmatrix}.
        \end{align*}
        From we the above two results, we have shown the property in (3), namely the \textbf{associativity with scalar multiplication}.

        We will next show the \textbf{distributivity with addition} property. Namely
        \begin{align}
            A(B + C) & = AB + AC \\
            (A + B)C & = AC + BC.
        \end{align}
        We first begin by showing the property depicted by (4). The LHS of (4) is
        \begin{align*}
            A(B + C) & = 
            \begin{bmatrix}
                1 & 2 \\
                3 & 4 \\
            \end{bmatrix}
            \Bigg(
                \begin{bmatrix}
                    5 & 6 \\
                    7 & 8 \\
                \end{bmatrix}
                + 
                \begin{bmatrix}
                    9 & 10 \\
                    11 & 12 \\
                \end{bmatrix}
            \Bigg)
            = 
            \begin{bmatrix}
                1 & 2 \\
                3 & 4 \\
            \end{bmatrix}
            \Bigg(
                \begin{bmatrix}
                    5 + 9 & 6 + 10 \\
                    7 + 11 & 8 + 12 \\
                \end{bmatrix}
            \Bigg)
            = 
            \begin{bmatrix}
                1 & 2 \\
                3 & 4 \\
            \end{bmatrix}
            \begin{bmatrix}
                14 & 16 \\
                18 & 20 \\
            \end{bmatrix} \\
            & = 
            \begin{bmatrix}
                1(14) + 2(18) & 1(16) + 2(20) \\
                3(14) + 4(18) & 3(16) + 4(20) \\
            \end{bmatrix}
            = 
            \begin{bmatrix}
                50 & 56 \\
                114 & 128 \\
            \end{bmatrix}.
        \end{align*}
        The RHS of (4) is
        \begin{align*}
            AB + BC & = 
            \begin{bmatrix}
                1 & 2 \\
                3 & 4 \\
            \end{bmatrix}
            \begin{bmatrix}
                5 & 6 \\
                7 & 8 \\
            \end{bmatrix}
            +
            \begin{bmatrix}
                1 & 2 \\
                3 & 4 \\
            \end{bmatrix}
            \begin{bmatrix}
                9 & 10 \\
                11 & 12 \\
            \end{bmatrix}
            = 
            \begin{bmatrix}
                19 & 22 \\
                43 & 50 \\
            \end{bmatrix}
            + 
            \begin{bmatrix}
                31 & 34 \\
                71 & 78 \\
            \end{bmatrix}
            =
            \begin{bmatrix}
                19 + 31 & 22 + 34 \\
                43 + 71 & 50 + 78 \\
            \end{bmatrix} \\
            & =
            \begin{bmatrix}
                50 & 56 \\
                114 & 128 \\
            \end{bmatrix}.
        \end{align*}
        From the results above we have shown the property found in (4). We now move on to showing (5). The LHS of (5) is
        \begin{align*}
            (A + B)C & = 
            \Bigg(
                \begin{bmatrix}
                    1 & 2 \\
                    3 & 4 \\
                \end{bmatrix}
                + 
                \begin{bmatrix}
                    5 & 6 \\
                    7 & 8 \\
                \end{bmatrix}
            \Bigg)
            \begin{bmatrix}
                9 & 10 \\
                11 & 12 \\
            \end{bmatrix}
            =
            \Biggl(
                \begin{bmatrix}
                    1 + 5 & 2 + 6 \\
                    3 + 7 & 4 + 18 \\
                \end{bmatrix}
            \Biggr)
            \begin{bmatrix}
                9 & 10 \\
                11 & 12 \\
            \end{bmatrix}
            = 
            \begin{bmatrix}
                6 & 8 \\
                10 & 22 \\
            \end{bmatrix}
            \begin{bmatrix}
                9 & 10 \\
                11 & 12 \\
            \end{bmatrix} \\
            & = 
            \begin{bmatrix}
                6(9) + 8(11) & 6(10) + 8(12) \\
                10(9) + 22(11) & 10(10) + 22(12) \\
            \end{bmatrix}
            = 
            \begin{bmatrix}
                142 & 156 \\
                222 & 244 \\
            \end{bmatrix}.
        \end{align*}
        The RHS of (5) is then
        \begin{align*}
            AC + BC & = 
            \begin{bmatrix}
                1 & 2 \\
                3 & 4 \\
            \end{bmatrix}
            \begin{bmatrix}
                9 & 10 \\
                11 & 12 \\
            \end{bmatrix}
            +
            \begin{bmatrix}
                5 & 6 \\
                7 & 8 \\
            \end{bmatrix}
            \begin{bmatrix}
                9 & 10 \\
                11 & 12 \\
            \end{bmatrix}
            = 
            \begin{bmatrix}
                31 & 34 \\
                71 & 78 \\
            \end{bmatrix}
            + 
            \begin{bmatrix}
                111 & 122 \\
                151 & 166 \\
            \end{bmatrix}
            = 
            \begin{bmatrix}
                31 + 111 & 34 + 122 \\
                71 + 151 & 78 + 166 \\
            \end{bmatrix} \\
            & = 
            \begin{bmatrix}
                142 & 156 \\
                222 & 244 \\
            \end{bmatrix}.
        \end{align*}
        From the results above we have shown the property found in (5). The results from the previous four calculations have successfully shown the \textbf{distributivity with addition}.

        The last of the four \textbf{properties of matrix multiplication} is the \textbf{transpose of product}. Namely

        \begin{equation}
            (AB)^{T} = B^{T}A^{T}.
        \end{equation}
        Using the definitions found in (1) we will now show (6). Using these matrices we now start with the LHS of (6). The LHS is then
        \begin{align*}
            (AB)^{T} & = 
            \biggl(
                \begin{bmatrix}
                    1 & 2 \\
                    3 & 4 \\
                \end{bmatrix}
                \begin{bmatrix}
                    5 & 6 \\
                    7 & 8 \\
                \end{bmatrix}
            \biggr)^{T}
            = 
            \begin{bmatrix}
                19 & 22 \\
                43 & 50 \\
            \end{bmatrix}^{T}
            = 
            \begin{bmatrix}
                19 & 43 \\
                22 & 50 \\
            \end{bmatrix}.
        \end{align*}
        The RHS of (6) is then
        \begin{align*}
            B^{T}A^{T} & = 
            \begin{bmatrix}
                5 & 6 \\
                7 & 8 \\
            \end{bmatrix}^{T}
            \begin{bmatrix}
                1 & 2 \\
                3 & 4 \\
            \end{bmatrix}^{T}
            = 
            \begin{bmatrix}
                5 & 7 \\
                6 & 8 \\
            \end{bmatrix}
            \begin{bmatrix}
                1 & 3 \\
                2 & 4 \\
            \end{bmatrix}
            = 
            \begin{bmatrix}
                5(1) + 7(2) & 5(3) + 7(4) \\
                6(1) + 8(2) & 6(3) + 8(4) \\
            \end{bmatrix}
            = 
            \begin{bmatrix}
                19 & 43 \\
                22 & 50 \\
            \end{bmatrix}.
        \end{align*}
        From the previous two results we have adequately shown the property depicted in (6). Therefore we have shown the \textbf{transpose of product}. We will now move on to other matrix multiplication
        properties. \vspace*{1em}

        \noindent \textbf{Inner Product And Matrix-Vector Products} \vspace*{1em}

        The \textbf{inner product and matrix-vector products} is 

        \begin{equation}
            y^{T}(Ax) = (y^{T}A)x = (A^{T}y)^{T}x.
        \end{equation}
        Using the definitions in (1) we can show these properties. These properties with the defined examples are 
        \begin{align*}
            y^{T}(Ax) & = 
            \begin{bmatrix}
                3 & 4 \\
            \end{bmatrix}
            \Biggl(
                \begin{bmatrix}
                    1 & 2 \\
                    3 & 4 \\
                \end{bmatrix}
                \begin{bmatrix}
                    1 \\
                    2 \\
                \end{bmatrix}
            \Biggr)
            = 
            \begin{bmatrix}
                3 & 4 \\
            \end{bmatrix}
            \begin{bmatrix}
                1(1) + 2(2) \\
                3(1) + 4(2) \\
            \end{bmatrix}
            =
            \begin{bmatrix}
                3 & 4 \\
            \end{bmatrix}
            \begin{bmatrix}
                5 \\
                11 \\
            \end{bmatrix}
            = 3(5) + 4(11) = 15 + 44 = 59 \\
            (y^{T}A)x & = 
            \Biggl(
                \begin{bmatrix}
                    3 & 4 \\
                \end{bmatrix}
                \begin{bmatrix}
                    1 & 2 \\
                    3 & 4 \\
                \end{bmatrix}
            \Biggr)
            \begin{bmatrix}
                1 \\
                2 \\
            \end{bmatrix}
            = 
            \begin{bmatrix}
                3(1) + 3(4) & 3(2) + 4(4) \\
            \end{bmatrix}
            \begin{bmatrix}
                1 \\
                2 \\
            \end{bmatrix}
            =
            \begin{bmatrix}
                15 & 22 \\
            \end{bmatrix}
            \begin{bmatrix}
                1 \\
                2 \\
            \end{bmatrix}
            = 15 + 44 = 59 \\
            (A^{T}y)^{T}x & = 
            \Biggl(
                \begin{bmatrix}
                    1 & 2 \\
                    3 & 4 \\
                \end{bmatrix}^{T}
                \begin{bmatrix}
                    3 \\
                    4 \\
                \end{bmatrix}
            \Biggr)^{T}
            \begin{bmatrix}
                1 \\
                2 \\
            \end{bmatrix}
            = 
            \Biggl(
                \begin{bmatrix}
                    1 & 3 \\
                    2 & 4 \\
                \end{bmatrix}
                \begin{bmatrix}
                    3 \\
                    4 \\
                \end{bmatrix}
            \Biggr)^{T}
            \begin{bmatrix}
                1 \\
                2 \\
            \end{bmatrix}
            = 
            \begin{bmatrix}
                1(3) + 3(4) \\
                2(3) + 4(4) \\
            \end{bmatrix}^{T}
            \begin{bmatrix}
                1 \\
                2 \\
            \end{bmatrix}
            = 
            \begin{bmatrix}
                15 \\
                22 \\
            \end{bmatrix}^{T}
            \begin{bmatrix}
                1 \\
                2 \\
            \end{bmatrix} \\
            & = 
            \begin{bmatrix}
                15 & 22 \\
            \end{bmatrix}
            \begin{bmatrix}
                1 \\
                2 \\
            \end{bmatrix}
            = 
            15(1) + 22(2) = 15 + 44 = 59.
        \end{align*}
        From the above, we can see that all three expressions in (7) have been satisfied. Thus we have adequately shown the \textbf{inner product and matrix-vector products}. \vspace*{1em}

        \noindent \textbf{Block Matrix Multiplication} \vspace*{1em}

        To demonstrate the \textbf{block matrix multiplication} we must define some more matrices along those found in (1)j. These new matrices are 

        \begin{equation}
            D = 
            \begin{bmatrix}
                1 & 1 \\
                0 & 0 \\
            \end{bmatrix}
            \hspace*{5pt} , \hspace*{5pt}
            E = 
            \begin{bmatrix}
                1 & 0 \\
                1 & 0 \\
            \end{bmatrix}
            \hspace*{5pt} , \hspace*{5pt}
            F = 
            \begin{bmatrix}
                1 & 0 \\
                0 & 0 \\
            \end{bmatrix}
            \hspace*{5pt} , \hspace*{5pt}
            G = 
            \begin{bmatrix}
                0 & 1 \\
                0 & 0 \\
            \end{bmatrix}
            \hspace*{5pt} , \hspace*{5pt}
            H = 
            \begin{bmatrix}
                0 & 0 \\
                0 & 1 \\
            \end{bmatrix}.
        \end{equation}
        The identity for \textbf{block matrix multiplication} is 

        \begin{equation}
            \begin{bmatrix}
                A & B \\
                C & D \\
            \end{bmatrix}
            \begin{bmatrix}
                E & F \\
                G & H \\
            \end{bmatrix}
            = 
            \begin{bmatrix}
                AE + BG & AF + BH \\
                CE + DG & CF + DH \\
            \end{bmatrix}.
        \end{equation}
        For brevity's sake I am going to abbreviate some of the matrix multiplications for this property. Beginning with the LHS of (9) we have
        \begin{align*}
            \begin{bmatrix}
                A & B \\
                C & D \\
            \end{bmatrix}
            \begin{bmatrix}
                E & F \\
                G & H \\
            \end{bmatrix}
            = 
            \begin{bmatrix}
                1 & 2 & 5 & 6 \\
                3 & 4 & 7 & 8 \\
                9 & 10 & 1 & 1 \\
                11 & 12 & 0 & 0 \\
            \end{bmatrix}
            \begin{bmatrix}
                1 & 0 & 1 & 0 \\
                1 & 0 & 0 & 0 \\
                0 & 1 & 0 & 0 \\
                0 & 0 & 0 & 1 \\
            \end{bmatrix}
            = 
            \begin{bmatrix}
                3 & 5 & 1 & 6 \\
                7 & 7 & 3 & 8 \\
                19 & 1 & 9 & 1 \\
                23 & 0 & 11 & 0 \\
            \end{bmatrix}.
        \end{align*}
        We will now show the RHS of (9). To do this, we will calculate the multiplications of each sub matrix. These multiplications are then 
        \begin{align*}
            AE + BG & = 
            \begin{bmatrix}
                1 & 2 \\
                3 & 4 \\
            \end{bmatrix}
            \begin{bmatrix}
                1 & 0 \\
                1 & 0 \\
            \end{bmatrix}
            + 
            \begin{bmatrix}
                5 & 6 \\
                7 & 8 \\
            \end{bmatrix}
            \begin{bmatrix}
                0 & 1 \\
                0 & 0 \\
            \end{bmatrix}
            = 
            \begin{bmatrix}
                3 & 0 \\
                7 & 0 \\
            \end{bmatrix}
            + 
            \begin{bmatrix}
                0 & 5 \\
                0 & 7 \\
            \end{bmatrix}
            = 
            \begin{bmatrix}
                3 & 5 \\
                7 & 7 \\
            \end{bmatrix} \\
            AF + BH & = 
            \begin{bmatrix}
                1 & 2 \\
                3 & 4 \\
            \end{bmatrix}
            \begin{bmatrix}
                1 & 0 \\
                0 & 0 \\
            \end{bmatrix}
            + 
            \begin{bmatrix}
                5 & 6 \\
                7 & 8 \\
            \end{bmatrix}
            \begin{bmatrix}
                0 & 0 \\
                0 & 1 \\
            \end{bmatrix}
            = 
            \begin{bmatrix}
                1 & 0 \\
                3 & 0 \\
            \end{bmatrix}
            + 
            \begin{bmatrix}
                0 & 6 \\
                0 & 8 \\
            \end{bmatrix}
            = 
            \begin{bmatrix}
                1 & 6 \\
                2 & 8 \\
            \end{bmatrix} \\
            CE + DG & = 
            \begin{bmatrix}
                9 & 10 \\
                11 & 12 \\
            \end{bmatrix}
            \begin{bmatrix}
                1 & 0 \\
                1 & 0 \\
            \end{bmatrix}
            + 
            \begin{bmatrix}
                1 & 1 \\
                0 & 0 \\
            \end{bmatrix}
            \begin{bmatrix}
                0 & 1 \\
                0 & 0 \\
            \end{bmatrix}
            = 
            \begin{bmatrix}
                19 & 0 \\
                23 & 0 \\
            \end{bmatrix}
            + 
            \begin{bmatrix}
                0 & 1 \\
                0 & 0 \\
            \end{bmatrix}
            =
            \begin{bmatrix}
                19 & 1 \\
                23 & 0 \\
            \end{bmatrix} \\
            CF + DH & =
            \begin{bmatrix}
                9 & 10 \\
                11 & 12 \\
            \end{bmatrix}
            \begin{bmatrix}
                1 & 0 \\
                0 & 0 \\
            \end{bmatrix}
            + 
            \begin{bmatrix}
                1 & 1 \\
                0 & 0 \\
            \end{bmatrix}
            \begin{bmatrix}
                0 & 0 \\
                0 & 1 \\
            \end{bmatrix}
            =
            \begin{bmatrix}
                9 & 0 \\
                11 & 0 \\
            \end{bmatrix}
            + 
            \begin{bmatrix}
                0 & 1 \\
                0 & 0 \\
            \end{bmatrix}
            =
            \begin{bmatrix}
                9 & 1 \\
                11 & 0 \\
            \end{bmatrix}.
        \end{align*}
        Stitching all of these results together we now have 
        \begin{equation*}
            \begin{bmatrix}
                AE + BG & AF + BH \\
                CE + DG & CF + DH \\
            \end{bmatrix}
            = 
            \begin{bmatrix}
                3 & 5 & 1 & 6 \\
                7 & 7 & 3 & 8 \\
                19 & 1 & 9 & 1 \\
                23 & 0 & 11 & 0 \\
            \end{bmatrix}.
        \end{equation*}
        The previous results have shown and verified the \textbf{block matrix multiplication} property with our given examples. \vspace*{1em}

        \noindent \textbf{Column Interpretation Of Matrix-Matrix Product} \vspace*{1em}

        The \textbf{column interpretation of matrix-matrix product} is as follows

        \begin{equation}
            AB = A 
            \begin{bmatrix}
                b_{1} & b_{2} & \dots & b_{n} \\
            \end{bmatrix}
            = 
            \begin{bmatrix}
                Ab_{1} & Ab_{2} & \dots & Ab_{n} \\
            \end{bmatrix}.
        \end{equation}
        Using the matrices from (1) we can demonstrate (10) as 
        \begin{align*}
            AB & =
            \begin{bmatrix}
                \begin{bmatrix}
                    1 & 2 \\
                    3 & 4 \\
                \end{bmatrix}
                \begin{bmatrix}
                    5 \\
                    7 \\
                \end{bmatrix} & 
                \begin{bmatrix}
                    1 & 2 \\
                    3 & 4 \\
                \end{bmatrix}
                \begin{bmatrix}
                    6 \\
                    8 \\
                \end{bmatrix} \\
            \end{bmatrix}
            = 
            \begin{bmatrix}
                \begin{bmatrix}
                    1(5) + 2(7) \\
                    3(5) + 4(7) \\
                \end{bmatrix} &
                \begin{bmatrix}
                    1(6) + 2(8) \\
                    3(6) + 4(8) \\
                \end{bmatrix} \\
            \end{bmatrix}
            = 
            \begin{bmatrix}
                \begin{bmatrix}
                    19 \\
                    43 \\
                \end{bmatrix} &
                \begin{bmatrix}
                    22 \\
                    50 \\
                \end{bmatrix} \\
            \end{bmatrix}
            = 
            \begin{bmatrix}
                19 & 22 \\
                43 & 50 \\
            \end{bmatrix}.
        \end{align*}
        In the above, we have correctly demonstrated the \textbf{column interpretation of matrix-matrix product} property by multiplying the matrix $A$ by each column of $B$. \vspace*{1em}

        \noindent \textbf{Multiple Sets Of Linear Equations} \vspace*{1em}

        The \textbf{multiple sets of linear equation} is as follows 

        \begin{equation}
            AX = B.
        \end{equation}
        The matrix $X$ is just a generalized matrix of variables. Using the matrices in (1) (11) can be expressed as 
        \begin{align*}
            AX = 
            \begin{bmatrix}
                1 & 2 \\
                3 & 4 \\
            \end{bmatrix}
            \begin{bmatrix}
                x_{1} & x_{2} \\
                x_{3} & x_{4} \\
            \end{bmatrix}
            = 
            \begin{bmatrix}
                x_{1} + 2x_{3} & x_{2} + 2x_{4} \\
                3x_{1} + 4x_{3} & 3x_{2} + 4x_{4} \\
            \end{bmatrix}
            = 
            \begin{bmatrix}
                5 & 6 \\
                7 & 8 \\
            \end{bmatrix} = B.
        \end{align*}
        The results from the above result in the following linear equations
        \begin{align*}
            x_{1} + 2x_{3} & = 5 \\
            x_{2} + 2x_{4} & = 6 \\
            3x_{1} + 4x_{3} & = 7 \\
            3x_{2} + 4x_{4} & = 8.
        \end{align*}
        The goal of (11) is to solve for $X$, using the matrices defined in (1) we can then solve for $X$ with 
        \begin{align*}
            X & = A^{-1}B = 
            \begin{bmatrix}
                1 & 2 \\
                3 & 4 \\
            \end{bmatrix}^{-1}
            \begin{bmatrix}
                5 & 6 \\
                7 & 8 \\
            \end{bmatrix}
            = 
            \begin{bmatrix}
                -2 & 1 \\
                \frac{3}{2} & -\frac{1}{2} \\
            \end{bmatrix}
            \begin{bmatrix}
                5 & 6 \\
                7 & 8 \\
            \end{bmatrix}
            = 
            \begin{bmatrix}
                -3 & -4 \\
                4 & 5 \\
            \end{bmatrix}
        \end{align*}
        The above then means the variables for $X$ are 
        \begin{align*}
            x_{1} & = -3 \\
            x_{2} & = -4 \\
            x_{3} & = 7 \\
            x_{4} & = 5.
        \end{align*}
        We have adequately shown the \textbf{multiple sets of linear equation} property. \vspace*{1em}

        \noindent \textbf{Inner Product Representation} \vspace*{1em}

        The \textbf{inner product representation} is

        \begin{equation}
            AB = 
            \begin{bmatrix}
                a_{1}^{T}b_{1} & a_{1}^{T}b_{2} & \dots & a_{1}^{T}b_{n} \\
                a_{2}^{T}b_{1} & a_{2}^{T}b_{2} & \dots & a_{2}^{T}b_{n} \\
                \vdots & \vdots & \ddots & \vdots \\
                a_{m}^{T}b_{1} & a_{m}^{T}b_{2} & \dots & a_{m}^{T}b_{n} \\
            \end{bmatrix}.
        \end{equation}
        Using the matrices from (1) we can demonstrate (12). This demonstration is then
        \begin{align*}
            a_{1}^{T}b_{1} & = 
            \begin{bmatrix}
                1 & 2 \\
            \end{bmatrix}
            \begin{bmatrix}
                5 \\
                7 \\
            \end{bmatrix}
            = 1(5) + 2(7) = 5 + 14 = 19 \\
            a_{1}^{T}b_{2} & = 
            \begin{bmatrix}
                1 & 2 \\
            \end{bmatrix}
            \begin{bmatrix}
                6 \\
                8 \\
            \end{bmatrix}
            = 1(6) + 2(8) = 6 + 16 = 22 \\
            a_{2}^{T}b_{1} & = 
            \begin{bmatrix}
                3 & 4 \\
            \end{bmatrix}
            \begin{bmatrix}
                5 \\
                7 \\
            \end{bmatrix}
            = 3(5) + 4(7) = 15 + 28 = 43 \\
            a_{2}^{T}b_{2} & = 
            \begin{bmatrix}
                3 & 4 \\
            \end{bmatrix}
            \begin{bmatrix}
                6 \\
                8 \\
            \end{bmatrix}
            = 3(6) + 4(8) = 18 + 32 = 50.
        \end{align*}
        Stitching this all together we then have
        
        \begin{equation*}
            AB = 
            \begin{bmatrix}
                a_{1}^{T}b_{1} & a_{1}^{T}b_{2} \\
                a_{2}^{T}b_{1} & a_{2}^{T}b_{2} \\
            \end{bmatrix}
            = 
            \begin{bmatrix}
                19 & 22 \\
                43 & 50 \\
            \end{bmatrix}.
        \end{equation*}
        From the previous result we have adequately demonstrated \textbf{inner product representation}. \vspace*{1em}

        \noindent \textbf{Gram Matrix} \vspace*{1em}

        The \textbf{Gram matrix} is as follows

        \begin{equation}
            G = A^{T}A = 
            \begin{bmatrix}
                a_{1}^{T}a_{1} & a_{1}^{T}a_{2} & \dots & a_{1}^{T}a_{n} \\
                a_{2}^{T}a_{1} & a_{2}^{T}a_{2} & \dots & a_{2}^{T}a_{n} \\
                \vdots & \vdots & \ddots & \vdots \\
                a_{m}^{T}a_{1} & a_{m}^{T}a_{2} & \dots & a_{m}^{T}a_{n} \\
            \end{bmatrix}.
        \end{equation}
        Using the matrix defined in (1) the \textbf{Gram matrix} representation of (13) is then 
        \begin{align*}
            a_{1}^{T}a_{1} & = 
            \begin{bmatrix}
                1 & 3 \\
            \end{bmatrix}
            \begin{bmatrix}
                1 \\
                3 \\
            \end{bmatrix}
            = 1(1) + 3(3) = 1 + 9 = 10 \\
            a_{1}^{T}a_{2} & = 
            \begin{bmatrix}
                1 & 3 \\
            \end{bmatrix}
            \begin{bmatrix}
                2 \\
                4 \\
            \end{bmatrix}
            = 1(2) + 3(4) = 2 + 12 = 14 \\
            a_{2}^{T}a_{2} & = 
            \begin{bmatrix}
                2 & 4 \\
            \end{bmatrix}
            \begin{bmatrix}
                1 \\
                3 \\
            \end{bmatrix}
            = 2(1) + 4(3) = 2 + 12 = 14 \\
            a_{2}^{T}b_{2} & = 
            \begin{bmatrix}
                2 & 4 \\
            \end{bmatrix}
            \begin{bmatrix}
                2 \\
                4 \\
            \end{bmatrix}
            = 2(2) + 4(4) = 4 + 16 = 20.
        \end{align*}
        Stitching this all together we then have 

        \begin{equation*}
            G = A^{T}A = 
            \begin{bmatrix}
                a_{1}^{T}a_{1} & a_{1}^{T}a_{2} \\
                a_{2}^{T}a_{1} & a_{2}^{T}a_{2} \\
            \end{bmatrix}
            = 
            \begin{bmatrix}
                10 & 14 \\
                14 & 20 \\
            \end{bmatrix}.
        \end{equation*}
        With the previous results we have adequately shown the \textbf{Gram matrix} representation. \vspace*{1em}

        \noindent \textbf{Outer Product Representation} \vspace*{1em}

        The \textbf{outer product representation} is as follows

        \begin{equation}
            AB = a_{1}b_{1}^{T} + \dots + a_{p}b_{p}^{T}.
        \end{equation}
        Using the matrices defined in (1) the \textbf{outer product representation} defined in (14) is 
        \begin{align*}
            a_{1}b_{1}^{T} & = 
            \begin{bmatrix}
                1 \\
                3 \\
            \end{bmatrix}
            \begin{bmatrix}
                5 & 6 \\
            \end{bmatrix}
            = 
            \begin{bmatrix}
                1(5) & 1(6) \\
                3(5) & 3(6) \\
            \end{bmatrix}
            = 
            \begin{bmatrix}
                5 & 6 \\
                15 & 18 \\
            \end{bmatrix} \\
            a_{2}b_{2}^{T} & = 
            \begin{bmatrix}
                2 \\
                4 \\
            \end{bmatrix}
            \begin{bmatrix}
                7 & 8 \\
            \end{bmatrix}
            = 
            \begin{bmatrix}
                2(7) & 2(8) \\
                4(7) & 4(8) \\
            \end{bmatrix}
            = 
            \begin{bmatrix}
                14 & 16 \\
                28 & 32 \\
            \end{bmatrix}.
        \end{align*}
        Stitching this all together we then have 

        \begin{equation*}
            AB = a_{1}b_{1}^{T} + a_{2}b_{2}^{T} = 
            \begin{bmatrix}
                5 & 6 \\
                15 & 18 \\
            \end{bmatrix}
            + 
            \begin{bmatrix}
                14 & 16 \\
                28 & 32 \\
            \end{bmatrix}
            = 
            \begin{bmatrix}
                5 + 14 & 6 + 16 \\
                15 + 28 & 18 + 32 \\
            \end{bmatrix}
            = 
            \begin{bmatrix}
                19 & 22 \\
                43 & 50 \\
            \end{bmatrix}.
        \end{equation*}
        From the above result we have adequately shown the \textbf{outer product representation} of a matrix. \vspace*{1em}

        \noindent \textbf{Summary} \vspace*{1em}

        Many of these examples `recycled' previous calculations to save space.
    \end{Highlight}
\end{problem}

% Problem 1 Summary
\begin{summary}{Problem 1 Summary}
    \begin{statement}{Procedure}
        \begin{itemize}
            \item For each property, generate an example for each one and show how the property is carried out arithmetically.
        \end{itemize}
    \end{statement}
    \begin{statement}{Key Concepts}
        \begin{itemize}
            \item \textbf{Four Properties of Matrix Multiplication:}
            \begin{enumerate}
                \item \textbf{Associativity}: Demonstrated using $(AB)C = A(BC)$ where $A$, $B$, and $C$ are matrices.
                \item \textbf{Associativity with Scalar Multiplication}: Shown through $\lambda(AB) = (\lambda A)B$.
                \item \textbf{Distributivity with Addition}: Verified using $A(B + C) = AB + AC$ and $(A + B)C = AC + BC$.
                \item \textbf{Transpose of Product}: Illustrated by $(AB)^T = B^T A^T$.
            \end{enumerate}
            \item \textbf{Inner Product and Matrix-Vector Products:}
            \begin{itemize}
                \item Demonstrated the property $y^T(Ax) = (y^T A)x = (A^T y)^T x$.
                \item Used simple matrices to show the equality of these expressions.
            \end{itemize}
            \item \textbf{Block Matrix Multiplication:}
            \begin{itemize}
                \item Showcased the multiplication of block matrices.
                \item Illustrated using block matrices and their multiplication property.
            \end{itemize}
            \item \textbf{Column Interpretation of Matrix-Matrix Product:}
            \begin{itemize}
                \item Demonstrated that $AB = A[b_1 \ b_2 \ \ldots \ b_n] = [Ab_1 \ Ab_2 \ \ldots \ Ab_n]$.
                \item Provided a clear example using specific matrices.
            \end{itemize}
            \item \textbf{Multiple Sets of Linear Equations:}
            \begin{itemize}
                \item Explored the concept using the matrix equation $AX = B$.
                \item Solved for matrix $X$ using given matrices.
            \end{itemize}
            \item \textbf{Inner Product Representation:}
            \begin{itemize}
                \item Illustrated the inner product representation of matrix multiplication.
                \item Used example matrices to demonstrate the property.
            \end{itemize}
            \item \textbf{Gram Matrix:}
            \begin{itemize}
                \item Explored the concept of the Gram matrix $G = A^T A$.
                \item Computed the Gram matrix using provided matrices.
            \end{itemize}
            \item \textbf{Outer Product Representation:}
            \begin{itemize}
                \item Demonstrated the outer product representation of matrix multiplication.
                \item Showed how a matrix product can be expressed as a sum of outer products.
            \end{itemize}
        \end{itemize}
    \end{statement}
    \begin{statement}{Variations}
        \begin{itemize}
            \item We could be asked to show examples of different properties.
            \begin{itemize}
                \item In this case we would perform the operations for these properties similar to what we did in this problem.
            \end{itemize}
        \end{itemize}
    \end{statement}
\end{summary}

% Problem 2
\begin{problem}{Problem 2}
    \begin{statement}{Problem Statement}
        Solve and Explain the solution to 10.39  here in your own words. (Since you are given a solution, you will be graded on your ability to explain). \vspace*{1em}

        \noindent \textbf{Original Question:} \vspace*{1em}

        \textit{Gram matrix and QR factorization}. Suppose the matrix $A$ has linearly independent columns and QR factorization $A = QR$. What is the relationship between the Gram matrix of $A$ and 
        the Gram matrix of $R$? What can you say about the angles between the columns of $A$ and the angles between the columns of $R$?
    \end{statement}

    \begin{Highlight}[Solution]
        \noindent For this problem I will be using \textbf{method 4}. \vspace*{1em}

        \noindent \textbf{Explanation:} \vspace*{1em}

        In its general form, the Gram matrix is defined as 

        \setcounter{equation}{0}
        \begin{equation}
            G = A^{T}A = 
            \begin{bmatrix}
                a_{1}^{T}a_{1} & a_{1}^{T}a_{2} & \dots & a_{1}^{T}a_{n} \\
                a_{2}^{T}a_{1} & a_{2}^{T}a_{2} & \dots & a_{2}^{T}a_{n} \\
                \vdots & \vdots & \ddots & \vdots \\
                a_{m}^{T}a_{1} & a_{m}^{T}a_{2} & \dots & a_{m}^{T}a_{n} \\
            \end{bmatrix}.
        \end{equation}
        In QR factorization, we can express a condition for the column vectors of $A$ such that they are orthonormal. Simply put this relationship is 

        \begin{equation}
            A^{T}A = I.
        \end{equation}
        Using the definition of a Gram matrix in (1) with the identity found in (2), we use $A = QR$ in (1). Doing so we have 

        \begin{equation}
            G = A^{T}A = (QR)^{T}(QR) = Q^{T}R^{T}QR = R^{T}Q^{T}QR = R^{T}IR = R^{T}R.
        \end{equation}
        From (3) we can see that the Gram matrix $G$ of $A$ is the same as the Gram matrix of $R$. In (3) we used the orthonormal condition of $Q$ in (2) to simplify the expression to demonstrate the
        result.

        Because the Gram matrix of $A$ is the same as the Gram matrix of $R$, this constitutes that the angles between the column vectors in $A$ have to be the same as the angles between the column
        vectors of $R$. Namely,

        \begin{equation}
            \angle (a_{i},a_{j}) = \angle (r_{i},r_{j}).
        \end{equation}
        We would not be able to arrive to the conclusion of the Gram matrix of $A$ found in (3) otherwise.
    \end{Highlight}
\end{problem}

% Problem 2 Summary
\begin{summary}{Problem 2 Summary}
    \begin{statement}{Procedure}
        \begin{itemize}
            \item Use the definition of the Gram matrix.
            \item Insert (2) into (1) and go through the algebra to get the final result.
        \end{itemize}
    \end{statement}
    \begin{statement}{Key Concepts}
        \begin{itemize}
            \item \textbf{Problem Statement:}
            \begin{itemize}
                \item The problem focuses on understanding the relationship between the Gram matrix of a matrix $A$ with linearly independent columns and the Gram matrix of $R$ in the QR factorization $A = QR$.
                \item It also explores the implications of this relationship on the angles between the columns of $A$ and $R$.
            \end{itemize}
            \item \textbf{Concepts:}
            \begin{itemize}
                \item \textbf{Gram Matrix}: Defined as $G = A^T A$, where $A$ is a matrix and $A^T$ is its transpose.
                \item \textbf{QR Factorization}: Decomposition of matrix $A$ into a product $QR$, where $Q$ has orthonormal columns and $R$ is upper triangular.
            \end{itemize}
            \item \textbf{Relationship Between Gram Matrices:}
            \begin{itemize}
                \item Using $A = QR$, the Gram matrix of $A$ is derived as:
                \[ G = (QR)^T (QR) = R^T Q^T QR = R^T R \]
                \item This demonstrates that the Gram matrix of $A$ is the same as the Gram matrix of $R$.
                \item The relationship is established by utilizing the orthonormal property of $Q$, where $Q^T Q = I$.
            \end{itemize}
            \item \textbf{Angles Between Column Vectors:}
            \begin{itemize}
                \item The equality of the Gram matrices implies that the angles between the column vectors of $A$ are the same as those between the column vectors of $R$.
                \item This is expressed as $\langle a_i, a_j \rangle = \langle r_i, r_j \rangle$ for column vectors $a_i, a_j$ of $A$ and $r_i, r_j$ of $R$.
            \end{itemize}
            \item \textbf{Linear Algebra Application:}
            \begin{itemize}
                \item This problem illustrates the application of linear algebra in understanding matrix factorizations and their properties.
                \item It highlights how QR factorization affects the properties of matrices and their geometric interpretations.
            \end{itemize}
        \end{itemize}
    \end{statement}
    \begin{statement}{Variations}
        \begin{itemize}
            \item We could be given a different matrix to demonstrate this operation.
            \begin{itemize}
                \item We would then go through the same procedure as depicted in this problem for the new matrix.
            \end{itemize}
        \end{itemize}
    \end{statement}
\end{summary}

% Problem 3
\begin{problem}{Problem 3}
    \begin{statement}{Problem Statement}
        Solve and Explain the solution to 10.40 here in your own words. (Since you are given a solution, you will be graded on your ability to explain). \vspace*{1em}

        \noindent \textbf{Original Question:} \vspace*{1em}

        \textit{QR factorization of first i columns of A}. Suppose the $n \times k$ matrix $A$ has QR factorization $A = QR$. We define the $n \times i$ matrices

        \setcounter{equation}{0}
        \begin{equation*}
            A_{i} = 
            \begin{bmatrix}
                a_{1} & \dots & a_{i}
            \end{bmatrix}
            \hspace*{10pt} , \hspace*{10pt}
            Q_{i} = 
            \begin{bmatrix}
                q_{1} & \dots & q_{i}
            \end{bmatrix},
        \end{equation*}
        for $i = 1,\dots,k$. Define the $i \times i$ matrix $R_{i}$ as the submatrix of $R$ containing its first $i$ rows and columns, for $i = 1,\dots,k$. Using index range notation, we have

        \begin{equation*}
            A_{i} = A_{1:n,1:i}
            \hspace*{10pt} , \hspace*{10pt}
            Q_{i} = A_{1:n,1:i}
            \hspace*{10pt} , \hspace*{10pt}
            R_{i} = R_{1:i,1:i}.
        \end{equation*}
        Show that $A_{i} = Q_{i}R_{i}$ is the QR factorization of $A_{i}$. This means that when you compute the QR factorization of $A$, you are also computing the QR factorization of all submatrices 
        $A_{1},\dots,A_{k}$.
    \end{statement}

    \begin{Highlight}[Solution]
        \noindent For this problem I will be using \textbf{method 4}. \vspace*{1em}

        \noindent \textbf{Explanation:} \vspace*{1em}

        We know in general that QR factorization is calculated with 

        \begin{equation}
            A = QR.
        \end{equation}
        In order to calculate the QR factorization for all submatrices we need to use a block matrix representation. The block representation of $A$, denoted as $\mathbf{A}$ is then 

        \begin{equation}
            \mathbf{A} = 
            \begin{bmatrix}
                A_{i} & A_{:,i+1:k} \\
            \end{bmatrix}.
        \end{equation}
        In (2), the first submatrix $A_{i}$ is accounting for all the rows and columns of $A$. The second submatrix $A_{:,i+1:k}$ is taking all of the rows and the second column up to $k$ for the columns.
        Consequently we also define to the block matrix of $Q$ as $\mathbf{Q}$ with 

        \begin{equation}
            \mathbf{Q} = 
            \begin{bmatrix}
                Q_{i} & Q_{:,i+1:k} \\
            \end{bmatrix}.
        \end{equation}
        Where the same row and column selection applies to (3) as it did to (2). We now have to work on determining the $i \times i$ block matrix $\mathbf{R}$. 

        In order to get the desired result of $A_{i} = Q_{i}R_{i}$ in our QR factorization, this means at least one entry in $\mathbf{R}$ must be zero. This entry is specifically $\mathbf{R}_{21}$.
        Consequently, the entry $\mathbf{R}_{11} = R_{i}$. Following the same pattern as the definition of $\mathbf{A}$ and $\mathbf{Q}$ we can then say $\mathbf{R}$ is

        \begin{equation}
            \mathbf{R} = 
            \begin{bmatrix}
                R_{i} & R_{1:i,i+1:k} \\
                0 & R_{i+1:k,i+1:k} \\
            \end{bmatrix}
        \end{equation}
        Stitching (2), (3), and (4) together we then have 

        \begin{equation}
            \begin{bmatrix}
                A_{i} & A_{:,i+1:k} \\
            \end{bmatrix}
            = 
            \begin{bmatrix}
                Q_{i} & Q_{:,i+1:k} \\
            \end{bmatrix}
            \begin{bmatrix}
                R_{i} & R_{1:i,i+1:k} \\
                0 & R_{i+1:k,i+1:k} \\
            \end{bmatrix}
            = 
            \begin{bmatrix}
                Q_{i}R_{i} & Q_{i}R_{1:i,i+1:k} + Q_{:,i+1:k}R_{i+1:k,i+1:k} \\
            \end{bmatrix}.
        \end{equation}
        We can see immediately from (5) that the first block matrix is the QR factorization of $A_{i}$.
    \end{Highlight}
\end{problem}

% Problem 3 Summary
\begin{summary}{Problem 3 Summary}
    \begin{statement}{Procedure}
        \begin{itemize}
            \item Illustrate what the matrices $A$ and $Q$ are represented as in block matrix form.
            \item Perform the matrix multiplication to find the final representation of $A$.
        \end{itemize}
    \end{statement}
    \begin{statement}{Key Concepts}
        \begin{itemize}
            \item \textbf{Problem Statement:}
            \begin{itemize}
                \item The problem involves understanding the QR factorization of the first $i$ columns of a matrix $A$.
                \item The matrix $A$ is an $n \times k$ matrix with QR factorization $A = QR$, where $Q$ has orthonormal columns and $R$ is upper triangular.
                \item Submatrices $A_i$, $Q_i$, and $R_i$ are defined, and the goal is to show that $A_i = Q_i R_i$ is the QR factorization of $A_i$.
            \end{itemize}
            \item \textbf{Matrix Notations:}
            \begin{itemize}
                \item $A_i = A_{1:n,1:i}$ represents the first $i$ columns of $A$.
                \item $Q_i = Q_{1:n,1:i}$ represents the first $i$ columns of $Q$.
                \item $R_i = R_{1:i,1:i}$ is the $i \times i$ submatrix of $R$ containing its first $i$ rows and columns.
            \end{itemize}
            \item \textbf{Demonstration of QR Factorization:}
            \begin{itemize}
                \item The demonstration involves using block matrix representation to show that $A_i$ can be expressed as the product of $Q_i$ and $R_i$.
                \item The block representation of $A$, $Q$, and $R$ are used to partition these matrices and demonstrate the QR factorization for the submatrix $A_i$.
            \end{itemize}
            \item \textbf{Solution and Explanation:}
            \begin{itemize}
                \item The solution is provided through detailed matrix block manipulation and algebraic expressions.
                \item The solution concludes that $A_i = Q_i R_i$ is indeed the QR factorization of $A_i$, showing that the QR factorization of $A$ simultaneously provides the QR factorization of all 
                its leading submatrices.
            \end{itemize}
            \item \textbf{Linear Algebra Concepts:}
            \begin{itemize}
                \item This problem provides insights into QR factorization and how it applies to submatrices.
                \item It emphasizes the use of block matrices in simplifying complex algebraic expressions and understanding matrix factorizations.
            \end{itemize}
        \end{itemize}
    \end{statement}
    \begin{statement}{Variations}
        \begin{itemize}
            \item We could be asked to demonstrate this multiplication with a different form.
            \begin{itemize}
                \item We would then go through the same procedure to depict the desired form that is requested from us.
            \end{itemize}
        \end{itemize}
    \end{statement}
\end{summary}

% Problem 4
\begin{problem}{Problem 4}
    \begin{statement}{Problem Statement}
        Solve and Explain the solution to 10.43 here in your own words. (Since you are given a solution, you will be graded on your ability to explain). (If you watched the author videos, this where 
        to take a moment to sit in awe of the speed of computers) \vspace*{1em}

        \noindent \textbf{Original Question:} \vspace*{1em}

        A particular computer takes about 0.2 seconds to multiply two $1500 \times 1500$ matrices. About how long would you guess the computer takes to multiply two $3000 \times 3000$ matrices? Give 
        your prediction (\textit{i.e.}, the time in seconds), and your (very brief) reasoning.
    \end{statement}

    \begin{Highlight}[Solution]
        \noindent For this problem I will be using \textbf{method 4}. \vspace*{1em}

        \noindent \textbf{Explanation:} \vspace*{1em}

        From page 102 of VMLS, we are told that the order of flops for matrix multiplication is on the order of $n^{3}$. In the problem statement we were told that the time to multiply two $1500 \times 1500$
        matrices took around 0.2 seconds. We are trying to determine the time taken to multiply two matrices together that are twice in size of the original matrix. 

        If we are trying to estimate the time taken to multiply two matrices that are twice the size of the original matrices (since $3000 / 1500 = 2$) then this means $n = 2$ in our expression. Taking
        this into account we can then say 
        
        \setcounter{equation}{0}
        \begin{equation}
            n^{3} = 2^{3} = 8.
        \end{equation}
        The result from (1) means that it would take approximately 8 times longer to multiply the $3000 \times 3000$ matrices. Since the original time is 0.2 seconds, this means the estimated time 
        taken to multiply the $3000 \times 3000$ matrices is roughly

        \begin{equation}
            8\cdot 0.2 \text{ seconds} = 1.6 \text{ seconds}.
        \end{equation}
        So a rough estimate would be 1.6 seconds.
    \end{Highlight}
\end{problem}

% Problem 4 Summary
\begin{summary}{Problem 4 Summary}
    \begin{statement}{Procedure}
        \begin{itemize}
            \item Use the result from the smaller matrix to calculate the time taken for the larger matrix with $n = 2$ in $n^{3}$.
        \end{itemize}
    \end{statement}
    \begin{statement}{Key Concepts}
        \begin{itemize}
            \item \textbf{Problem Statement:}
            \begin{itemize}
                \item The problem assesses the computational complexity of matrix multiplication in relation to matrix size.
                \item Given data: a computer takes approximately 0.2 seconds to multiply two $1500 \times 1500$ matrices.
                \item The task is to estimate the time it would take for the same computer to multiply two $3000 \times 3000$ matrices.
            \end{itemize}
            \item \textbf{Solution Approach:}
            \begin{itemize}
                \item The solution employs the cubic complexity of matrix multiplication, denoted as $n^3$, where $n$ represents the matrix size.
                \item Since the size of the matrices increases from $1500$ to $3000$, which is double the size, the complexity factor $n$ is set to 2.
            \end{itemize}
            \item \textbf{Computational Complexity Estimation:}
            \begin{itemize}
                \item It is deduced that the time taken for matrix multiplication increases by a factor of $n^3$, hence $2^3 = 8$ in this case.
                \item This implies that multiplying $3000 \times 3000$ matrices will take approximately 8 times longer than $1500 \times 1500$ matrices.
            \end{itemize}
            \item \textbf{Time Calculation:}
            \begin{itemize}
                \item The estimated time is calculated as $8 \times 0.2$ seconds, resulting in 1.6 seconds.
                \item The final answer for the estimated time is 1.6 seconds.
            \end{itemize}
            \item \textbf{Linear Algebra and Computer Science Application:}
            \begin{itemize}
                \item This problem illustrates the application of linear algebra concepts in the field of computer science, specifically in understanding the computational efficiency of matrix operations.
                \item It highlights the significance of matrix dimensions in determining the computational cost of matrix multiplication.
            \end{itemize}
        \end{itemize}
    \end{statement}
    \begin{statement}{Variations}
        \begin{itemize}
            \item We could be given a different dimension for the new matrix.
            \begin{itemize}
                \item We would then figure out the $n$ value for this new matrix and feed it into the formula to find the time.
            \end{itemize}
        \end{itemize}
    \end{statement}
\end{summary}