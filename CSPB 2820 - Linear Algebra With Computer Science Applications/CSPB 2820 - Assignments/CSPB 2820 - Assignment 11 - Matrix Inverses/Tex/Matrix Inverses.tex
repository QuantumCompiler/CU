\clearpage
\chapter{Study Guide 11}

\section{Matrix Inverses}
\horizontalline{0}{0}

\begin{center}
    \large{\textbf{Study Guide Instructions}}
\end{center}

\horizontalline{-1}{0}

\begin{itemize}
    \item Submit your work in Gradescope as a PDF - you will identify where your “questions are.”
    \item Identify the question number as you submit.  Since we grade "blind" if the questions are NOT identified, the work WILL NOT BE GRADED and a 0 will be recorded. Always leave enough time to 
    identify the questions when submitting.
    \item One section per page (if a page or less) - We prefer to grade the main solution in a single page, extra work can be included on the following page.
    \item Long instructions may be removed to fit on a single page.
    \item \textbf{Do not start a new question in the middle of a page.}
    \item Solutions to book questions are provided for reference.
    \item You may NOT submit given solutions - this includes minor modifications - as your own.
    \item Solutions that do not show individual engagement with the solutions will be marked as no credit and can be considered a violation of honor code.
    \item If you use the given solutions you must reference or explain how you used them, in particular...
\end{itemize}

\horizontalline{-1}{0}

\begin{center}
    \large{\textbf{Method Selection}}
\end{center}

\horizontalline{-1}{0}

\textbf{For full credit,  EACH book exercise in the Study Guides must use one or more of the following methods and FOR EACH QUESTION.  Identify the number the method by number to ensure full credit.}

\begin{itemize}
    \item \textbf{Method 1} - Provide original examples which demonstrate the ideas of the exercise in addition to your solution.
    \item \textbf{Method 2} - Include and discuss the specific topics needed from the chapter and how they relate to the question.
    \item \textbf{Method 3} - Include original Python code, of reasonable length (as screenshot or text)  to show how the topic or concept was explored.
    \item \textbf{Method 4} - Expand the given solution in a significant way, with additional steps and comments. All steps are justified. This is a good method for a proof for which you are only given a basic outline.
    \item \textbf{Method 5} - Attempt the exercise without looking at the solution and then the solution is used to check work. Words are used to describe the results.
    \item \textbf{Method 6} - Provide an analysis of the strategies used to understand the exercise, describing in detail what was challenging, who helped you or what resources were used. The process of understanding is
    described.
\end{itemize}

% Problem 1
\begin{problem}{Problem 1}
    \begin{statement}{Problem Statement}
        Annotation/reprove/add your own explanation communicating the ideas behind the section Triangular Matrices on VMLS page 206. Why is this result interesting and useful? Why is this proof so much fun?        
    \end{statement}

    \begin{highlight}[Solution]
        A triangular matrix is a matrix where the elements above or below the main diagonal of the matrix are zero. For a matrix to be upper triangular the elements above the main diagonal are zero. Namely,

        \begin{equation*}
            U =
            \begin{bmatrix}
                u_{11} & 0 & 0 & 0 \\
                u_{21} & u_{22} & 0 & 0 \\
                u_{31} & u_{32} & u_{33} & 0 \\
                u_{41} & u_{42} & u_{43} & u_{44} \\
            \end{bmatrix}.
        \end{equation*}
        For a matrix to be lower triangular the elements below the main diagonal are zero. Namely,

        \begin{equation*}
            L = 
            \begin{bmatrix}
                l_{11} & l_{12} & l_{13} & l_{14} \\
                0 & l_{22} & l_{23} & l_{24} \\
                0 & 0 & l_{33} & l_{34} \\
                0 & 0 & 0 & l_{44} \\
            \end{bmatrix}.
        \end{equation*}
        The above are only $4 \times 4$ matrices but the concepts that are encapsulated in them can be extrapolated to larger matrices.

        The proof from page 206 in VMLS shows that a lower triangular matrix has linearly independent columns. For the columns of this matrix to be linearly independent it must follow that

        \setcounter{equation}{0}
        \begin{equation}
            Lx = 0.
        \end{equation}
        Recall, for a vector to be considered linearly independent it must follow that for a given vector

        \begin{equation}
            \beta_{1}x_{1} + \beta_{2}x_{2} + \dots + \beta_{i}x_{i} = 0
        \end{equation}
        where the $\beta$'s must all be zero for (2) to be true.

        If we revisit the proof from page 206 we have
        \begin{align}
            L_{11}x_{1} & = 0 & \text{(First row, first column element)} \\
            L_{21}x_{1} + L_{22}x_{2} & = 0 & \text{(Second row, first two column elements)} \\
            & \vdots & \\
            L_{n1}x_{1} + L_{n2}x_{2} + \dots + L_{n,n-1}x_{n-1} + L_{nn}x_{n} & = 0 & \text{(Last row, all column elements.)}
        \end{align}
        We can see from equations (3) - (6) that the inner products of the column vectors with $x$ all are equal to zero. The result from (3) shows us

        \begin{equation}
            L_{11}x_{1} = 0 \hspace*{10pt} \rightarrow \hspace*{10pt} x_{1} = \frac{0}{L_{11}} \hspace*{10pt} \therefore \hspace*{10pt} x_{1} = 0.
        \end{equation}
        With $x_{1} = 0$, this means for the result in (4) we have 

        \begin{equation}
            L_{21}(0) + L_{22}x_{2} = 0 \hspace*{10pt} \rightarrow \hspace*{10pt} L_{22}x_{2} = 0 \hspace*{10pt} \rightarrow \hspace*{10pt} x_{2} = \frac{0}{L_{22}} \hspace*{10pt} \therefore \hspace*{10pt} x_{2} = 0.
        \end{equation}
        If we follow this pattern, we can see that all elements in $x$ are going to be zero. Revisiting the linear independence equation in (2) we have 

        \begin{equation}
            x_{1}L_{11} + x_{2}L_{21} + \dots + x_{n}L_{nn} = 0.
        \end{equation}
        From the previous results we can see that the only way (9) is true is if $x_{i}$ are zero. This is in turn means that the column vectors of $L$ are linearly independent.

        This proof is interesting because of how the column vectors of $L$ are determined to be linearly independent. Because the diagonal elements in $L$ are all nonzero, we can extrapolate rather easily
        what the elements of $x$ must be. Since we are able to easily determine what the elements of $x$ are, we can then conclude that the column vectors of $L$ must be linearly independent.
    \end{highlight}
\end{problem}

% Problem 1 Summary
\begin{summary}{Problem 1 Summary}
    \begin{statement}{Procedure}
        \begin{itemize}
            \item Demonstrate what it means for a matrix to be upper / lower triangular.
            \item Show what each element of $L$ is going to be based on the conditions.
            \item Demonstrate that the columns of $L$ are linearly independent.
        \end{itemize}
    \end{statement}
    \begin{statement}{Key Concepts}
        \begin{itemize}
            \item \textbf{Problem Statement:}
            \begin{itemize}
                \item The problem discusses triangular matrices and their properties, particularly focusing on linear independence of column vectors.
                \item It asks to annotate, reprove, or add an explanation to the ideas behind triangular matrices as presented in the book VMLS (Vector and Matrix Library for Students) on page 206.
            \end{itemize}
            \item \textbf{Triangular Matrices:}
            \begin{itemize}
                \item A triangular matrix is one where elements above (upper triangular) or below (lower triangular) the main diagonal are zero.
                \item The problem specifically considers lower triangular matrices and their properties.
            \end{itemize}
            \item \textbf{Linear Independence of Columns:}
            \begin{itemize}
                \item The proof on page 206 of VMLS shows that a lower triangular matrix has linearly independent columns.
                \item This is proven by considering a linear system $Lx = 0$, where $L$ is a lower triangular matrix.
                \item The linear independence is deduced by sequentially determining the values of the elements in vector $x$, which must all be zero for the equation to hold.
            \end{itemize}
            \item \textbf{Method of Proof:}
            \begin{itemize}
                \item The proof method involves examining the inner products of the column vectors with vector $x$ and showing that they are all equal to zero.
                \item This method reveals that each element in $x$ must be zero, proving the linear independence of the columns of $L$.
            \end{itemize}
            \item \textbf{Significance of the Proof:}
            \begin{itemize}
                \item The proof is interesting because it provides a straightforward way to conclude the linear independence of the columns of a lower triangular matrix.
                \item It highlights the importance of nonzero diagonal elements in determining the linear independence of the columns.
            \end{itemize}
            \item \textbf{Linear Algebra Concepts:}
            \begin{itemize}
                \item This problem demonstrates key concepts in linear algebra related to matrix types, specifically triangular matrices, and their inherent properties.
                \item It emphasizes the utility of these matrices in determining linear independence, a fundamental aspect of linear algebra.
            \end{itemize}
        \end{itemize}
    \end{statement}
    \begin{statement}{Variations}
        \begin{itemize}
            \item We could be given a different initial matrix.
            \begin{itemize}
                \item In this case we would use the same procedure of examining the inner products of the columns and the vector to determine the linear independence of the vectors.
            \end{itemize}
        \end{itemize}
    \end{statement}
\end{summary}

% Problem 2
\begin{problem}{Problem 2}
    \begin{statement}{Problem Statement}
        Solve and Explain the solution to 11.2 here in your own words. (Since you are given a solution, you will be graded on your ability to explain)

        \subsubsection*{Original Question:}

        \textit{Left and right inverses of a vector}. Suppose that $x$ is a nonzero n-vector with $n > 1$.

        \begin{enumerate}[label = (\alph*)]
            \item Does $x$ have a left inverse?
            \item Does $x$ have a right inverse?
        \end{enumerate}
        In each case, if the answer is yes, give a left or right inverse; if the answer is no, give a specific nonzero vector and show that it is not left or right-invertible.
    \end{statement}

    \begin{highlight}[Solution - Part (a)]
        \noindent For this problem, I am going to use \textbf{Method 4}. \vspace*{1em}

        For a matrix to be defined as left-invertible it must follow 

        \setcounter{equation}{0}
        \begin{equation}
            XA = I
        \end{equation}
        where $X$ is defined as the left inverse of $A$. Of course, in our example we are working with an $n$ vector instead of a matrix. 

        For $x$ to be left invertible, we need to find a vector representation such that

        \begin{equation}
            x^{-1}x = 1.
        \end{equation}
        The simplest way to achieve this is to make sure that when we multiply $x$ with $x^{-1}$ we design it such that the multiplication will add up to 1. For example, if we perform the multiplication 
        of $x^{T}$ with $x$ we have

        \begin{equation}
            x^{T}x = x_{1}^{T}x_{1} + x_{2}^{T}x_{2} + \dots + x_{n}^{T}x_{n} = \|x\|^{2}.
        \end{equation}
        With the result in (3), we need to construct an inverse such that the norm square of $x$ is in the denominator. We can achieve this by defining our inverse as 

        \begin{equation}
            x^{-1} = (x^{T}x)^{-1}x^{T} = \frac{1}{\|x\|^{2}} x^{T}.
        \end{equation}
        If we use the result from (4) we can then show

        \begin{equation}
            x^{-1}x = \frac{1}{\|x\|^{2}} x^{T} \cdot x = \frac{1}{\|x\|^{2}} \cdot (\|x\|^{2}) = 1
        \end{equation}
        thus showing that $x$ can be shown to be left-invertible.

        Take for instance a simple example for $x$

        \begin{equation*}
            x = 
            \begin{bmatrix}
                1 \\
                2 \\
            \end{bmatrix}.
        \end{equation*}
        The norm squared of this matrix is then $\|x\|^{2} = 1^{2} + 2^{2} = 1 + 4 = 5$. This then means $x^{-1}$ is 

        \begin{equation*}
            x^{-1} = \frac{1}{5}
            \begin{bmatrix}
                1 & 2 \\
            \end{bmatrix}.
        \end{equation*}
        If we perform the operation in (5) we then have

        \begin{equation*}
            x^{-1}x = \frac{1}{\|x\|^{2}}x^{T}x = \frac{1}{5} 
            \begin{bmatrix}
                1 & 2 \\
            \end{bmatrix}
            \begin{bmatrix}
                1 \\
                2 \\
            \end{bmatrix}
            = \frac{1}{5}(1(1) + 2(2)) = \frac{1}{5}(5) = 1.
        \end{equation*}
    \end{highlight}

    \begin{highlight}[Solution - Part (b)]
        \noindent For this problem, I am going to use \textbf{Method 4}. \vspace*{1em}

        For a matrix to be defined as right-invertible it must follow

        \begin{equation}
            AX = I
        \end{equation}
        where $X$ is defined as the right inverse of $A$.

        For $x$ in our example to be right-invertible, its rows must be linearly independent. Namely,

        \begin{equation}
            \beta_{1}x_{1} + \beta_{2}x_{2} + \dots + \beta_{i}x_{i} = 0.
        \end{equation}
        Since $x$ is a column vector (or tall) each row only has one element in it. In this case, the rows of our vector $x$ are not linearly independent. Because of this, we can say that $x$ is not 
        right-invertible.
    \end{highlight}
\end{problem}

% Problem 2 Summary
\begin{summary}{Problem 2 Summary}
    \begin{statement}{Procedure}
        \begin{itemize}
            \item For part (a), determine an inverse for the vector that will result in 1 for the inner product of the original vector.
            \item For part (b), because of $A$ being a tall matrix, it cannot be right invertible.
        \end{itemize}
    \end{statement}
    \begin{statement}{Key Concepts}
        \begin{itemize}
            \item \textbf{Problem Statement:}
            \begin{itemize}
                \item The problem explores the concepts of left and right inverses in the context of an $n$-vector $x$, where $n > 1$.
                \item It asks whether $x$ has a left inverse and a right inverse, providing specific examples and explanations.
            \end{itemize}
            \item \textbf{Left Inverse:}
            \begin{itemize}
                \item A left inverse of $x$ is defined, satisfying $XA = I$ where $X$ is the left inverse of $A$. In this case, $A$ is the vector $x$.
                \item The solution shows that $x$ can be left-invertible by defining an inverse such that $x^{-1} = (x^T x)^{-1}x^T$, ensuring the product results in 1.
                \item An example is provided with a specific vector $x$, calculating its norm squared and constructing the left inverse.
            \end{itemize}
            \item \textbf{Right Inverse:}
            \begin{itemize}
                \item The right inverse is investigated using the condition $AX = I$, where $X$ is the right inverse of $A$, and $A$ is the vector $x$.
                \item It is concluded that for $x$ to be right-invertible, its rows must be linearly independent. However, since $x$ is a tall vector, its rows are not linearly independent, 
                and thus $x$ is not right-invertible.
            \end{itemize}
            \item \textbf{Linear Algebra Concepts:}
            \begin{itemize}
                \item This problem demonstrates the application of the concepts of left and right inverses to vectors, contrasting their definitions and conditions for existence.
                \item It highlights the differences in invertibility criteria for left and right inverses in the context of vectors.
            \end{itemize}
        \end{itemize}
    \end{statement}
    \begin{statement}{Variations}
        \begin{itemize}
            \item We could be asked to do this for a matrix instead of a vector.
            \begin{itemize}
                \item We would then use the same procedure for the vector as for the matrix to determine if it is left and right invertible, using the defined conditions required for determining
                invertibility.
            \end{itemize}
        \end{itemize}
    \end{statement}
\end{summary}

% Problem 3
\begin{problem}{Problem 3}
    \begin{statement}{Problem Statement}
        Solve and Explain the solution to 11.3 here in your own words. (Since you are given a solution, you will be graded on your ability to explain)

        \subsubsection*{Original Question:}

        \textit{Matrix cancellation.} Suppose the scalars $a, x,$ and $y$ satisfy $ax = ay$. When $a \neq 0$ we can conclude that $x = y$; that is, we can cancel the $a$ on the left of the equation. 
        In this exercise we explore the matrix analog of cancellation, specifically, what properties of $A$ are needed to conclude $X = Y$ from $AX = AY$, for matrices $A, X,$ and $Y$?

        \begin{enumerate}[label = (\alph*)]
            \item Give an example showing that $A \neq 0$ is not enough to conclude that $X = Y$.
            \item Show that if $A$ is left-invertible, we can conclude from $AX = AY$ that $X = Y$.
            \item Show that if $A$ is not left-invertible, there are matrices $X$ and $Y$ with $X = Y$, and $AX = AY$.
        \end{enumerate}
        \textit{Remark}. Parts (b) and (c) show that you can cancel a matrix on the left when, and only when, the matrix is left-invertible.
    \end{statement}
\end{problem}

% Problem 4
\begin{problem}{Problem 4}
    \begin{statement}{Problem Statement}
        Solve and Explain the solution to 11.4 here in your own words. (Since you are given a solution, you will be graded on your ability to explain)

        \subsubsection*{Original Question:}

        \textit{Transpose of orthogonal matrix}. Let $U$ be an orthogonal $n \times n$ matrix. Show that its transpose $U^{T}$ is also orthogonal.
    \end{statement}
    \begin{highlight}[Solution]
        \noindent For this problem, I am going to use \textbf{Method 4}. \vspace*{1em}

        For a matrix to be considered orthogonal, the rows and columns of that matrix must be orthonormal. In a mathematical sense a matrix is defined to be orthogonal if

        \setcounter{equation}{0}
        \begin{equation}
            A^{T}A = AA^{T} = I.
        \end{equation}
        Because we are told that our $n \times n$ matrix $U$ is orthogonal, this implies

        \begin{equation}
            U^{T}U = UU^{T} = I.
        \end{equation}
        We are now interested in showing that the transpose of $U$ is also orthogonal. If we use the transposed matrix of $U$ in (2) we have

        \begin{equation}
            (U^{T})^{T}U^{T} = UU^{T} = I
        \end{equation}
        where we have used the fact that a transpose of a transpose of a matrix is just the original matrix. We can demonstrate this quality with

        \begin{equation}
            U = 
            \begin{bmatrix}
                u_{11} & u_{12} \\
                u_{21} & u_{22} \\
            \end{bmatrix}
            \hspace*{10pt} , \hspace*{10pt}
            U^{T} = 
            \begin{bmatrix}
                u_{11} & u_{21} \\
                u_{12} & u_{22} \\
            \end{bmatrix}
            \hspace*{10pt} , \hspace*{10pt}
            (U^{T})^{T} = 
            \begin{bmatrix}
                u_{11} & u_{12} \\
                u_{21} & u_{22} \\
            \end{bmatrix}
        \end{equation}
        where we see that the transpose of the transpose of $U$ is the original matrix $U$. From this we can deduce that the property in (2) is valid for the transpose of $U$ and thus the transpose of 
        $U$ is also orthogonal.
    \end{highlight}
\end{problem}

% Problem 4 Summary
\begin{summary}{Problem 4 Summary}
    \begin{statement}{Procedure}
        \begin{itemize}
            \item Take the transpose of the matrix.
            \item Use the orthogonality condition of transposed matrix.
            \item Demonstrate that the transpose of the transposed matrix is equivalent to the original matrix.
        \end{itemize}
    \end{statement}
    \begin{statement}{Key Concepts}
        \begin{itemize}
            \item \textbf{Problem Statement:}
            \begin{itemize}
                \item The task is to show that the transpose of an orthogonal $n \times n$ matrix $U$ is also orthogonal.
            \end{itemize}
            \item \textbf{Definition of Orthogonal Matrix:}
            \begin{itemize}
                \item A matrix $U$ is defined as orthogonal if it satisfies:
                \begin{equation*}
                    U^T U = UU^T = I
                \end{equation*}
                where $U^T$ is the transpose of $U$ and $I$ is the identity matrix.
            \end{itemize}
            \item \textbf{Approach:}
            \begin{itemize}
                \item The solution involves applying the definition of an orthogonal matrix and properties of matrix transpose.
            \end{itemize}
            \item \textbf{Proof of Orthogonality of $U^T$:}
            \begin{itemize}
                \item Given that $U$ is orthogonal, it follows that:
                \begin{equation*}
                    U^T U = UU^T = I
                \end{equation*}
                \item To show that $U^T$ is orthogonal, the proof utilizes the property of transpose of a transpose, which returns the original matrix:
                \begin{equation*}
                    (U^T)^T U^T = UU^T = I
                \end{equation*}
                \item By demonstrating that $(U^T)^T U^T = I$, it is established that $U^T$ is orthogonal.
                \item The proof is further illustrated with a simple matrix example.
            \end{itemize}
            \item \textbf{Linear Algebra Concepts:}
            \begin{itemize}
                \item This problem exemplifies important linear algebra concepts, including matrix transposition, orthogonality, and the properties of orthogonal matrices.
            \end{itemize}
        \end{itemize}
    \end{statement}
    \begin{statement}{Variations}
        \begin{itemize}
            \item We could be asked to demonstrate a different property.
            \begin{itemize}
                \item We would then demonstrate this new property with new examples.
            \end{itemize}
        \end{itemize}
    \end{statement}
\end{summary}

% Problem 5
\begin{problem}{Problem 5}
    \begin{statement}{Problem 5}
        Solve and Explain the solution to 11.5 here in your own words. (Since you are given a solution, you will be graded on your ability to explain).

        \subsubsection*{Original Question:}

        \textit{Inverse of a block matrix}. Consider the $(n + 1) \times (n + 1)$ matrix

        \begin{equation*}
            A = 
            \begin{bmatrix}
                I & a \\
                a^{T} & 0 \\
            \end{bmatrix},
        \end{equation*}
        where $a$ is an $n$-vector.

        \begin{enumerate}[label = (\alph*)]
            \item When is $A$ invertible? Give your answer in terms of $a$. Justify your answer.
            \item Assuming the condition you found in part (a) holds, give an expression for the inverse matrix $A^{-1}$.
        \end{enumerate}
    \end{statement}

    \begin{highlight}[Solution - Part (a)]
        \noindent For this problem, I am going to use \textbf{Method 4}. \vspace*{1em}

        We know that an inverse exists for a square matrix if and only if the columns of those matrices are linearly independent. Namely,

        \setcounter{equation}{0}
        \begin{equation}
            Ax = 0.
        \end{equation}
        Take for instance the arbitrary vector $x$

        \begin{equation}
            x = 
            \begin{bmatrix}
                x_{1} \\
                x_{2} \\
            \end{bmatrix}.
        \end{equation}
        Performing the matrix / vector multiplication of $A$ with $x$ we have

        \begin{equation}
            Ax = 
            \begin{bmatrix}
                I & a \\
                a^{T} & 0 \\
            \end{bmatrix}
            \begin{bmatrix}
                x_{1} \\
                x_{2} \\
            \end{bmatrix}
            = 
            \begin{bmatrix}
                Ix_{1} + ax_{2} \\
                a^{T}x_{1} \\
            \end{bmatrix}
            = 0.
        \end{equation}
        This implies then that
        \begin{align}
            Ix_{1} + ax_{2} & = 0 \\
            x_{1} & = -ax_{2} \\
            -\frac{x_{1}}{x_{2}} & = a.
        \end{align}
        This then means $A$ is invertible if 
        
        \begin{equation}
            a = -\frac{x_{1}}{x_{2}}.
        \end{equation}
    \end{highlight}

    \begin{highlight}[Solution - Part (b)]
        \noindent For this problem, I am going to use \textbf{Method 4}. \vspace*{1em}

        We know that for a $2 \times 2$ matrix the inverse of the matrix can be found with 

        \begin{equation}
            A^{-1} = \frac{1}{A_{11}A_{22} - A_{12}A_{21}}
            \begin{bmatrix}
                A_{22} & -A_{12} \\
                -A_{21} & A_{11} \\
            \end{bmatrix}
        \end{equation}
        Assuming that the condition in (7) is true, this means that our inverse of $A$ is going to be

        \begin{equation}
            A^{-1} = \frac{1}{I(0) - a(a^{T})} 
            \begin{bmatrix}
                0 & -a \\
                -a^{T} & I \\
            \end{bmatrix}
            = \frac{1}{-aa^{T}}
            \begin{bmatrix}
                0 & -a \\
                -a^{T} & I \\
            \end{bmatrix}.
        \end{equation}
        We can show that (9) is the inverse by showing that $AA^{-1} = I$. Namely,
        \begin{align}
            AA^{-1} & = 
            \begin{bmatrix}
                I & a \\
                a^{T} & 0 \\
            \end{bmatrix}
            \frac{1}{-aa^{T}}
            \begin{bmatrix}
                0 & -a \\
                -a^{T} & I \\
            \end{bmatrix} \\
            & = \frac{1}{-aa^{T}}
            \begin{bmatrix}
                I & a \\
                a^{T} & 0 \\
            \end{bmatrix}
            \begin{bmatrix}
                0 & -a \\
                -a^{T} & I \\
            \end{bmatrix} \\
            & = -\frac{1}{aa^{T}}
            \begin{bmatrix}
                I(0) - aa^{T} & -I(a) + a(I) \\
                a^{T}(0) - 0(a^{T}) & -a^{T}(a) + 0(I) \\
            \end{bmatrix} \\
            & = -\frac{1}{aa^{T}}
            \begin{bmatrix}
                -aa^{T} & 0 \\
                0 & -a^{T}a \\
            \end{bmatrix}
            = -\frac{1}{aa^{T}}
            \begin{bmatrix}
                -aa^{T} & 0 \\
                0 & -aa^{T} \\
            \end{bmatrix} \\
            & = -\frac{-aa^{T}}{aa^{T}}
            \begin{bmatrix}
                1 & 0 \\
                0 & 1 \\
            \end{bmatrix} \\
            & = I.
        \end{align}
        Therefore, the inverse that was found in (9) is indeed valid.
    \end{highlight}
\end{problem}

% Problem 5 Summary
\begin{summary}{Problem 5 Summary}
    \begin{statement}{Procedure}
        \begin{itemize}
            \item For part (a), perform the matrix / vector multiplication and solve for $a$.
            \item For part (b), calculate the inverse of the matrix and show that it is indeed the inverse.
        \end{itemize}
    \end{statement}
    \begin{statement}{Key Concepts}
        \begin{itemize}
            \item \textbf{Problem Statement:}
            \begin{itemize}
                \item The problem involves determining the inverse of a block matrix $A$ which is an $(n + 1) \times (n + 1)$ matrix. The matrix $A$ is composed of an identity matrix 
                $I$, a zero vector, an $n$-vector $a$, and its transpose $a^T$.
            \end{itemize}
            \item \textbf{Matrix Representation:}
            \begin{itemize}
                \item The matrix $A$ is represented as:
                \begin{equation*}
                    A = 
                    \begin{pmatrix} 
                        I & a \\ 
                        a^T & 0 
                    \end{pmatrix}
                \end{equation*}
                \item Here, $I$ is an $n \times n$ identity matrix, $a$ is an $n$-vector, and $a^T$ is its transpose.
            \end{itemize}
            \item \textbf{Conditions for Invertibility:}
            \begin{itemize}
                \item The problem seeks to find conditions under which the matrix $A$ is invertible.
                \item The invertibility of $A$ is expressed in terms of the vector $a$ and its properties.
            \end{itemize}
            \item \textbf{Solution for Inverse Matrix:}
            \begin{itemize}
                \item Assuming the condition for invertibility is met, an expression for the inverse matrix $A^{-1}$ is derived.
                \item The solution involves manipulating the blocks of the matrix and applying the properties of matrix inversion.
            \end{itemize}
            \item \textbf{Verification of Solution:}
            \begin{itemize}
                \item The inverse matrix $A^{-1}$ is verified by showing that the product $AA^{-1}$ equals the identity matrix.
                \item The verification includes detailed matrix multiplication steps to confirm the identity.
            \end{itemize}
            \item \textbf{Linear Algebra Concepts:}
            \begin{itemize}
                \item This problem illustrates the concepts of block matrices and their inverses in linear algebra.
                \item It emphasizes the importance of matrix properties and operations in determining the invertibility of complex matrix structures.
            \end{itemize}
        \end{itemize}
    \end{statement}
    \begin{statement}{Variations}
        \begin{itemize}
            \item We could be given a different original matrix.
            \begin{itemize}
                \item In this case we would use the same procedure from the original problem with the new matrix.
            \end{itemize}
        \end{itemize}
    \end{statement}
\end{summary}

% Problem 6
\begin{problem}{Problem 6}
    \begin{statement}{Problem 6}
        Solve and Explain the solution to 11.16 here in your own words. (Since you are given a solution, you will be graded on your ability to explain). Be sure to add your own interpretation and 
        explanation. What is a “running sum” matrix? Why is it called this?

        \subsubsection*{Original Question:}

        \textit{Inverse of running sum matrix}. Find the inverse of the $n \times n$ running sum matrix,

        \begin{equation*}
            S = 
            \begin{bmatrix}
                1 & 0 & \dots & 0 & 0 \\
                1 & 1 & \dots & 0 & 0 \\
                \vdots & \vdots & \ddots & \vdots & \vdots \\
                1 & 1 & \dots & 1 & 0 \\
                1 & 1 & \dots & 1 & 1 \\
            \end{bmatrix}.
        \end{equation*}
        Does your answer make sense?
    \end{statement}

    \begin{highlight}[Solution]
        \noindent For this problem, I am going to use \textbf{Method 4}. \vspace*{1em}

        A running sum matrix is a matrix where the first entry of the first row and column is 1. Each succeeding row has another column entry as 1. The last row of the matrix has a 1 in every column entry.

        When searching for the inverse of $S$, we seek to have 

        \setcounter{equation}{0}
        \begin{equation}
            SS^{-1} = I.
        \end{equation}
        When we perform the multiplication of $S$ with its inverse we want the resultant matrix have 1s in the diagonals and 0s everywhere else.

        Take for instance a simple $3 \times 3$ running sum matrix $s$

        \begin{equation}
            s = 
            \begin{bmatrix}
                1 & 0 & 0 \\
                1 & 1 & 0 \\
                1 & 1 & 1 \\
            \end{bmatrix}.
        \end{equation}
        If we seek to find the inverse of this matrix we can use an arbitrary $s^{-1}$ matrix with

        \begin{equation}
            s^{-1} =
            \begin{bmatrix}
                s_{11} & s_{12} & s_{13} \\
                s_{21} & s_{22} & s_{23} \\
                s_{31} & s_{32} & s_{33} \\
            \end{bmatrix}.
        \end{equation}
        The multiplication of $s$ and $s^{-1}$ would then be

        \begin{align}
            ss^{-1} & = 
            \begin{bmatrix}
                1 & 0 & 0 \\
                1 & 1 & 0 \\
                1 & 1 & 1 \\
            \end{bmatrix}
            \begin{bmatrix}
                s_{11} & s_{12} & s_{13} \\
                s_{21} & s_{22} & s_{23} \\
                s_{31} & s_{32} & s_{33} \\
            \end{bmatrix} \\
            & = 
            \begin{bmatrix}
                1(s_{11}) + 0(s_{21}) + 0(s_{31}) & 1(s_{12}) + 0(s_{22}) + 0(s_{23}) & 1(s_{13}) + 0(s_{23}) + 0(s_{33}) \\
                1(s_{11}) + 1(s_{21}) + 0(s_{31}) & 1(s_{12}) + 1(s_{22}) + 0(s_{23}) & 1(s_{13}) + 1(s_{23}) + 0(s_{33}) \\
                1(s_{11}) + 1(s_{21}) + 1(s_{31}) & 1(s_{12}) + 1(s_{22}) + 1(s_{23}) & 1(s_{13}) + 1(s_{23}) + 1(s_{33}) \\
            \end{bmatrix} \\
            & = 
            \begin{bmatrix}
                s_{11} & s_{12} & s_{13} \\
                s_{11} + s_{21} & s_{12} + s_{22} & s_{13} + s_{23} \\
                s_{11} + s_{21} + s_{31} & s_{12} + s_{22} + s_{23} & s_{13} + s_{23} + s_{33} \\
            \end{bmatrix}.
        \end{align}
        We are aiming to have only the diagonals in (6) be nonzero. We can then infer that the following must be true
        \begin{align*}
            s_{11} & = 1 \\
            s_{12} & = 0 \\
            s_{13} & = 0 \\
            s_{11} + s_{21} & = 0 \\
            s_{12} + s_{22} & = 1 \\
            s_{13} + s_{23} & = 0 \\
            s_{11} + s_{21} + s_{31} & = 0 \\
            s_{12} + s_{22} + s_{23} & = 0 \\
            s_{13} + s_{23} + s_{33} & = 1.
        \end{align*}
        From the above we can deduce that the inverse of $s$ must then be

        \begin{equation}
            s^{-1} = 
            \begin{bmatrix}
                1 & 0 & 0 \\
                -1 & 1 & 0 \\
                0 & -1 & 1 \\
            \end{bmatrix}.
        \end{equation}
        If we use the result from our $3 \times 3$ running sum matrix we can extrapolate it to $S$. This then means the inverse of the running sum matrix in our problem statement is then

        \begin{equation}
            S^{-1} = 
            \begin{bmatrix}
                1 & 0 & \dots & 0 & 0 \\
                -1 & 1 & \dots & 0 & 0 \\
                \vdots & \vdots & \ddots & \vdots & \vdots \\
                0 & 0 & \dots & 1 & 0 \\
                0 & 0 & \dots & -1 & 1 \\
            \end{bmatrix}.
        \end{equation}
        Comparing this with our $3 \times 3$ matrix, this inverse does make sense.
    \end{highlight}
\end{problem}

% Problem 6 Summary
\begin{summary}{Problem 6 Summary}
    \begin{statement}{Procedure}
        \begin{itemize}
            \item Determine the inverse of a $3 \times 3$ running sum matrix and extrapolate it into an arbitrary size matrix.
        \end{itemize}
    \end{statement}
    \begin{statement}{Key Concepts}
        \begin{itemize}
            \item \textbf{Problem Statement:}
            \begin{itemize}
                \item The task is to find the inverse of an $n \times n$ running sum matrix $S$ and explain the solution, including what a “running sum” matrix is and why it is called this.
            \end{itemize}
            \item \textbf{Definition of Running Sum Matrix:}
            \begin{itemize}
                \item A running sum matrix $S$ is defined such that the first entry of the first row and column is 1, with each succeeding row having an additional column entry of 1. The last row 
                has a 1 in every column entry.
                \item This structure leads to the term "running sum" as each row represents a cumulative sum of the previous rows.
            \end{itemize}
            \item \textbf{Inverse Calculation:}
            \begin{itemize}
                \item To find the inverse $S^{-1}$, the problem demonstrates setting up an arbitrary matrix and solving the equation $SS^{-1} = I$, where $I$ is the identity matrix.
                \item A step-by-step calculation is performed for a simple $3 \times 3$ running sum matrix to understand the inverse's structure.
            \end{itemize}
            \item \textbf{Solution Approach:}
            \begin{itemize}
                \item The approach involves systematically solving for each entry in the inverse matrix such that the product of $S$ and $S^{-1}$ results in the identity matrix.
                \item The conditions for the diagonals to be nonzero and off-diagonal elements to be zero are used to deduce the entries of $S^{-1}$.
            \end{itemize}
            \item \textbf{Generalization of Inverse:}
            \begin{itemize}
                \item The solution obtained for the $3 \times 3$ case is extrapolated to the $n \times n$ running sum matrix.
                \item The inverse $S^{-1}$ is presented in a general form applicable to any size $n$.
            \end{itemize}
            \item \textbf{Linear Algebra Concepts:}
            \begin{itemize}
                \item This problem showcases the concept of matrix inverses, particularly for a specialized matrix like the running sum matrix.
                \item It emphasizes the importance of understanding matrix structures and their properties in computing inverses.
            \end{itemize}
        \end{itemize}
    \end{statement}
    \begin{statement}{Variations}
        \begin{itemize}
            \item We could be given a different initial matrix.
            \begin{itemize}
                \item In this case we would use the same procedure for finding the inverse of this new matrix as we did for the original matrix.
            \end{itemize}
        \end{itemize}
    \end{statement}
\end{summary}