{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "OvsIpputCaE5"
   },
   "source": [
    "# Final Project - K-Means Algorithm\n",
    "## CSPB 2820 -  Linear Algebra with Computer Science Applications\n",
    "*Name*: $<$*Write your name here*$>$ \n",
    "\n",
    "Submit only this Jupyter notebook with the name format `Final_Project_<yourname>.ipynb`. Do not compress it using tar, rar, zip, etc. You must test this notebook in Jupyter Lab.\n",
    "Your solutions to analysis questions should be written directly below the associated question. You can add a write-up markdown cell or an extra Python cell if it wasn't provided.\n",
    "\n",
    "All the written code must be commented as shown in this link https://realpython.com/python-comments-guide/ \n",
    "\n",
    "Remember that you are encouraged to discuss the problems with your classmates and instructors, \n",
    "but *you must write all code and solutions on your own*, and list any people or sources consulted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UGn1CT7RvIkZ"
   },
   "source": [
    "The goal of K-means is to partition the data into $k$ clusters such that the sum of intra-cluster variances is minimal.\n",
    "\n",
    "In this problem, we'll be implementing K-means and evaluate it on the multi_blobs data shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7u8Bt-YOC43f"
   },
   "source": [
    "### Question 1 : What is the significance of K in K-Means algorithm ? [2 point]\n",
    "\n",
    "### Answer : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "NwSbUPByv1cP",
    "outputId": "9868be1f-40cc-4b2b-8edc-899aca9491bf"
   },
   "outputs": [],
   "source": [
    "# Do not modify this cell\n",
    "class DataBlobs:\n",
    "    def __init__(self, centers, std=1.75):\n",
    "        self.X, self.labels = make_blobs(n_samples=400, n_features=2, cluster_std=std, centers=centers,\n",
    "                                         shuffle=False, random_state=5622)\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.labels,\n",
    "                                                                                test_size=0.3, random_state=5622)\n",
    "multi_blobs = DataBlobs(centers=5, std=1.5)\n",
    "\n",
    "plt.title(\"multi_blobs\")\n",
    "plt.scatter(multi_blobs.X[:, 0], multi_blobs.X[:, 1], c=multi_blobs.labels)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WXwrcd3UvwPV"
   },
   "source": [
    "We will be using Euclidean distance as our variance measure, so for cluster $C_i = \\{x_1,x_2,... x_{m_i}\\}$, its intra-cluster variance $V(C_i)$ is defined as:\n",
    "\n",
    "$$\n",
    "V(C_i) = \\sum_{j=1}^{m_i} ||x_j - \\mu_i||^2\n",
    "$$\n",
    "\n",
    "where $\\mu_i = \\frac{1}{m} \\sum_{i=1}^{m_i} x_i$. $\\mu_i$ is called the centroid of cluster $C_i$.\n",
    "\n",
    "So for $k$ clusters, K-means objective is:\n",
    "$$\n",
    "\\min_{C_1,C_2\\ldots C_k}\\sum_{i=1}^{k}V(C_i) = \\min_{C_1,C_2\\ldots C_k} \\sum_{i=1}^{k} \\sum_{j=1}^{m_i} ||x_j - \\mu_i||^2\n",
    "$$\n",
    "\n",
    "Each sample $x_i$ is assigned to the cluster of the closest centroid. Hence, finding the optimal partition $\\{C_1,C_2...C_k\\}$ that minimizes the objective is equivalent to finding the optimal centroids.\n",
    "\n",
    "Unfortunately, there is no algorithm that reaches the global optima for K-means, but we'll be implementing the most famous heuristic for the problem: Llyod algorithm. It works as follows:\n",
    "\n",
    "- Initialize the centroids with **unique** random samples (`initialize_centroids`), initial objective = $+\\infty$\n",
    "- Repeat until convergence:\n",
    "    - Compute the distances matrix $D$ between samples and centroids (`compute_distances`)\n",
    "    - Use $D$ to assign each sample to the cluster with the closest centroid (`computes_assignments`)\n",
    "    - Update the centroids as centers of the new cluster assignments (`compute_centroids`)\n",
    "    - Compute the new objective (`compute_objective`)\n",
    "    - Stop if the improvement ratio of the objective is less than $\\epsilon$\n",
    "\n",
    "The improvement ratio equal to `|new_objective - previous_objective|/|previous_objective|`.\n",
    "\n",
    "\n",
    "- **Implementation.1 (I.1) [8 points]** `initialize_centroids` : select K **distinct** samples from the matrix data `X` and use them as the initial centroids. Store these centroids in the class attribute `self.centroids` as an `np.array` of shape $k \\times d$.\n",
    "- **I.2 [8 points]** `compute_distances` : compute the distance of each sample $x_i$ to every centroid $c_j$ and return the result as a matrix `distances_matrix` of size $N \\times k$ where `N` is the number of samples and `k` is the chosen number of clusters to be found. The cell `(i,k)` shall contain the euclidean distance between sample $x_i$ and centroid $m_k$.\n",
    "- **I.3 [8 points]** `compute_assignments` : given the distances matrix of size $N \\times k$ return an array of labels in which each element is an integer in the range $[0, k-1]$ and it represents which centroid it's closest to.\n",
    "- **I.4 [10 points]** `compute_centroids` : Compute the new centroids depending on the new set of samples that has been alloted to each cluster.\n",
    "- **I.5 [12 points]** `fit` : This shall contain your main loop which implements the algorithm described above. You'll sequentially call the methods above to find the $k$ centroids. Break the loop when the improvement ratio of the objective is within `rtol`. At the end (or start, depending on how you code it) of each iteration, call the method `save_plot` to save the current clustering status and save the current objective value in the `history` list.\n",
    "- **I.6 [8 points]** `predict` : Given new samples, return their assigned clusters that were learned in the `fit` step.\n",
    "\n",
    "While we're working on 2-d data (d=2) for visualization purposes, your implementation should handle any number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TXjPPia0nQFz"
   },
   "outputs": [],
   "source": [
    "class KMeans:\n",
    "    def __init__(self, k, rtol=1e-3):\n",
    "        \"\"\"\n",
    "        :param k: Number of centroids\n",
    "        :param rtol: Epsilon\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.centroids = None\n",
    "        self.snapshots = []  # buffer for progress plots\n",
    "        self.rtol = rtol\n",
    "\n",
    "    def initialize_centroids(self, X):\n",
    "        \"\"\"\n",
    "        Randomly select k **distinct** samples from the dataset in X as centroids\n",
    "        @param X: np.ndarray of dimension (num_samples, num_features)\n",
    "        @return: centroids array of shape (k, num_features)\n",
    "        \"\"\"\n",
    "        # Workspace I.1\n",
    "        #BEGIN \n",
    "        # code here\n",
    "\n",
    "        #END\n",
    "        return centroids\n",
    "\n",
    "    def compute_distances(self, X):\n",
    "        \"\"\"\n",
    "        Compute a distance matrix of size (num_samples, k) where each cell (i, j) represents the distance between\n",
    "        i-th sample and j-th centroid. We shall use Euclidean distance here.\n",
    "        :param X: np.ndarray of shape (num_samples, num_features)\n",
    "        :return: distances_matrix : (np.ndarray) of the dimension (num_samples, k)\n",
    "        \"\"\"\n",
    "        distances_matrix = np.zeros((X.shape[0], self.k))\n",
    "        # Workspace I.2\n",
    "        #BEGIN \n",
    "        # code here\n",
    "                \n",
    "        #END\n",
    "        return distances_matrix\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_assignments(distances_to_centroids):\n",
    "        \"\"\"\n",
    "        Compute the assignment array of shape (num_samples,) where assignment[i] = j if and only if\n",
    "        sample i belongs to the cluster of centroid j\n",
    "        :param distances_to_centroids: The computed pairwise distances matrix of shape (num_samples, k)\n",
    "        :return: assignments array of shape (num_samples,)\n",
    "        \"\"\"\n",
    "\n",
    "        assignments = np.zeros((distances_to_centroids.shape[0],))\n",
    "\n",
    "       # Workspace I.3\n",
    "        #BEGIN \n",
    "        # code here   \n",
    "        \n",
    "        #END\n",
    "        return assignments\n",
    "\n",
    "    def compute_centroids(self, X, assignments):\n",
    "        \"\"\"\n",
    "        Given the assignments array for the samples, compute the new centroids\n",
    "        :param X: data matrix of shape (num_samples, num_features)\n",
    "        :param assignments: array of shape (num_samples,) where assignment[i] is the current cluster of sample i\n",
    "        :return: The new centroids array of shape (k, num_features)\n",
    "        \"\"\"\n",
    "        # Workspace I.4\n",
    "        centroids = np.zeros((self.k, X.shape[1]))\n",
    "        #BEGIN \n",
    "        # code here\n",
    "       \n",
    "        #END\n",
    "        return centroids\n",
    "\n",
    "    def compute_objective(self, X, assignments):\n",
    "        return np.sum(np.linalg.norm(X - self.centroids[assignments], axis=1) ** 2)\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Implement the K-means algorithm here as described above. Loop until the improvement ratio of the objective\n",
    "        is lower than rtol. At the end of each iteration, save the k-means objective and return the objective values\n",
    "        at the end\n",
    "\n",
    "        @param X:\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        self.centroids = self.initialize_centroids(X)\n",
    "        objective = np.inf\n",
    "        assignments = np.zeros((X.shape[0],))\n",
    "\n",
    "        # Workspace I.5\n",
    "\n",
    "        while True:\n",
    "            #BEGIN \n",
    "            # code here\n",
    "            \n",
    "            #END\n",
    "        return history\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Workspace I.6\n",
    "        assignments = np.zeros((X.shape[0],))\n",
    "        #BEGIN \n",
    "        # code here\n",
    "       \n",
    "        #END\n",
    "        return assignments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nGMioJi7A4hi"
   },
   "source": [
    "### Question 2 : Explain how will the intial centroids affect the working of the K-Means algorithm. [2 point]\n",
    "\n",
    "### Answer : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "irlgsaIfz2vX"
   },
   "outputs": [],
   "source": [
    "k_means = KMeans(5)\n",
    "objective_history = k_means.fit(multi_blobs.X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCscLkq60Hu_"
   },
   "source": [
    "## Evaluating K-means\n",
    "The easiest way to evaluate the clustering quality is to use the true labels. The natural question here is: which cluster corresponds to which label?\n",
    "\n",
    "Let's first formulate the question using `multi_blobs` dataset. We have 5 clusters and 5 classes in our data. Let's create a _confusion matrix_ $C$ between the clusters and the labels so that $ C_{i,j} = \\text{size}(\\text{cluster}_i \\cap \\text{class}_j)$.\n",
    "\n",
    "We model the unknown mapping using the $5\\times 5$ boolean matrix $X$ such that $X_{i,j}=1$ if and only if $\\text{cluster}_i$ is mapped to $\\text{class}_j$.\n",
    "\n",
    "To avoid having a cluster assigned to multiple classes, each row of $X$ is constrained to have only one non-zero entry.\n",
    "\n",
    "Now, given a mapping $X$ and confusion matrix $C$, the number of correctly \"classified\" samples (not really classification, more of clustering here) is:\n",
    "\n",
    "\\begin{align}\n",
    "\\#\\text{correct} = \\sum_i \\sum_j C_{i,j} X_{i,j}\n",
    "\\end{align}\n",
    "The goal is to find $\\hat{X}$ that maximizes $\\#\\text{correct}$. To solve for $X$ we're going to use `scipy`'s `linear_sum_assignment`\n",
    "([documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.linear_sum_assignment.html)).\n",
    "\n",
    "**I.7 [10 points]** Complete `evaluate_clustering` to return the accuracy of K-means using the optimal mapping $\\hat{X}$ defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linear_sum_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-HIUtYs80MYl"
   },
   "outputs": [],
   "source": [
    "def evaluate_clustering(trained_model, X, labels):\n",
    "    \"\"\"\n",
    "    Compute the ratio of correct matches between clusters from the trained model and the true labels\n",
    "    :param trained_model: Unsupervised learning model that predicts clusters\n",
    "    :param X: samples array, shape (num_samples, num_features)\n",
    "    :param labels: true labels array, shape (num_samples,\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # We can assume that the number of clusters and the number of class labels are the same\n",
    "    confusion_matrix = np.zeros((5,5))\n",
    "    boolean_matrix_X = np.zeros((5,5))\n",
    "    clusters = trained_model.predict(X)\n",
    "    # Workspace I.7\n",
    "    #BEGIN \n",
    "    # code here\n",
    "    \n",
    "    #END\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bWYsvO3xBDJd"
   },
   "source": [
    "### Question 3 : What other performance metrics can be used to evaluate K-Means performance ? [2 point]\n",
    "\n",
    "### Answer :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ontKTMx40aoZ"
   },
   "source": [
    "**I.8 [16 points]** Run K-means on the full `multi_blobs` for 20 times. Plot the histogram (`plt.hist`) of the clustering evaluation from `evaluate_clustering`. Also report the mean clustering performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z5oK0-Cz0RS7"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "accuracies = []\n",
    "# Workspace I.8\n",
    "#BEGIN \n",
    "# code here\n",
    "\n",
    "#END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oU9LQDJABT1C"
   },
   "source": [
    "### Question 4 : Is there any modification you can do to the K-Means to improve the performance ? If so explain. [4 point]\n",
    "\n",
    "### Answer :"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LinearAlgebraProject.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
