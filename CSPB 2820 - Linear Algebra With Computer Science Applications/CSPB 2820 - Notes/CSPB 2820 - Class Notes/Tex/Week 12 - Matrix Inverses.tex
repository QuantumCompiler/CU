\clearpage
\chapter{Week 12: (11/13 - 11/19)}

\section{Matrix Inverses}
\horizontalline{0}{0}

\subsection{Assigned Reading}

The reading assignments for this week can be found below:

\begin{itemize}
    \item \textbf{Sections 11.1, 11.2, 11.3, 11.4, 11.5} from VMLS.
\end{itemize}

\subsection{Piazza}

Must post / respond to at least \textbf{four} Piazza posts this week. \due{(11/17/23)} \cbox{piazza-week12}

\subsection{Lectures}

The lectures for this week and their links can be found below:

\begin{itemize}
    \item \href{https://applied.cs.colorado.edu/mod/hvp/view.php?id=50802}{11.1 Left \& Right Inverses} $\approx$ 18 min.
    \item \href{https://applied.cs.colorado.edu/mod/hvp/view.php?id=50803}{11.2 Inverse} $\approx$ 20 min.
    \item \href{https://applied.cs.colorado.edu/mod/hvp/view.php?id=50804}{11.2 Inverse: Inverse Of Products} $\approx$ 2 min.
    \item \href{https://applied.cs.colorado.edu/mod/hvp/view.php?id=50805}{11.3 Solving Linear Equations} $\approx$ 28 min.
    \item \href{https://applied.cs.colorado.edu/mod/hvp/view.php?id=50806}{11.4 Pseudo-Inverse} $\approx$ 15 min.
    \item \href{https://applied.cs.colorado.edu/mod/hvp/view.php?id=50807}{11.5 Finding Pseudo-Inverse With QR Factorization} $\approx$ 14 min.
    \item \href{https://applied.cs.colorado.edu/mod/hvp/view.php?id=50808}{Eigenvalues And Eigenvectors} $\approx$ 21 min.
    \item \href{https://applied.cs.colorado.edu/mod/hvp/view.php?id=50809}{Eigenvalues In Dynamical Systems} $\approx$ 27 min.
\end{itemize}

\subsection{Assignments}

The assignment for this week is \pdflink{./Assignments/VMLS Assignment 11 - Chapter 11.pdf}{VMLS Assignment 11: Chapter 11} \due{(11/20/23)} \cbox{assignment-week12}

\subsection{Quiz}

The quiz for this week is \href{https://applied.cs.colorado.edu/mod/quiz/view.php?id=50812}{Chapter 11.} \due{(11/20/23)} \cbox{quiz-week12}

\subsection{Chapter Summary}

The chapter that we will review this week is \textbf{VMLS Chapter 11 - Matrix Inverses}. The first section that we will be covering this week is \textbf{VMLS Section 11.1 - Left And Right Inverses}.

\begin{notes}{Section 11.1 - Left And Right Inverses}
    \subsection*{Overview}

    The concept of left and right inverses in matrices extends the idea of a multiplicative inverse from numbers to matrices. However, due to the non-commutative nature of matrix multiplication, the 
    distinction between left and right inverses becomes crucial. \vspace*{1em}

    \begin{Highlight}[Left Inverse]
        \begin{itemize}
            \item \textbf{Definition:} A matrix $X$ is a left inverse of matrix $A$ if it satisfies $XA = I$, where $I$ is the identity matrix.
            \item \textbf{Existence and Uniqueness:} A left inverse exists if and only if the columns of $A$ are linearly independent. It may not be unique unless $A$ is square.
            \item \textbf{Mathematical Representation:}
            \begin{equation*}
                XA = I
            \end{equation*}
            where $X$ is an $n \times m$ matrix if $A$ is an $m \times n$ matrix.
        \end{itemize}
    \end{Highlight}

    \begin{Highlight}[Right Inverse]
        \begin{itemize}
            \item \textbf{Definition:} A matrix $Y$ is a right inverse of matrix $A$ if it satisfies $AY = I$.
            \item \textbf{Existence and Uniqueness:} A right inverse exists if and only if the rows of $A$ are linearly independent. It is not unique unless $A$ is square.
            \item \textbf{Mathematical Representation:} 
            \begin{equation*}
                AY = I
            \end{equation*}
            where $Y$ is an $n \times m$ matrix if $A$ is an $m \times n$ matrix.
        \end{itemize}
    \end{Highlight}

    \begin{Highlight}[Invertibility]
        \begin{itemize}
            \item A matrix with both a left and a right inverse is invertible or nonsingular, and the left and right inverses are equal.
            \item \textbf{Mathematical Representation:} 
            \begin{equation*}
                AX = YA = I
            \end{equation*}
            Here, $X$ and $Y$ are equal and represent the inverse of $A$, denoted as $A^{-1}$.
            \item \textbf{Conditions for Invertibility:} A matrix is invertible if it is square and its columns (or rows) are linearly independent.
        \end{itemize}
    \end{Highlight}

    \subsection*{Applications}
    \begin{enumerate}
        \item Solving linear systems: If $A$ is left-invertible, $Ax = b$ has a unique solution $x = Xb$. If $A$ is right-invertible, any solution to $Ax = b$ can be found using a right inverse.
        \item Simplifying matrix equations like $AX = B$ or $YA = B$.
    \end{enumerate}

    \subsection*{Properties of Inverses}
    \begin{itemize}
        \item \textbf{Transpose Inverses:} If $A$ has a right (or left) inverse, then $A^T$ has a left (or right) inverse, given by $(A^{-1})^T$.
        \item \textbf{Product Inverses:} For invertible matrices $A$ and $B$, $(AB)^{-1} = B^{-1}A^{-1}$.
    \end{itemize}

    \subsection*{Implications}
    The existence of left and right inverses is crucial in linear algebra and relates to linear independence, dimensionality, solvability of linear systems, and stability of solutions.
\end{notes}

The next section that we will cover this week is \textbf{VMLS Section 11.2 - Inverse}.

\begin{notes}{VMLS Section 11.2 - Inverse}
    \subsection*{Overview}

    This section focuses on the concept of matrix inverses, their properties, and their applications in solving linear equations and other mathematical problems. \vspace*{1em}

    \subsection*{Left and Right Inverse of Matrix Product:}
    \begin{itemize}
        \item If matrices $A$ and $D$ are compatible for multiplication and both have right inverses $B$ and $E$ respectively, then $EB$ is a right inverse of $AD$.
        \item Similarly, if $A$ and $D$ have left inverses $C$ and $F$, then $FC$ is a left inverse of $AD$.
    \end{itemize}

    \subsection*{Uniqueness of Inverses:}
    \begin{itemize}
        \item If a matrix is both left- and right-invertible, the left and right inverses are unique and equal.
        \item A matrix with both a left inverse $Y$ and a right inverse $X$ is called invertible or nonsingular, and denoted as $A^{-1}$.
        \item Invertible matrices must be square, as tall matrices are not right-invertible and wide matrices are not left-invertible.
    \end{itemize}

    \subsection*{Solving Linear Equations with Inverse:}
    \begin{itemize}
        \item For square systems of linear equations $Ax = b$, if $A$ is invertible, then the unique solution is $x = A^{-1}b$.
        \item The solution $x$ is a linear function of the right-hand side vector $b$.
    \end{itemize}

    \subsection*{Examples and Special Cases:}
    \begin{itemize}
        \item The identity matrix $I$ is invertible with its inverse equal to itself.
        \item A diagonal matrix $A$ is invertible if and only if its diagonal entries are nonzero.
        \item Orthogonal matrices (square matrices with orthonormal columns) are invertible with the inverse equal to their transpose.
    \end{itemize}

    \subsection*{Inverse of Transpose and Matrix Product:}
    \begin{itemize}
        \item The inverse of a matrix transpose $(A^T)$ is the transpose of the inverse $(A^{-1})^T$.
        \item For invertible matrices $A$ and $B$ of the same size, $(AB)^{-1} = B^{-1}A^{-1}$.
    \end{itemize}

    \subsection*{Negative Matrix Powers and Triangular Matrices:}
    \begin{itemize}
        \item Negative matrix powers are defined for square invertible matrices, e.g., $(A^{-1})^k$ for positive integer $k$.
        \item Triangular matrices with nonzero diagonal elements are invertible.
    \end{itemize}

    \subsection*{Inverse via QR Factorization:}
    \begin{itemize}
        \item The QR factorization provides a method to compute the inverse of an invertible square matrix.
        \item For a matrix $A$ with QR factorization $A = QR$, where $Q$ is orthogonal and $R$ is upper triangular, the inverse is $A^{-1} = R^{-1}Q^T$.
    \end{itemize}

    This section provides a comprehensive understanding of matrix inverses, their properties, and applications, especially in the context of solving linear equations and determining the invertibility of matrices.
\end{notes}

The next section that we will be covering this week is \textbf{VMLS Section 11.3 - Solving Linear Equations}. 

\begin{notes}{VMLS Section 11.3 - Solving Linear Equations}
    \subsection*{Overview}

    This section starts with an algorithm for solving linear equations $Rx = b$ where $R$ is an upper triangular $n \times n$ matrix with nonzero diagonal entries, making it invertible. The process, 
    known as back substitution, involves solving for the variables one at a time, starting from $x_n$ and working backwards. \vspace*{1em}

    \begin{Highlight}[Algorithm For Back Substitution:]
        \item Given an upper triangular matrix $R$ and an $n$-vector $b$.
        \item For $i = n$ down to $1$, calculate $x_i$ using the formula:
        \begin{equation*}
            x_i = \frac{b_i - \sum_{j=i+1}^{n} R_{ij}x_j}{R_{ii}}
        \end{equation*}
        \item This method computes $x = R^{-1}b$ and is guaranteed to work due to nonzero diagonal entries of $R$.
    \end{Highlight}

    \subsection*{Complexity Analysis:}
    \begin{itemize}
        \item The number of floating-point operations (flops) for back substitution is $n^2$.
    \end{itemize}

    \subsection*{Solving Equations Using QR Factorization}
    The section introduces a method to solve square systems of linear equations $Ax = b$ using QR factorization, particularly useful when $A$ is invertible.

    \begin{Highlight}[Algorithm for Solving via QR Factorization:]
        \begin{itemize}
            \item Given an invertible $n \times n$ matrix $A$ and an $n$-vector $b$.
            \item Compute the QR factorization $A = QR$.
            \item Calculate $y = Q^Tb$.
            \item Solve the triangular equation $Rx = y$ using back substitution.
        \end{itemize}
    \end{Highlight}

    \subsection*{Complexity Analysis:}
    \begin{itemize}
        \item The total number of flops for this method is approximately $2n^3$, indicating a cubic order of complexity.
        \item The QR factorization step dominates the computational cost.
    \end{itemize}

    \subsection*{Factor-Solve Methods}
    Factor-solve schemes are common in solving linear equations and involve two main steps: factorization and solving. The factor step involves decomposing the coefficient matrix, while the solve step 
    involves solving one or more linear equations using these factors. 

    \subsection*{Multiple Right-Hand Sides:}
    \begin{itemize}
        \item For solving multiple sets of equations $AX = B$ with the same coefficient matrix $A$ but different right-hand sides, the factorization step can be reused.
        \item The cost of solving multiple sets is approximately the same as solving one set, due to the reuse of factorization.
    \end{itemize}

    \subsection*{Computing the Matrix Inverse}
    The section describes a method to compute the inverse $B = A^{-1}$ of an invertible $n \times n$ matrix $A$ using QR factorization.

    \begin{Highlight}[Algorithm for Computing the Inverse:]
        \begin{itemize}
            \item Perform QR factorization of $A$.
            \item For each $i = 1$ to $n$, solve $Rb_i = \tilde{q}_i$ using back substitution, where $b_i$ and $\tilde{q}_i$ are columns of $B$ and $Q^T$, respectively.
        \end{itemize}
    \end{Highlight}

    \subsection*{Complexity Analysis:}
    \begin{itemize}
        \item The total computational cost is around $3n^3$ flops.
    \end{itemize}

    This section provides a thorough understanding of various methods to solve linear equations, emphasizing the importance of factorization techniques and their practical applications in 
    computational efficiency.
\end{notes}

The next section that we will be covering this week is \textbf{VMLS Section 11.4 - Examples}.

\begin{notes}{VMLS Section 11.4 - Examples}
    \subsection*{Sparse Linear Equations}
    \begin{itemize}
        \item Systems of linear equations with sparse coefficient matrices are common in various applications. 
        \item These equations can be solved more efficiently by exploiting the sparsity of the coefficient matrix, typically using a variant of the QR factorization algorithm.
        \item The memory usage and computational complexity depend on the sparsity pattern of the coefficient matrix, often requiring significantly fewer resources than for non-sparse matrices.
    \end{itemize}

    \subsection*{Polynomial Interpolation}
    \begin{itemize}
        \item The chapter discusses using matrix inverses for polynomial interpolation. 
        \item Given a cubic polynomial $p(x) = c_1 + c_2x + c_3x^2 + c_4x^3$, the coefficients can be determined by solving a system of linear equations $Ac = b$, where $A$ is a specific Vandermonde 
        matrix.
        \item The unique solution for the coefficients is given by $c = A^{-1}b$.
    \end{itemize}

    \subsection*{Balancing Chemical Reactions}
    \begin{itemize}
        \item The process involves setting up and solving a set of linear equations to balance atoms and charges in a chemical reaction.
        \item The reactant and product matrices are formed and used to balance the equation. 
        \item The coefficients for reactants and products are determined by solving the corresponding linear equations.
    \end{itemize}

    \subsection*{Heat Diffusion}
    \begin{itemize}
        \item The chapter examines a diffusion system with nodes having fixed potential and others with zero external source.
        \item This models a thermal system where some nodes are in constant contact with an external heat source, while others are internal without heat sources.
        \item The system is represented as a set of linear equations $Bs + Ce = d$, where $B$ and $C$ are diagonal matrices, and $d$ is a vector representing fixed potential and zero source conditions.
        \item The entire system, including flow conservation and boundary conditions, is assembled into a larger set of linear equations and solved assuming the coefficient matrix is invertible.
    \end{itemize}

    This section provides a diverse range of practical applications of matrix inverses, illustrating their significance in solving real-world problems in various fields.
\end{notes}

The last section that we will cover this week is \textbf{VMLS Section 11.5 - Pseuod-Inverse}.

\begin{notes}{VMLS Section 11.5 - Pseuod-Inverse}
    \subsection*{Overview}
    The pseudo-inverse, denoted as $A^\dagger$ or $A^+$, is a generalization of the matrix inverse that applies to matrices that are not necessarily square or invertible. It plays a crucial role in 
    solving over-determined or under-determined systems of linear equations and has significant applications in various fields. \vspace*{1em}
    
    \begin{Highlight}[Linearly Independent Columns and Gram Invertibility]
        \begin{itemize}
            \item An $m \times n$ matrix $A$ has linearly independent columns if and only if its $n \times n$ Gram matrix $A^T A$ is invertible.
            \item The pseudo-inverse for a square or tall matrix with linearly independent columns is given by:
            \begin{equation*}
                A^\dagger = (A^T A)^{-1}A^T
            \end{equation*}
        \end{itemize}
    \end{Highlight}
    
    \begin{Highlight}[Pseudo-Inverse of Square or Wide Matrix]
        \begin{itemize}
            \item For a square or wide matrix $A$, a right inverse is given by:
            \begin{equation*}
                A^\dagger = A^T (A A^T)^{-1}
            \end{equation*}
            \item This formula also defines the pseudo-inverse and reduces to the ordinary inverse when $A$ is square.
        \end{itemize}
    \end{Highlight}
    
    \subsection*{General Cases}
    The pseudo-inverse is defined for any matrix, including cases where $A$ is tall with linearly dependent columns or wide with linearly dependent rows, and even when $A$ is square but not invertible.
    
    \begin{Highlight}[Pseudo-Inverse via QR Factorization]
        \begin{itemize}
            \item For matrices that are left-invertible, the pseudo-inverse can be computed using QR factorization.
            \item The formula is given by:
            \begin{equation*}
                A^\dagger = R^{-1}Q^T
            \end{equation*}
            \item This computation involves QR factorization followed by back substitution on the columns of $Q^T$.
            \item The complexity of this method is $3mn^2$ flops for an $n \times m$ matrix.
        \end{itemize}
    \end{Highlight}
    
    \subsection*{Solving Over and Under-Determined Systems}
    \begin{itemize}
        \item The pseudo-inverse provides a method to solve over-determined systems (with linearly independent columns) and under-determined systems (with linearly independent rows).
        \item For an over-determined system $Ax = b$, the solution is $x = A^\dagger b$ if it exists.
        \item For an under-determined system $Ax = b$, any vector $b$ has a solution given by $x = A^\dagger b$.
    \end{itemize}
    
    \subsection*{Numerical Example}
    \begin{itemize}
        \item A numerical example using a $3 \times 2$ matrix $A$ with linearly independent columns demonstrates the application of the pseudo-inverse in solving an over-determined system of linear equations.
        \item The unique solution of $Ax = b$ is found using $A^\dagger$.
    \end{itemize}
    
    This chapter elaborates on the utility of the pseudo-inverse in various contexts, especially in solving linear systems that do not have a unique solution or where the coefficient matrix is not 
    square. The concept of the pseudo-inverse extends the applicability of matrix inverses to a broader range of matrices, enhancing the problem-solving toolkit in linear algebra.
\end{notes}