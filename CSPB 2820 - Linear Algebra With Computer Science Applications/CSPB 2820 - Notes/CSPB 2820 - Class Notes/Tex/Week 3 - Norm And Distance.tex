\clearpage

\renewcommand{\ChapTitle}{Norm And Distance}
\renewcommand{\SectionTitle}{Norm And Distance}

\chapter{\ChapTitle}
\section{\SectionTitle}
\horizontalline{0}{0}

\subsection{Assigned Reading}

The reading assignments for this week are from, \VMLS \hspace*{1pt} and \PyCap:

\begin{itemize}
    \item \pdflink{\ReadMatDir VMLS Chapter 3.1 - Norm.pdf}{VMLS Chapter 3.1 - Norm}
    \item \pdflink{\ReadMatDir VMLS Chapter 3.2 - Distance.pdf}{VMLS Chapter 3.2 - Distance}
    \item \pdflink{\ReadMatDir VMLS Chapter 3.3 - Standard Deviation.pdf}{VMLS Chapter 3.3 - Standard Deviation}
    \item \pdflink{\ReadMatDir VMLS Chapter 3.4 - Angle.pdf}{VMLS Chapter 3.4 - Angle}
    \item \pdflink{\ReadMatDir Python Companion Chapter 3.1 - Norm.pdf}{Python Companion Chapter 3.1 - Norm}
    \item \pdflink{\ReadMatDir Python Companion Chapter 3.2 - Distance.pdf}{Python Companion Chapter 3.2 - Distance}
    \item \pdflink{\ReadMatDir Python Companion Chapter 3.3 - Standard Deviation.pdf}{Python Companion Chapter 3.3 - Standard Deviation}
    \item \pdflink{\ReadMatDir Python Companion Chapter 3.4 - Angle.pdf}{Python Companion Chapter 3.4 - Angle}
\end{itemize}

\subsection{Piazza}

Must post / respond to at least \textbf{four} Piazza posts this week.

\subsection{Lectures}

The lectures for this week and their links can be found below:

\begin{itemize}
    \item \href{https://applied.cs.colorado.edu/mod/hvp/view.php?id=50692}{Norms} $\approx$ 25 min.
    \item \href{https://applied.cs.colorado.edu/mod/hvp/view.php?id=50693}{Chebyshev Inequality} $\approx$ 15 min.
    \item \href{https://applied.cs.colorado.edu/mod/hvp/view.php?id=50694}{Standard Deviation} $\approx$ 23 min.
    \item \href{https://applied.cs.colorado.edu/mod/hvp/view.php?id=50695}{Angle} $\approx$ 20 min.
    \item \href{https://applied.cs.colorado.edu/mod/hvp/view.php?id=50696}{Random Exercise} $\approx$ 14 min.
\end{itemize}

\subsection{Assignments}

The assignment for this week is:

\begin{itemize}
    \item \pdflink{\AssDir Assignment 3 - Norm And Distance.pdf}{Assignment 3 - Norm And Distance}
\end{itemize}

\subsection{Quiz}

The quiz for this week is:

\begin{itemize}
    \item \pdflink{\QuizDir Quiz 3 - Norm And Distance.pdf}{Quiz 3 - Norm And Distance}
\end{itemize}

\subsection{Chapter Summary}

The chapter that we will review this week is \textbf{VMLS Chapter 3 - Norm And Distance}. The first section that we will cover is \textbf{VMLS Section 3.1 - Norm}.

\begin{notes}{VMLS Section 3.1 - Norm}
    \subsection*{Overview}

    The norm of a vector is a mathematical concept that quantifies the length or magnitude of a vector in a multi-dimensional space. It provides a measure of how `big' or `small' a vector is. The most 
    common norm is the Euclidean norm, also known as the 2-norm or L2-norm, which calculates the vector's length as the square root of the sum of its squared components. In general, the norm of a vector 
    in an n-dimensional space is denoted as $||b||$, where $b$ represents the vector. The norm is a fundamental concept in linear algebra and has various applications in fields such as physics, 
    engineering, and machine learning, where it is used for tasks like vector normalization and distance calculations.

    \begin{highlight}[Norm Equation]
        The equation for calculating a norm is

        \begin{equation*}
            ||x|| = \sqrt{x^{2}_{1} + x^{2}_{2} + \dots + x^{2}_{n}}.
        \end{equation*}
    \end{highlight}

    \subsection*{Properties of Norm}

    Norms are mathematical functions used to measure the `size' or `length' of vectors in vector spaces. They possess fundamental properties that make them indispensable in various mathematical fields. 
    These properties include non-negativity, meaning that the norm of a vector is always non-negative, being zero if and only if the vector is the zero vector. Homogeneity reflects that scaling a vector 
    proportionally scales its norm. The triangle inequality asserts that the shortest path between two points in a vector space is a straight line. The submultiplicative property relates to the Hadamard 
    product of vectors. Norms also have equivalents in the context of infinite-dimensional spaces and sequences. Equivalence of norms shows that different norms offer similar notions of vector size. Finally, 
    norms satisfy the property that the norm of a linear combination of vectors is less than or equal to the sum of the norms of the individual vectors. These properties establish norms as a versatile and 
    essential tool in mathematics, finding applications in optimization, functional analysis, and signal processing, among other areas. Precisely, we can summarize these properties as

    \begin{itemize}
        \item Nonnegative homogeneity: $|| \beta x || = ||\beta|| || x ||$. Multiplying a vector by a scalar multiplies the norm by the absolute value of the scalar.
        \item Triangle inequality: $|| x + y || \leq || x || + || y ||$. The Euclidean norm of a sum of two vectors is no more than the sum of their norms. (The name of this property will be explained later.) 
        Another name for this inequality is subadditivity.
        \item Nonnegativity: $|| x || \geq 0$.
        \item Definiteness: $|| x || = 0$ only if $x = 0$.
    \end{itemize}

    \subsection*{Root Mean Square}

    The root-mean-square (RMS) value, denoted as RMS or rms, is a mathematical measure used to represent the effective magnitude or power of a varying quantity, such as a signal or dataset. It is computed by 
    taking the square root of the mean (average) of the squares of the values. This calculation method allows it to capture both positive and negative contributions, making it particularly useful for oscillating 
    or alternating quantities. The RMS value preserves the energy content of the dataset and is often used in fields like physics, engineering, and signal processing to quantify signal strength, power, or 
    variability. It serves as a common metric for comparing varying quantities to constant values, providing a meaningful and widely applicable representation of their magnitude.

    \begin{highlight}[Root Mean Square Equation]
        The equation for the root mean square is

        \begin{equation*}
            \mathbf{rms}(x) = \sqrt{\frac{x^{2}_{1} + \dots + x^{2}_{n}}{n}} = \frac{||x||}{\sqrt{n}}.
        \end{equation*}
    \end{highlight}

    \subsection*{Norm Of Sum}

    The norm of the sum of two vectors, often denoted as $\|x + y\|$, represents the length or magnitude of the resultant vector when two vectors, $x$ and $y$, are added together. This concept is fundamental in vector 
    algebra and geometry. The norm of the sum obeys the triangle inequality, which states that the magnitude of the sum of two vectors is always less than or equal to the sum of their individual magnitudes, i.e., 
    $\|x + y\| \leq \|x\| + \|y\|$. This inequality signifies that the shortest path between two points in Euclidean space is a straight line. The norm of the sum plays a crucial role in various mathematical and 
    scientific disciplines, such as physics and engineering, where it helps quantify the combined effect or magnitude of multiple vector quantities.

    \begin{highlight}[Norm Of Sum Equation]
        To calculate the norm of sum of vectors we use

        \begin{equation*}
            ||x + y|| = \sqrt{||x||^{2} + 2x^{T}y + ||y||^{2}}.
        \end{equation*}
    \end{highlight}

    \subsection*{Norm Of Block Vectors}

    The norm of block vectors, often used in linear algebra and functional analysis, extends the concept of vector norm to structured vectors composed of multiple sub-vectors or blocks. In this context, a block 
    vector consists of several concatenated sub-vectors, each representing a distinct component or feature of the overall vector. The norm of a block vector is typically defined as the square root of the sum of 
    the squares of the norms of its constituent sub-vectors. This allows for the measurement of the overall magnitude or length of the block vector, while considering the individual magnitudes of its components. 
    Block vectors and their norms find applications in various areas, including multivariate statistics, data analysis, and numerical optimization, where structured data representations are common.

    \begin{highlight}[Norm Of Block Vectors Equation]
        To calculate the norm of block vectors we use

        \begin{equation*}
            ||(a,b,c)|| = \sqrt{||a||^{2} + ||b||^{2} + ||c||^{2}} = ||(||a||,||b||,||c||)||.
        \end{equation*}
    \end{highlight}


    \subsection*{Chebyshev Inequality}

    The Chebyshev Inequality is a fundamental statistical theorem that provides bounds on the probability of a random variable deviating from its mean. It is particularly useful when the exact probability distribution 
    of the variable is unknown or when dealing with data exhibiting variability. The inequality states that for any random variable with a finite mean and variance, the probability that the random variable deviates from 
    its mean by more than a specified number of standard deviations is bounded. Specifically, it asserts that no more than 1 divided by the square of the specified number of standard deviations will be the probability 
    of such an event occurring. This inequality is valuable for estimating the likelihood of extreme events or outliers in a dataset and is widely used in statistics and probability theory for setting confidence intervals 
    and making probabilistic statements about data distributions.

    \begin{highlight}[Chebyshev Inequality Equation]
        We can represent the Chebyshev Inequality in terms of the root mean square of a vector with

        \begin{equation*}
            \frac{k}{n} \leq \Bigg(\frac{\mathbf{rms}(x)}{a}\Bigg)^{2}.
        \end{equation*}
    \end{highlight}
\end{notes}

The next section that we will review this week is \textbf{VMLS Section 3.4 - Angle}.

\begin{notes}{VMLS Section 3.4 - Angle}
    \subsection*{Overview}

    The concept of the angle between two vectors is fundamental in vector mathematics and linear algebra. It provides a measure of the angular separation between the directions of two vectors within a vector space. The 
    angle between two vectors can be determined by utilizing the dot product and trigonometric functions. Specifically, the dot product formula relates the dot product of vectors \(A\) and \(B\) to the magnitudes of the 
    vectors and the cosine of the angle \(θ\) between them. Solving for \(θ\) through inverse cosine (\(\cos^{-1}\)), we can express it as the arc cosine of the dot product divided by the product of the magnitudes. This 
    mathematical approach offers a versatile method for calculating the angle between vectors in various applications, including physics, engineering, and computer graphics, where understanding spatial orientations is crucial.

    \subsection*{Cauchy-Schwarz Inequality}

    The Cauchy-Schwarz inequality is a fundamental mathematical inequality that applies to inner product spaces, including Euclidean spaces. It states that for any two vectors, the absolute value of their inner product is always 
    less than or equal to the product of their individual norms or magnitudes. Mathematically, for vectors \( \mathbf{a} \) and \( \mathbf{b} \), the inequality is expressed as \( | \mathbf{a}^{T}\mathbf{b} | \leq \| \mathbf{a} \| 
    \cdot \| \mathbf{b} \| \). Geometrically, this means that the angle between the vectors is related to their inner product and magnitudes, providing a measure of how much the vectors align. The Cauchy-Schwarz inequality has 
    wide-ranging applications in mathematics, physics, engineering, and statistics, serving as a fundamental tool for establishing bounds and relationships between vectors in various contexts.

    \begin{highlight}[Cauchy-Schwarz Inequality Equation]
        To calculate the Cauchy-Schwarz inequality we use

        \begin{equation*}
            |a^{T}b| \leq ||a|| ||b||.
        \end{equation*}
    \end{highlight}

    \subsection*{Correlation Coefficient}

    The correlation coefficient is a statistical measure used to assess the strength and direction of the linear relationship between two variables. It quantifies how well the variations in one variable can be predicted by the 
    variations in another. The coefficient typically ranges from -1 to 1, with -1 indicating a perfect negative linear relationship, 1 representing a perfect positive linear relationship, and 0 indicating no linear relationship. 
    A positive coefficient implies that as one variable increases, the other tends to increase as well, while a negative coefficient suggests that as one variable increases, the other tends to decrease. The correlation coefficient 
    is widely employed in fields like statistics, economics, and social sciences to analyze data, make predictions, and determine the degree of association between variables.

    \begin{highlight}[Correlation Coefficient Equation]
        Given two vectors $a$ and $b$, their associated de-meaned vectors are

        \begin{equation*}
            \tilde{a} = a - \mathbf{avg}(a)\mathbf{1} \hspace*{5pt} , \hspace*{5pt} \tilde{b} = b - \mathbf{avg}(b)\mathbf{1}.
        \end{equation*}

        The correlation coefficient $\rho$ is then calculated with 

        \begin{equation*}
            \rho = \frac{\tilde{a}^{T}\tilde{b}}{||\tilde{a}|| ||\tilde{b}||}.
        \end{equation*}

        With 

        \begin{equation*}
            u = \frac{\tilde{a}}{\mathbf{std}(a)} \hspace*{5pt} , \hspace*{5pt} v = \frac{\tilde{b}}{\mathbf{std}(b)}
        \end{equation*}

        we can also calculate the correlation coefficient with

        \begin{equation*}
            \rho = \frac{u^{T}v}{n}.
        \end{equation*}
    \end{highlight}

    \subsection*{Standard Deviation Of Sum}

    The standard deviation of the sum of vectors is a statistical measure used to quantify the spread or variability of the resultant vector when multiple random vectors are added together. In the context of vectors, particularly 
    in statistics and probability theory, this concept is valuable for understanding the uncertainty or dispersion that arises when combining multiple random variables. It extends the notion of standard deviation to vector spaces 
    and is often employed in fields like finance and data analysis to assess risk or measure the overall variability of a portfolio or dataset resulting from the aggregation of individual vector components. Calculating the standard 
    deviation of the sum involves considering both the individual standard deviations of the vectors and their pairwise covariances, providing valuable insights into the behavior of the combined vector.

    \begin{highlight}[Standard Deviation Of Sum Equation]
        To calculate the standard deviation of sum we use

        \begin{equation*}
            \mathbf{std}(a + b) = \sqrt{\mathbf{std}(a)^{2} + 2\rho\mathbf{std}(a)\mathbf{std}(b) + \mathbf{std}(b)^{2}}.
        \end{equation*}
    \end{highlight}
\end{notes}