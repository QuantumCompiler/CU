\clearpage

\renewcommand{\ChapTitle}{Code Complexity, Binary Search Tree, Sorting Methods}

\chapter{\ChapTitle}
\section{\ChapTitle}
\horizontalline{0}{0}

\subsection{Activities}

The following are the activities that are planned for Week 6 of this course.

\begin{itemize}
    \item Watch week videos.
    \item Read Ch. 9 and Ch. 10 of the Data Structures zyBook and take the in chapter quizzes (due next Monday).
    \item BST Homework is due Tuesday.
    \item Start on Sorting Homework (Due next Tuesday).
    \item Make sure your Proctorio extension is installed and working. --- Take verification quiz to make sure your browser is set up and ready for the exam next week.
\end{itemize}

\subsection{Lectures}

Here are the lectures that can be found for this week:

\begin{itemize}
    \item \href{https://applied.cs.colorado.edu/mod/hvp/view.php?id=45942}{Complexity}
    \item \href{https://applied.cs.colorado.edu/mod/hvp/view.php?id=45943}{BST Operation Complexity}
    \item \href{https://applied.cs.colorado.edu/mod/hvp/view.php?id=45944}{Hard Problems}
    \item \href{https://applied.cs.colorado.edu/mod/hvp/view.php?id=45945}{Bubble Sort}
    \item \href{https://applied.cs.colorado.edu/mod/hvp/view.php?id=45946}{Merge Sort}
    \item \href{https://applied.cs.colorado.edu/mod/hvp/view.php?id=45947}{Item Sort}
\end{itemize}

\noindent The lectures for this week are:

\begin{itemize}
    \item \pdflink{\LectureNotesDir Complexity & Sorting Algorithms Lecture Notes.pdf}{Complexity \& Sorting Algorithms Lecture Notes}
\end{itemize}

\subsection{Programming Assignment}

The programming assignment for Week 6 is:

\begin{itemize}
    \item \href{https://github.com/QuantumCompiler/CU/tree/main/CSPB%202270%20-%20Data%20Structures/Assignments/Assignment%204%20-%20Sorting%20Algorithms}{Programming Assignment 4 - Sorting}
    \item \pdflink{\InterviewDir Sorting Algorithms Interview Notes.pdf}{Sorting Algorithms Interview Notes}
\end{itemize}

\subsection{Chapter Summary}

The first chapter of this week is \textbf{Chapter 9: Searching \& Algorithm Analysis}.

\begin{notes}{Section 9.1 - Constant Time Operations}
    Constant time operations in the context of object-oriented programming refer to operations that execute in a fixed and predictable amount of time, regardless of the size or complexity of the data being processed. These operations provide efficient and consistent performance, allowing developers to perform tasks with a time complexity of $\mathcal{O}(1)$. 
    By implementing constant time operations, developers can optimize their code for quick and reliable execution, resulting in improved scalability and responsiveness in their object-oriented programs.
\end{notes}

\begin{notes}{Section 9.2 - Growth of Functions \& Complexity}
    \subsubsection*{Upper \& Lower Bounds}

    Upper and lower bounds play a crucial role in analyzing the growth rates and complexities of functions and algorithms. An upper bound represents the worst-case growth rate of a function, while a lower bound represents the best-case growth rate. The big $\mathcal{O}$ notation ($\mathcal{O}$ notation) is used to express upper bounds, while the big $\Omega$ notation (Omega notation) represents 
    lower bounds. By combining both upper and lower bounds, the tightest possible bound, denoted by $\Theta$ notation (Theta notation), can be determined. Analyzing these bounds enables us to understand how the performance of algorithms and functions scales with input size, aiding in algorithm selection, optimization, and predicting efficiency in different scenarios.
    
    \begin{center}
        \begin{tabular}{|c|c|c|}
            \hline Notation & General Form & Meaning \\ \hline
            $\mathcal{O}$ & $T(N)=\mathcal{O}(f(N))$ & A positive constant $c$ exists such that, for all $N \geq 1$, $T(N) \leq c * f(N)$ \\ \hline
            $\Omega$ & $T(N)=\Omega(f(N))$ & A positive constant $c$ exists such that, for all $N \geq 1$, $T(N) \geq c * f(N)$ \\ \hline
            $\Theta$ & $T(N)=\Omega(f(N))$ & $T(N)=\mathcal{O}(f(N))$ and $T(N)=\Omega(f(N))$ \\ \hline
        \end{tabular}
    \end{center}
\end{notes}

\begin{notes}{Section 9.3 - $\mathcal{O}$ Notation}
    \subsubsection*{Big $\mathcal{O}$ Notation}

    Big $\mathcal{O}$ notation is a fundamental concept in computer science used to analyze the efficiency and scalability of algorithms. It provides a standardized way to express the upper bound on the worst-case time complexity of an algorithm in terms of the input size. Big $\mathcal{O}$ notation represents the growth rate of an algorithm as a function of the input size, allowing comparisons between different algorithms 
    and helping in algorithm selection. It disregards constant factors and lower-order terms, focusing solely on the dominant term that determines the algorithm's scalability. Big $\mathcal{O}$ notation provides a concise and abstract representation of algorithmic complexity, enabling engineers and developers to understand and reason about the efficiency of algorithms without getting caught up in implementation details.
    
    \subsubsection*{Big $\mathcal{O}$ Notation of Composite Functions}
    
    When dealing with composite functions, Big $\mathcal{O}$ notation provides a concise way to express the overall growth rate of the composed functions. The Big $\mathcal{O}$ notation of composite functions is determined by considering the dominant term in each function and evaluating how they interact with one another. If two functions $f(n)$ and $g(n)$ have respective Big $\mathcal{O}$ notations of $\mathcal{O}(f(n))$ and 
    $\mathcal{O}(g(n))$, the Big $\mathcal{O}$ notation of the composite function is determined by taking the maximum growth rate among the two. In other words, if the growth rate of $f(n)$ is faster than $g(n)$, the composite function's Big $\mathcal{O}$ notation will be $\mathcal{O}(f(n))$. This approach allows for a simplified representation of the overall complexity of a composite function and facilitates analysis of algorithmic 
    efficiency when multiple functions are involved.
    
    \begin{center}
        \begin{tabular}{|c|c|}
            \hline Composite Function & Big $\mathcal{O}$ Notation \\ \hline
            $c \cdot \mathcal{O}(f(N))$ & $\mathcal{O}(f(N))$ \\ \hline
            $c + \mathcal{O}(f(N))$ & $\mathcal{O}(f(N))$ \\ \hline
            $g(N) \cdot \mathcal{O}(f(N))$ & $\mathcal{O}(g(N)*f(N))$ \\ \hline
            $g(N) + \mathcal{O}(f(N))$ & $\mathcal{O}(g(N) + f(N))$ \\ \hline
        \end{tabular}
    \end{center}
    
    \subsubsection*{Runtime Growth Rate}
    
    Runtime growth rate refers to how the execution time of an algorithm increases as the input size grows. It is commonly measured using Big $\mathcal{O}$ notation, which expresses the worst-case upper bound on the growth rate of an algorithm. The runtime growth rate provides insights into the scalability and efficiency of an algorithm, allowing developers to understand how the algorithm performs with larger inputs. By analyzing the runtime 
    growth rate, developers can make informed decisions about algorithm selection, identify potential performance bottlenecks, and optimize their code to improve overall efficiency. Understanding the runtime growth rate is crucial for designing algorithms that can handle larger and more complex datasets effectively.
    
    \subsubsection*{Common Big $\mathcal{O}$ Complexities}
    
    Common Big $\mathcal{O}$ complexities are used to describe the performance characteristics of algorithms as the input size increases. Some of the most common Big $\mathcal{O}$ complexities include $\mathcal{O}(1)$ O(1), $\mathcal{O}(\log{(n)})$ (logarithmic time), $\mathcal{O}(n)$ (linear time), $\mathcal{O}(n\log{(n)})$ (linearithmic time), $\mathcal{O}(n^2)$ (quadratic time), $\mathcal{O}(2^n)$ (exponential time), and $\mathcal{O}(n!)$
    (factorial time). These complexities represent different rates of growth and have a significant impact on the efficiency of algorithms. Algorithms with constant time complexity execute in a fixed and predictable time regardless of input size, while linear, quadratic, and exponential time complexities indicate a proportional increase in execution time with the input size. Understanding these common Big $\mathcal{O}$ complexities helps in 
    analyzing and comparing algorithms, selecting appropriate solutions for specific problem domains, and optimizing code to achieve efficient algorithmic performance.
    
    \begin{center}
        \begin{tabular}{|c|c|}
            \hline Notation & Name \\ \hline
            $\mathcal{O}(1)$ & Constant \\ \hline
            $\mathcal{O}(\log{(N)})$ & Logarithmic \\ \hline
            $\mathcal{O}(N)$ & Linear \\ \hline
            $\mathcal{O}(N\log{(N)})$ & Linearithmic \\ \hline
            $\mathcal{O}(N^2)$ & Quadratic \\ \hline
            $\mathcal{O}(c^N)$ & Exponential \\ \hline
        \end{tabular}
    \end{center}
    
    \begin{highlight}[Common Big $\mathcal{O}$ Complexity Examples]
        Below are examples of some common Big $\mathcal{O}$ complexities in C++: 
    
    \begin{code}[C++]
    #include <iostream>
    #include <vector>
    #include <algorithm>
    
    // O(1) - Constant time complexity
    void printFirstElement(const std::vector<int>& data) {
        if (!data.empty())
            std::cout << "First element: " << data[0] << std::endl;
    }
    
    // O(log n) - Logarithmic time complexity
    void binarySearch(const std::vector<int>& data, int target) {
        int low = 0;
        int high = data.size() - 1;
        while (low <= high) {
            int mid = low + (high - low) / 2;
            if (data[mid] == target) {
                std::cout << "Found at index " << mid << std::endl;
                return;
            }
            else if (data[mid] < target)
                low = mid + 1;
            else
                high = mid - 1;
        }
        std::cout << "Not found!" << std::endl;
    }
    
    // O(n) - Linear time complexity
    void linearSearch(const std::vector<int>& data, int target) {
        for (int i = 0; i < data.size(); i++) {
            if (data[i] == target) {
                std::cout << "Found at index " << i << std::endl;
                return;
            }
        }
        std::cout << "Not found!" << std::endl;
    }
    
    // O(n log n) - Linearithmic time complexity
    void mergeSort(std::vector<int>& data) {
        if (data.size() <= 1)
            return;
    
        std::vector<int> left(data.begin(), data.begin() + data.size() / 2);
        std::vector<int> right(data.begin() + data.size() / 2, data.end());
    
        mergeSort(left);
        mergeSort(right);
    
        std::merge(left.begin(), left.end(), right.begin(), 
        right.end(), data.begin());
    }
    
    // O(n^2) - Quadratic time complexity
    void bubbleSort(std::vector<int>& data) {
        for (int i = 0; i < data.size() - 1; i++) {
            for (int j = 0; j < data.size() - i - 1; j++) {
                if (data[j] > data[j + 1])
                    std::swap(data[j], data[j + 1]);
            }
        }
    }
    
    // O(2^n) - Exponential time complexity
    int fibonacci(int n) {
        if (n <= 1)
            return n;
        return fibonacci(n - 1) + fibonacci(n - 2);
    }
    
    // O(n!) - Factorial time complexity
    int factorial(int n) {
        if (n <= 1)
            return 1;
        return n * factorial(n - 1);
    }
    
    int main() {
        std::vector<int> data = { 5, 3, 1, 4, 2 };
        int target = 4;
    
        printFirstElement(data); // O(1)
        binarySearch(data, target); // O(log n)
        linearSearch(data, target); // O(n)
        mergeSort(data); // O(n log n)
        bubbleSort(data); // O(n^2)
        int fib = fibonacci(5); // O(2^n)
        int fact = factorial(5); // O(n!)
    
        std::cout << "Fibonacci: " << fib << std::endl;
        std::cout << "Factorial: " << fact << std::endl;
    
        return 0;
    }
    \end{code}
        The examples provided showcase different common Big $\mathcal{O}$ complexities in C++. They illustrate algorithms with varying growth rates as the input size increases. The examples include constant time complexity $\mathcal{O}(1)$, logarithmic time complexity $\mathcal{O}(log{(n)})$, linear time complexity $\mathcal{O}(n)$, linearithmic time complexity $\mathcal{O}(n\log{(n)})$, quadratic time complexity $\mathcal{O}(n^2)$, 
        exponential time complexity $\mathcal{O}(2^n)$, and factorial time complexity $\mathcal{O}(n!)$. Understanding these complexities is crucial for analyzing algorithmic efficiency, selecting appropriate solutions, and optimizing code. By considering the performance characteristics of algorithms, developers can make informed decisions to design efficient and scalable solutions.
    \end{highlight}
\end{notes}

\begin{notes}{Section 9.4 - Algorithm Analysis}
    \subsubsection*{Worst-Case Algorithm Analysis}

    Worst-case algorithm analysis is a method used to assess the maximum possible running time of an algorithm for a given input size. It involves identifying the input that would lead to the algorithm's worst performance and analyzing its execution time under that scenario. By focusing on the worst-case scenario, this approach provides a guarantee on the upper bound of the algorithm's running time. Worst-case analysis is widely used 
    in algorithm design and evaluation, as it allows programmers to make informed decisions about algorithm selection, optimization, and predicting the algorithm's behavior in real-world scenarios. It helps ensure that the algorithm performs efficiently and reliably even in unfavorable or challenging inputs.
    
    \subsubsection*{Counting Constant Time Operations}
    
    Counting constant time operations involves determining the number of basic operations executed by an algorithm, where each operation takes a constant amount of time regardless of the input size. It entails identifying and quantifying individual operations such as arithmetic operations, assignments, comparisons, or accessing elements in an array. By counting these operations, we can evaluate the efficiency of an algorithm in terms 
    of its time complexity and analyze how it scales with different input sizes. Counting constant time operations provides a quantitative measure to compare and reason about the efficiency of algorithms, enabling developers to make informed decisions regarding algorithm selection and optimization.
    
    \subsubsection*{Runtime Analysis of Nested Loops}
    
    Runtime analysis of nested loops involves analyzing the time complexity of algorithms that contain multiple nested loops. This analysis helps determine how the execution time of the algorithm grows as the input size increases. By examining the nesting structure and the number of iterations performed by each loop, we can derive the overall complexity of the nested loops. The runtime analysis considers the number of iterations and the 
    operations within each loop to estimate the overall efficiency of the algorithm. Understanding the runtime analysis of nested loops is essential for identifying potential performance bottlenecks, optimizing code, and designing efficient algorithms for handling larger input sizes.
    
    \begin{highlight}[Algorithm Analysis Example]
        Below is an example of analyzing an algorithm in C++: 
    
        \begin{code}[C++]
        bool searchValue(const std::vector<std::vector<int>>& array, int target) {
            // Outer loop iterates through each row
            for (const auto& row : array) {         
                // Inner loop iterates through each element in the row
                for (const auto& element : row) {     
                    if (element == target)
                        return true;
                }
            }
            return false;
        }
        \end{code}
        The provided example demonstrates the concepts of worst-case algorithm analysis, counting constant time operations, and runtime analysis of nested loops. The worst-case analysis involves considering the scenario where the target element is at the last position or absent in the array, resulting in the algorithm traversing through all elements. Counting constant time operations involves recognizing that each comparison operation 
        (`element == target') within the nested loops takes a constant amount of time. The runtime analysis of the nested loops determines that the time complexity of the algorithm is $\mathcal{O}(m*n)$, where $m$ is the number of rows and $n$ is the number of columns in the array. Understanding these concepts allows us to evaluate the algorithm's efficiency, optimize code if necessary, and predict its performance as the size of the input array changes.
    \end{highlight}
\end{notes}

\begin{notes}{Section 9.5 - Searching \& Algorithms}
    \subsubsection*{Algorithms}

    Algorithms are step-by-step procedures or sets of rules used to solve problems, perform computations, or achieve specific tasks. They are fundamental to computer science and play a vital role in software development. Algorithms can be designed to handle a wide range of tasks, such as sorting data, searching for elements, graph traversal, optimization, and more. The efficiency of an algorithm is evaluated through its time complexity (how long it takes to run) 
    and space complexity (how much memory it requires). Efficient algorithms are designed to minimize resource usage and deliver fast and scalable solutions. The study of algorithms involves analyzing their properties, designing new algorithms, and selecting the most appropriate algorithmic approach for a given problem. Ultimately, algorithms enable the automation and optimization of processes, providing the foundation for various computational tasks and applications.
    
    \subsubsection*{Algorithm Runtime}
    
    Algorithm runtime refers to the amount of time it takes for an algorithm to execute and produce a result. It is a critical aspect of algorithm analysis, as it helps evaluate the efficiency and performance of algorithms. The runtime of an algorithm is influenced by factors such as the input size, the complexity of the operations performed, and the algorithmic design itself. Analyzing algorithm runtime involves estimating how the execution time increases as the input 
    size grows, typically using Big $\mathcal{O}$ notation. Understanding algorithm runtime allows developers to identify bottlenecks, optimize code, and make informed decisions about algorithm selection to ensure efficient and scalable solutions. By striving for optimal runtime, developers can create algorithms that deliver faster results and handle larger data sets effectively.
\end{notes}

\begin{notes}{Section 9.6 - Analyzing the Time Complexity of Recursive Algorithms}
    \subsubsection*{Recurrence Relations}

    Recurrence relations are mathematical equations used to describe the time complexity of recursive algorithms. They provide a way to express the runtime of an algorithm in terms of the runtime of smaller subproblems. Recurrence relations are often used when analyzing the time complexity of divide-and-conquer algorithms, dynamic programming algorithms, or any algorithm that exhibits recursive behavior. By solving the recurrence relation, we can obtain an explicit formula 
    or a bound on the time complexity of the algorithm. Recurrence relations help in understanding how the algorithm's runtime grows with the input size and aid in comparing and evaluating different recursive algorithms. They are a powerful tool for reasoning about the time complexity of recursive algorithms and guiding algorithmic design decisions.
    
    \subsubsection*{Recursion Trees}
    
    Recursion trees are graphical representations used to analyze the time complexity of recursive algorithms. They provide a visual depiction of the recursive calls and their respective sizes during the execution of the algorithm. Each node in the tree represents a recursive call, and the edges represent the recursive relationship between the calls. The tree's depth corresponds to the number of recursive calls made, and the branches represent the sizes of the subproblems at 
    each level. By examining the recursion tree, we can analyze the total number of nodes (recursive calls) and the work done at each node to determine the algorithm's time complexity. This analysis helps in understanding the growth pattern of the algorithm as the input size increases and provides insights into optimizing the algorithm or making design choices to improve its efficiency. Recursion trees provide a clear and visual representation of the recursive algorithm's 
    execution, aiding in the analysis and understanding of its time complexity.
\end{notes}

The second chapter of this week is \textbf{Chapter 10: Sorting Algorithms}.

\begin{notes}{Section 10.1 - Sorting: Introduction}
    Sorting in object-oriented programming (OOP) involves organizing a collection of objects or data structures in a particular order based on specific criteria. OOP provides various techniques and algorithms to achieve efficient sorting, aiming to improve code reusability and maintainability. By encapsulating sorting functionality within classes or methods, OOP enables the creation of reusable and modular sorting components. This approach enhances code readability, simplifies 
maintenance, and allows for easy adaptation of sorting algorithms based on changing requirements. Additionally, OOP encourages the use of interfaces and inheritance, facilitating the implementation of different sorting strategies while adhering to common contracts or base classes.
\end{notes}

\begin{notes}{Section 10.2 - Quicksort}
    \subsubsection*{Quicksort}

    Quicksort is a highly efficient sorting algorithm commonly used in computer science. It follows a divide-and-conquer strategy, recursively dividing an array or list into smaller subarrays based on a pivot element, and then sorting these subarrays independently. The key idea behind Quicksort is to select a pivot element, typically the last or middle element of the array, and partition the remaining elements into two groups: those smaller than the pivot and those greater than 
    the pivot. This process is repeated recursively on the two resulting subarrays until the entire array is sorted. Quicksort's average and best-case time complexity is $\mathcal{O}(n\log{(n)})$, making it one of the fastest sorting algorithms in practice. However, in its worst-case scenario, when the pivot selection is unbalanced, the time complexity can degrade to $\mathcal{O}(n^2)$. Various optimizations, such as randomized pivot selection or using different partitioning schemes, 
    can be employed to mitigate the worst-case scenario.
    
    \subsubsection*{Partitioning Algorithm}
    
    The partitioning algorithm is a crucial step in many sorting algorithms, including Quicksort. It is responsible for rearranging elements in an array or list so that all elements smaller than a chosen pivot element are placed to its left, and all elements larger than the pivot are placed to its right. The partitioning process involves maintaining two pointers, one starting from the left end and the other from the right end of the array. These pointers traverse towards each other, 
    swapping elements when necessary, until they meet. This ensures that the pivot element is in its final sorted position, with smaller elements on its left and larger elements on its right. The partitioning algorithm enables efficient sorting by dividing the problem into smaller subproblems, allowing for subsequent recursive sorting on the resulting subarrays. It is a fundamental component of many efficient sorting algorithms, contributing to their overall performance and effectiveness.
    
    \subsubsection*{Recursively Sorting Partitions}
    
    Recursively sorting partitions is a common approach used in various sorting algorithms, such as Quicksort and Merge Sort. This technique involves dividing an array or list into smaller partitions or subarrays and recursively applying the sorting algorithm to each of these partitions. By recursively sorting smaller subarrays, the algorithm gradually reduces the problem size until it reaches base cases where the subarrays are either empty or contain a single element, which are inherently 
    sorted. The sorted subarrays are then merged or combined in a manner that guarantees the final sorted result. This recursive approach leverages the divide-and-conquer strategy, allowing for efficient sorting of larger datasets by breaking them down into more manageable pieces and leveraging the sorted results to obtain the overall sorted array.
    
    \subsubsection*{Quicksort Runtime}
    
    The runtime of Quicksort, on average and in the best case, is considered very efficient, with a time complexity of $\mathcal{O}(n\log{(n)})$. This means that the sorting algorithm can efficiently sort a collection of $n$ elements by making approximately $\log{(n)}$ recursive partitioning steps, each of which requires linear time to perform the necessary element comparisons and swaps. However, it's important to note that Quicksort's worst-case time complexity is $\mathcal{O}(n^2)$, which 
    occurs when the chosen pivot is consistently the smallest or largest element in the array, resulting in unbalanced partitions. To mitigate this, randomized pivot selection or other techniques can be employed to ensure a more balanced partitioning, reducing the likelihood of the worst-case scenario. In practice, Quicksort is often a preferred choice due to its average-case efficiency and adaptability to various datasets.
    
    \begin{highlight}[Quick Sort Example]
        Below is an example of a quicksort example in C++:
    
    \begin{code}[C++]
    #include <iostream>
    #include <vector>
    
    int partition(std::vector<int>& arr, int low, int high) {
        int pivot = arr[high];
        int i = low - 1;
    
        for (int j = low; j <= high - 1; j++) {
            if (arr[j] < pivot) {
                i++;
                std::swap(arr[i], arr[j]);
            }
        }
        std::swap(arr[i + 1], arr[high]);
        return i + 1;
    }
    
    void quickSort(std::vector<int>& arr, int low, int high) {
        if (low < high) {
            int pi = partition(arr, low, high);
            quickSort(arr, low, pi - 1);
            quickSort(arr, pi + 1, high);
        }
    }
    
    int main() {
        std::vector<int> arr = { 7, 2, 1, 6, 8, 5, 3, 4 };
        int size = arr.size();
    
        quickSort(arr, 0, size - 1);
    
        std::cout << "Sorted array: ";
        for (int i = 0; i < size; i++) {
            std::cout << arr[i] << " ";
        }
        std::cout << std::endl;
    
        return 0;
    }  
    \end{code}
        The provided example demonstrates the Quick Sort algorithm implemented in C++. Quick Sort is a divide-and-conquer algorithm that works by selecting a pivot element and partitioning the array into two subarrays based on the pivot. The partitioning step places the pivot element in its correct position, with smaller elements to its left and 
        larger elements to its right. The algorithm recursively applies this partitioning process to the subarrays until the entire array is sorted. The example showcases how the Quick Sort algorithm can be used to sort a vector of integers. By selecting appropriate pivot elements and efficiently partitioning the array, Quick Sort achieves an 
        average-case time complexity of $\mathcal{O}(\log{(n)})$. The example demonstrates the sorted array as the output, highlighting the effectiveness of the Quick Sort algorithm for sorting data efficiently.
    \end{highlight}
\end{notes}

\begin{notes}{Section 10.3 - Merge Sort}
    \subsubsection*{Merge Sort Overview}

    Merge Sort is a popular sorting algorithm that follows the divide-and-conquer paradigm. It works by recursively dividing the input array into smaller subarrays until they become single elements. Then, it merges the subarrays in a sorted order to obtain a fully sorted array. The algorithm utilizes the merge operation, which combines two sorted arrays 
    into a single sorted array. Merge Sort has a time complexity of $\mathcal{O}(n\log{(n)})$ in the worst, best, and average cases, making it efficient for large input sizes. It is known for its stability, as it maintains the relative order of equal elements. Merge Sort is widely used due to its reliable performance and ability to handle large datasets. However, it 
    requires additional space for the temporary arrays used during the merge process, which can be a consideration when dealing with limited memory resources. Overall, Merge Sort is a reliable and efficient sorting algorithm for a wide range of applications.l
    
    \subsubsection*{Merge Sort Partitioning}
    
    Merge Sort partitioning is the process by which the input array is divided into smaller subarrays during the execution of the Merge Sort algorithm. It follows a recursive approach, repeatedly halving the array until each subarray contains only one element. This process is known as the "divide" step in the divide-and-conquer paradigm. Merge Sort 
    partitions the array by calculating the middle index and recursively applying the partitioning process to the left and right halves of the array. This division continues until the subarrays contain only one element, at which point the merging step begins. Partitioning is a key component of Merge Sort that enables the subsequent merging of sorted 
    subarrays, ultimately leading to the generation of a fully sorted array.
    
    \subsubsection*{Merge Sort Algorithm}
    
    Merge Sort is a widely used sorting algorithm that employs a divide-and-conquer approach to efficiently sort an array of elements. The algorithm divides the input array into smaller subarrays until each subarray consists of only one element. Then, it merges adjacent subarrays in a sorted manner until the entire array is sorted. The merging process 
    compares the elements from the subarrays and combines them into a single sorted subarray. Merge Sort has a time complexity of 
    $\mathcal{O}(n\log{(n)})$ in the worst, best, and average cases, making it highly efficient for large input sizes. It is known for its stability, meaning that it maintains the relative order of equal elements. Merge Sort is a reliable and widely used sorting algorithm due to its efficient performance, stability, and scalability.
    
    \begin{highlight}[Merge Sort Algorithm Example]
        Below is an example of the merge sort algorithm in C++:
    
    \begin{code}[C++]
    #include <iostream>
    #include <vector>
    
    void merge(std::vector<int>& arr, int left, int middle, int right) {
        int leftSize = middle - left + 1;
        int rightSize = right - middle;
    
        std::vector<int> leftArr(leftSize);
        std::vector<int> rightArr(rightSize);
    
        for (int i = 0; i < leftSize; i++) {
            leftArr[i] = arr[left + i];
        }
    
        for (int j = 0; j < rightSize; j++) {
            rightArr[j] = arr[middle + 1 + j];
        }
    
        int i = 0, j = 0, k = left;
    
        while (i < leftSize && j < rightSize) {
            if (leftArr[i] <= rightArr[j]) {
                arr[k] = leftArr[i];
                i++;
            } else {
                arr[k] = rightArr[j];
                j++;
            }
            k++;
        }
    
        while (i < leftSize) {
            arr[k] = leftArr[i];
            i++;
            k++;
        }
    
        while (j < rightSize) {
            arr[k] = rightArr[j];
            j++;
            k++;
        }
    }
    
    void mergeSort(std::vector<int>& arr, int left, int right) {
        if (left < right) {
            int middle = left + (right - left) / 2;
    
            mergeSort(arr, left, middle);
            mergeSort(arr, middle + 1, right);
    
            merge(arr, left, middle, right);
        }
    }
    
    int main() {
        std::vector<int> arr = { 7, 2, 1, 6, 8, 5, 3, 4 };
        int size = arr.size();
    
        mergeSort(arr, 0, size - 1);
    
        std::cout << "Sorted array: ";
        for (int i = 0; i < size; i++) {
            std::cout << arr[i] << " ";
        }
        std::cout << std::endl;
    
        return 0;
    }
    \end{code}
        The provided example demonstrates the Merge Sort algorithm implemented in C++. Merge Sort is a divide-and-conquer algorithm that efficiently sorts an array by recursively dividing it into smaller subarrays and then merging them in a sorted manner. The example showcases how the Merge Sort algorithm can be used to sort a vector of integers. 
        By dividing the array, merging sorted subarrays, and eventually producing a fully sorted array, Merge Sort achieves an average-case time complexity of $\mathcal{O}(n\log{(n)})$. The example displays the sorted array as the output, highlighting the effectiveness of the Merge Sort algorithm in sorting data efficiently.
    \end{highlight}
\end{notes}

\begin{notes}{Section 10.4 - Bubble Sort}
    Bubble Sort is a simple and intuitive sorting algorithm that repeatedly compares adjacent elements and swaps them if they are in the wrong order. It iterates through the array multiple times, with each pass pushing the largest unsorted element to the end. The algorithm continues until the entire array is sorted. Bubble Sort has a time complexity of 
    $\mathcal{O}(n^2)$ in the worst, best, and average cases. While Bubble Sort is easy to understand and implement, it is not the most efficient sorting algorithm for large datasets. However, it can be useful for sorting small arrays or when simplicity is prioritized over efficiency.
    
    \begin{highlight}[Bubble Sort Algorithm Example]
        Below is an example of the bubble sort algorithm example in C++:
    
    \begin{code}[C++]
    #include <iostream>
    #include <vector>
    
    void bubbleSort(std::vector<int>& arr) {
        int n = arr.size();
        bool swapped;
    
        for (int i = 0; i < n - 1; i++) {
            swapped = false;
    
            for (int j = 0; j < n - i - 1; j++) {
                if (arr[j] > arr[j + 1]) {
                    std::swap(arr[j], arr[j + 1]);
                    swapped = true;
                }
            }
    
            if (!swapped) {
                // If no swapping occurred in the inner loop, 
                // the array is already sorted
                break;
            }
        }
    }
    
    int main() {
        std::vector<int> arr = { 7, 2, 1, 6, 8, 5, 3, 4 };
        int size = arr.size();
    
        bubbleSort(arr);
    
        std::cout << "Sorted array: ";
        for (int i = 0; i < size; i++) {
            std::cout << arr[i] << " ";
        }
        std::cout << std::endl;
    
        return 0;
    }
    \end{code}
        The provided example demonstrates the Bubble Sort algorithm implemented in C++. Bubble Sort is a simple and inefficient sorting algorithm that repeatedly compares adjacent elements and swaps them if they are out of order. The algorithm iterates through the array multiple times, gradually pushing the larger elements towards the end of the array.
        Bubble Sort has a time complexity of $\mathcal{O}(n^2)$ in the worst, best, and average cases, making it inefficient for large datasets. However, it is easy to understand and implement, making it suitable for small arrays or educational purposes. The example showcases the sorted array as the output, highlighting the step-by-step comparison 
        and swapping of elements that Bubble Sort performs to achieve a sorted result.
    \end{highlight}
\end{notes}

\begin{notes}{Section 10.5 - Selection Sort}
    Selection Sort is a simple sorting algorithm that repeatedly selects the smallest element from the unsorted portion of the array and swaps it with the element at the beginning of the sorted portion. It maintains two subarrays within the array: the left portion represents the sorted elements, while the right portion represents the unsorted elements. 
    In each iteration, the algorithm finds the smallest element from the unsorted portion and places it in its correct position in the sorted portion. This process continues until the entire array is sorted. Selection Sort has a time complexity of $\mathcal{O}(n^2)$ in the worst, best, and average cases, making it inefficient for large datasets. However, 
    it requires minimal additional memory and performs well for small arrays or when memory usage is a concern.
    
    \begin{highlight}[Selection Sort Algorithm Example]
        Below is an example of the selection sort algorithm in C++:
    
    \begin{code}[C++]
    #include <iostream>
    #include <vector>
    
    void selectionSort(std::vector<int>& arr) {
        int n = arr.size();
    
        for (int i = 0; i < n - 1; i++) {
            int minIndex = i;
    
            for (int j = i + 1; j < n; j++) {
                if (arr[j] < arr[minIndex]) {
                    minIndex = j;
                }
            }
    
            std::swap(arr[i], arr[minIndex]);
        }
    }
    
    int main() {
        std::vector<int> arr = { 7, 2, 1, 6, 8, 5, 3, 4 };
        int size = arr.size();
    
        selectionSort(arr);
    
        std::cout << "Sorted array: ";
        for (int i = 0; i < size; i++) {
            std::cout << arr[i] << " ";
        }
        std::cout << std::endl;
    
        return 0;
    }
    \end{code}
        The provided example demonstrates the Selection Sort algorithm implemented in C++. Selection Sort is a simple and inefficient sorting algorithm that repeatedly selects the smallest element from the unsorted portion of the array and swaps it with the element at the beginning of the sorted portion. The algorithm maintains two subarrays
        within the array, with the left portion representing the sorted elements and the right portion representing the unsorted elements. Selection Sort has a time complexity of $\mathcal{O}(n^2)$ in the worst, best, and average cases, making it inefficient for large datasets. However, it requires minimal additional memory and performs adequately
        for small arrays or situations where memory usage is a concern. The example showcases the sorted array as the output, demonstrating the step-by-step selection and swapping of elements that Selection Sort performs to achieve the desired sorting result.
    \end{highlight}
\end{notes}

\begin{notes}{Section 10.6 - Insertion Sort}
    Insertion Sort is a simple and efficient sorting algorithm that builds the final sorted array one element at a time. It starts with an initially empty sorted portion and gradually inserts each unsorted element into its correct position within the sorted portion. The algorithm iterates through the unsorted portion, comparing each element with the elements in
    the sorted portion and shifting them to the right if they are greater. This process continues until all elements are inserted into their appropriate positions, resulting in a fully sorted array. Insertion Sort has a time complexity of $\mathcal{O}(n^2)$ in the worst, best, and average cases. However, it performs exceptionally well on small datasets or partially
    sorted arrays, making it a suitable choice for such scenarios. Insertion Sort is an in-place algorithm, meaning it does not require additional memory beyond the input array, making it memory-efficient.
    
    \begin{highlight}[Insertion Sort Algorithm Example]
        Below is an example of the insertion sort algorithm in C++:
    
    \begin{code}[C++]
    #include <iostream>
    #include <vector>
    
    void insertionSort(std::vector<int>& arr) {
        int n = arr.size();
    
        for (int i = 1; i < n; i++) {
            int key = arr[i];
            int j = i - 1;
    
            while (j >= 0 && arr[j] > key) {
                arr[j + 1] = arr[j];
                j--;
            }
    
            arr[j + 1] = key;
        }
    }
    
    int main() {
        std::vector<int> arr = { 7, 2, 1, 6, 8, 5, 3, 4 };
        int size = arr.size();
    
        insertionSort(arr);
    
        std::cout << "Sorted array: ";
        for (int i = 0; i < size; i++) {
            std::cout << arr[i] << " ";
        }
        std::cout << std::endl;
    
        return 0;
    }
    \end{code}
        The provided example demonstrates the Insertion Sort algorithm implemented in C++. Insertion Sort is a simple and efficient sorting algorithm that builds the final sorted array one element at a time. It starts with an initially empty sorted portion and gradually inserts each unsorted element into its correct position within the sorted portion. 
        The algorithm iterates through the unsorted portion, comparing each element with the elements in the sorted portion and shifting them to the right if they are greater. Insertion Sort has a time complexity of $\mathcal{O}(n^2)$ in the worst, best, and average cases. However, it performs exceptionally well on small datasets or partially 
        sorted arrays. The example showcases the sorted array as the output, illustrating the step-by-step insertion of elements and the shifting of values that Insertion Sort performs to achieve the desired sorting result.
    \end{highlight}
\end{notes}

\begin{notes}{Section 10.7 - Shell Sort}
    Shell Sort is an efficient and adaptive sorting algorithm that improves upon the Insertion Sort algorithm by reducing the number of comparisons and data movements. It divides the array into smaller subarrays and performs Insertion Sort on each subarray. The key idea is to sort elements that are far apart, then progressively reduce the gap between elements being
    compared and swapped. This process is known as "diminishing increment sorting." Shell Sort utilizes a sequence of gaps, typically generated using the Knuth sequence, to determine the gap sizes. The algorithm continues to decrease the gap until it reaches 1, where it performs a final Insertion Sort pass. Shell Sort has a time complexity of $\mathcal{O}(n\log{(n)})$
    in the best case, and its worst-case time complexity depends on the chosen gap sequence. Overall, Shell Sort is a versatile algorithm that performs well on medium-sized arrays and offers better performance than Insertion Sort.
    
    \begin{highlight}[Shell Sort Algorithm Example]
        Below is an example of the shell sort algorithm in C++:
    
    \begin{code}[C++]
    #include <iostream>
    #include <vector>
    
    void shellSort(std::vector<int>& arr) {
        int n = arr.size();
    
        for (int gap = n / 2; gap > 0; gap /= 2) {
            for (int i = gap; i < n; i++) {
                int temp = arr[i];
                int j;
    
                for (j = i; j >= gap && arr[j - gap] > temp; j -= gap) {
                    arr[j] = arr[j - gap];
                }
    
                arr[j] = temp;
            }
        }
    }
    
    int main() {
        std::vector<int> arr = { 7, 2, 1, 6, 8, 5, 3, 4 };
        int size = arr.size();
    
        shellSort(arr);
    
        std::cout << "Sorted array: ";
        for (int i = 0; i < size; i++) {
            std::cout << arr[i] << " ";
        }
        std::cout << std::endl;
    
        return 0;
    }
    \end{code}
        The provided example demonstrates the Shell Sort algorithm implemented in C++. Shell Sort is an efficient sorting algorithm that improves upon Insertion Sort by dividing the array into smaller subarrays and performing Insertion Sort on each subarray. The algorithm utilizes a sequence of gaps, typically generated using the Knuth sequence,
        to determine the gap sizes. It starts with a large gap and progressively reduces the gap until it reaches 1, where it performs a final Insertion Sort pass. Shell Sort has a time complexity of $\mathcal{O}(n\log{(n)})$ in the best case, and its worst-case time complexity depends on the chosen gap sequence. The example showcases the sorted
        array as the output, demonstrating the step-by-step sorting process and the reduction of gaps that Shell Sort performs to achieve an efficiently sorted result.
    \end{highlight}
\end{notes}

\begin{notes}{Section 10.8 - Radix Sort}
    Radix Sort is a linear-time sorting algorithm that operates on the individual digits or characters of the elements being sorted. It groups the elements by their least significant digit, then by the next digit, and so on until all digits have been considered. This sorting process is typically performed using counting sort as a subroutine. Radix Sort
    is effective for sorting numbers, strings, or other data types where elements have multiple digits or characters. The time complexity of Radix Sort depends on the number of digits or characters in the elements being sorted, and it can be expressed as 
    $\mathcal{O}(d*(n+k))$, where $d$ is the number of digits or characters, $n$ is the number of elements, and $k$ is the range of values for each digit or character. Radix Sort is efficient for large datasets, especially when the number of digits or characters is relatively small.
    
    \begin{highlight}[Radix Sort Algorithm Example]
        Below is an example of the radix sort algorithm in C++:
    
        \begin{code}[C++]
        #include <iostream>
        #include <vector>
        
        int getMax(const std::vector<int>& arr) {
            int max = arr[0];
            for (int i = 1; i < arr.size(); i++) {
                if (arr[i] > max) {
                    max = arr[i];
                }
            }
            return max;
        }
        
        void countingSort(std::vector<int>& arr, int exp) {
            const int radix = 10;
            int n = arr.size();
            std::vector<int> count(radix, 0);
            std::vector<int> output(n, 0);
        
            for (int i = 0; i < n; i++) {
                count[(arr[i] / exp) % radix]++;
            }
        
            for (int i = 1; i < radix; i++) {
                count[i] += count[i - 1];
            }
        
            for (int i = n - 1; i >= 0; i--) {
                output[count[(arr[i] / exp) % radix] - 1] = arr[i];
                count[(arr[i] / exp) % radix]--;
            }
        
            for (int i = 0; i < n; i++) {
                arr[i] = output[i];
            }
        }
        
        void radixSort(std::vector<int>& arr) {
            int max = getMax(arr);
        
            for (int exp = 1; max / exp > 0; exp *= 10) {
                countingSort(arr, exp);
            }
        }
        
        int main() {
            std::vector<int> arr = { 170, 45, 75, 90, 802, 24, 2, 66 };
            int size = arr.size();
        
            radixSort(arr);
        
            std::cout << "Sorted array: ";
            for (int i = 0; i < size; i++) {
                std::cout << arr[i] << " ";
            }
            std::cout << std::endl;
        
            return 0;
        }
        \end{code}
        The provided example demonstrates the Radix Sort algorithm implemented in C++. Radix Sort is a linear-time sorting algorithm that operates on the individual digits of the elements being sorted. It utilizes a counting sort subroutine to group the elements by their least significant digit, then by the next digit, and so on until all digits have
        been considered. Radix Sort is efficient for sorting numbers or strings where elements have multiple digits or characters. The example showcases the sorted array as the output, illustrating the step-by-step sorting process performed by Radix Sort. The algorithm finds the maximum value in the array to determine the number of digits, then performs
        counting sort for each digit position. Radix Sort has a time complexity of $\mathcal{O}(d*(n+k))$, where $d$ is the number of digits, $n$ is the number of elements, and $k$ is the range of values for each digit.
    \end{highlight}
\end{notes}

\begin{notes}{Section 10.9 - Overview of Fast Sorting Algorithms}
    Fast sorting algorithms are efficient algorithms that aim to sort data quickly, especially for large datasets. These algorithms, such as QuickSort, Merge Sort, and Heap Sort, employ different techniques to achieve fast and reliable sorting. QuickSort utilizes a divide-and-conquer strategy by selecting a pivot element and partitioning the array into 
    subarrays based on the pivot. Merge Sort divides the array into smaller subarrays, recursively sorts them, and then merges them to obtain the final sorted array. Heap Sort constructs a binary heap from the input array and repeatedly extracts the maximum element to build the sorted array. These fast sorting algorithms typically have a time complexity of 
    $\mathcal{O}(n\log{(n)})$ in the average and worst cases, making them suitable for sorting large datasets efficiently. They often outperform slower sorting algorithms like Insertion Sort and Bubble Sort, especially when dealing with significant amounts of data.
    
    \subsubsection*{Sorting Algorithm's Average Runtime Complexity}
    
    \begin{center}
        \begin{tabular}{|c|c|c|}
            \hline Sorting Algorithm & Average Case Runtime Complexity & Fast? \\ \hline
            Selection Sort & $\mathcal{O}(N^2)$ & No \\ \hline
            Insertion Sort & $\mathcal{O}(N^2)$ & No \\ \hline
            Shell Sort & $\mathcal{O}(N^{1.5})$ & No \\ \hline
            Quick Sort & $\mathcal{O}(N\log{(N)})$ & Yes \\ \hline
            Merge Sort & $\mathcal{O}(N\log{(N)})$ & Yes \\ \hline
            Heap Sort & $\mathcal{O}(N\log{(N)})$ & Yes \\ \hline
            Radix Sort & $\mathcal{O}(N)$ & Yes \\ \hline
        \end{tabular}
    \end{center}
    
    \subsubsection*{Best Sorting Algorithm's Runtime Complexity}
    
    \begin{center}
        \begin{tabular}{|c|c|c|c|}
            \hline \scriptsize{Sorting Algorithm} & \scriptsize{Best Case Runtime Complexity} & \scriptsize{Average Case Runtime Complexity} & \scriptsize{Worst Case Runtime Complexity} \\ \hline
            Quicksort & $\mathcal{O}(N\log{(N)})$ & $\mathcal{O}(N\log{(N)})$ & $\mathcal{O}(N^2)$ \\ \hline
            Merge Sort & $\mathcal{O}(N\log{(N)})$ & $\mathcal{O}(N\log{(N)})$ & $\mathcal{O}(N\log{(N)})$ \\ \hline
            Heap Sort & $\mathcal{O}(N)$ & $\mathcal{O}(N\log{(N)})$ & $\mathcal{O}(N\log{(N)})$ \\ \hline
            Radix Sort & $\mathcal{O}(N)$ & $\mathcal{O}(N)$ & $\mathcal{O}(N)$ \\ \hline
        \end{tabular}
    \end{center}
\end{notes}